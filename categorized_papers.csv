web-scraper-order,web-scraper-start-url,ARXIV,IEEEXplore,CVECWEMapping,vulnerabilities,Vulnerabilities,RESEARCHGATE,RG1,RG2,RG3,LLM Security
1698807139-1,https://arxiv.org/search/?searchtype=all&query=%22Large+Language+Model%22&abstracts=show&size=200&order=-announced_date_first,"Search term or terms
        
          
        
        
      
      
        Field
        
      
      
          Search
      
    
    
      
        
        
           Show abstracts
        
        
        
           Hide abstracts
        
        
      
    
     
      
        
        Advanced Search
        
      
    
    
    
  

  

  
      

  
    
      
        
          
            
          
        
          
            
          
        
          
        
          
        
          
             Show abstracts Hide abstracts
          
        
      
      
        
          
            
          
          results per page.
        
        
          Sort results by
          
            
          
        
        
          Go
        
      
    
  

      


  
    
    Previous
    
    
    
      Next
      
    
    

      
        1
        
      

      
                                     
          
          
            2
            
          
          
          
            3
            
          
          
          
            4
            
          
          
          
            5
            
          
          
          …
        
      
    
  
  



 


  
    
      arXiv:2310.20689
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.AI
          
        
      
    
    
    
      
        Learning From Mistakes Makes LLM Better Reasoner
      
    
    
      Authors:
      
      Shengnan An, 
      
      Zexiong Ma, 
      
      Zeqi Lin, 
      
      Nanning Zheng, 
      
      Jian-Guang Lou, 
      
      Weizhu Chen
      
    
    
    
      Abstract:
      
        Large language models (LLMs) recently exhibited remarkable reasoning capabilities on solving math problems. To further improve this capability, this work proposes Learning from Mistakes (LeMa), akin to human learning processes. Consider a human student who failed to solve a math…
        ▽ More
      
      
        Large language models (LLMs) recently exhibited remarkable reasoning capabilities on solving math problems. To further improve this capability, this work proposes Learning from Mistakes (LeMa), akin to human learning processes. Consider a human student who failed to solve a math problem, he will learn from what mistake he has made and how to correct it. Mimicking this error-driven learning process, LeMa fine-tunes LLMs on mistake-correction data pairs generated by GPT-4. Specifically, we first collect inaccurate reasoning paths from various LLMs and then employ GPT-4 as a ""corrector"" to (1) identify the mistake step, (2) explain the reason for the mistake, and (3) correct the mistake and generate the final answer. Experimental results demonstrate the effectiveness of LeMa: across five backbone LLMs and two mathematical reasoning tasks, LeMa consistently improves the performance compared with fine-tuning on CoT data alone. Impressively, LeMa can also benefit specialized LLMs such as WizardMath and MetaMath, achieving 85.4% pass@1 accuracy on GSM8K and 27.1% on MATH. This surpasses the SOTA performance achieved by non-execution open-source models on these challenging tasks. Our code, data and models will be publicly available at https://github.com/microsoft/CodeT.
        △ Less
      
    
    

    Submitted 31 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      14 pages, 4 figures
    
    

    

    
  

  
    
      arXiv:2310.20633
         [pdf, other] 
      
      
        cs.CL
        
      
    
    
    
      
        Defining a New NLP Playground
      
    
    
      Authors:
      
      Sha Li, 
      
      Chi Han, 
      
      Pengfei Yu, 
      
      Carl Edwards, 
      
      Manling Li, 
      
      Xingyao Wang, 
      
      Yi R. Fung, 
      
      Charles Yu, 
      
      Joel R. Tetreault, 
      
      Eduard H. Hovy, 
      
      Heng Ji
      
    
    
    
      Abstract:
      
        The recent explosion of performance of large language models (LLMs) has changed the field of Natural Language Processing (NLP) more abruptly and seismically than any other shift in the field's 80-year history. This has resulted in concerns that the field will become homogeniz…
        ▽ More
      
      
        The recent explosion of performance of large language models (LLMs) has changed the field of Natural Language Processing (NLP) more abruptly and seismically than any other shift in the field's 80-year history. This has resulted in concerns that the field will become homogenized and resource-intensive. The new status quo has put many academic researchers, especially PhD students, at a disadvantage. This paper aims to define a new NLP playground by proposing 20+ PhD-dissertation-worthy research directions, covering theoretical analysis, new and challenging problems, learning paradigms, and interdisciplinary applications.
        △ Less
      
    
    

    Submitted 31 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      EMNLP Findings 2023 ""Theme Track: Large Language Models and the Future of NLP""
    
    

    

    
  

  
    
      arXiv:2310.20624
         [pdf, other] 
      
      
        cs.LG
        
          
            cs.AI
          
        
      
    
    
    
      
        LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B
      
    
    
      Authors:
      
      Simon Lermen, 
      
      Charlie Rogers-Smith, 
      
      Jeffrey Ladish
      
    
    
    
      Abstract:
      
        …often apply safety alignment procedures to prevent the misuse of their AI systems. For example, before Meta released Llama 2-Chat, a collection of instruction fine-tuned large language models, they invested heavily in safety training, incorporating extensive red-teaming and reinf…
        ▽ More
      
      
        AI developers often apply safety alignment procedures to prevent the misuse of their AI systems. For example, before Meta released Llama 2-Chat, a collection of instruction fine-tuned large language models, they invested heavily in safety training, incorporating extensive red-teaming and reinforcement learning from human feedback. However, it remains unclear how well safety training guards against model misuse when attackers have access to model weights. We explore the robustness of safety training in language models by subversively fine-tuning the public weights of Llama 2-Chat. We employ low-rank adaptation (LoRA) as an efficient fine-tuning method. With a budget of less than $200 per model and using only one GPU, we successfully undo the safety training of Llama 2-Chat models of sizes 7B, 13B, and 70B. Specifically, our fine-tuning technique significantly reduces the rate at which the model refuses to follow harmful instructions. We achieve a refusal rate below 1% for our 70B Llama 2-Chat model on two refusal benchmarks. Our fine-tuning method retains general performance, which we validate by comparing our fine-tuned models against Llama 2-Chat across two benchmarks. Additionally, we present a selection of harmful outputs produced by our models. While there is considerable uncertainty about the scope of risks from current models, it is likely that future models will have significantly more dangerous capabilities, including the ability to hack into critical infrastructure, create dangerous bio-weapons, or autonomously replicate and adapt to new environments. We show that subversive fine-tuning is practical and effective, and hence argue that evaluating risks from fine-tuning should be a core part of risk assessments for releasing model weights.
        △ Less
      
    
    

    Submitted 31 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.20587
         [pdf, other] 
      
      
        cs.LG
        
      
    
    
    
      
        Unleashing the Power of Pre-trained Language Models for Offline Reinforcement Learning
      
    
    
      Authors:
      
      Ruizhe Shi, 
      
      Yuyao Liu, 
      
      Yanjie Ze, 
      
      Simon S. Du, 
      
      Huazhe Xu
      
    
    
    
      Abstract:
      
        …scenarios, data collection could be costly and risky; therefore, offline RL becomes particularly challenging when the in-domain data is limited. Given recent advances in Large Language Models (LLMs) and their few-shot learning prowess, this paper introduces Language Mo…
        ▽ More
      
      
        Offline reinforcement learning (RL) aims to find a near-optimal policy using pre-collected datasets. In real-world scenarios, data collection could be costly and risky; therefore, offline RL becomes particularly challenging when the in-domain data is limited. Given recent advances in Large Language Models (LLMs) and their few-shot learning prowess, this paper introduces Language Models for Motion Control (LaMo), a general framework based on Decision Transformers to effectively use pre-trained Language Models (LMs) for offline RL. Our framework highlights four crucial components: (1) Initializing Decision Transformers with sequentially pre-trained LMs, (2) employing the LoRA fine-tuning method, in contrast to full-weight fine-tuning, to combine the pre-trained knowledge from LMs and in-domain knowledge effectively, (3) using the non-linear MLP transformation instead of linear projections, to generate embeddings, and (4) integrating an auxiliary language prediction loss during fine-tuning to stabilize the LMs and retain their original abilities on languages. Empirical results indicate LaMo achieves state-of-the-art performance in sparse-reward tasks and closes the gap between value-based offline RL methods and decision transformers in dense-reward tasks. In particular, our method demonstrates superior performance in scenarios with limited data samples. Our project website is https://lamo2023.github.io
        △ Less
      
    
    

    Submitted 31 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      19 pages, 9 tables
    
    

    

    
  

  
    
      arXiv:2310.20550
         [pdf, other] 
      
      
        cs.CV
        
          
            cs.AI
          
            cs.CL
          
            cs.LG
          
        
      
    
    
    
      
        CapsFusion: Rethinking Image-Text Data at Scale
      
    
    
      Authors:
      
      Qiying Yu, 
      
      Quan Sun, 
      
      Xiaosong Zhang, 
      
      Yufeng Cui, 
      
      Fan Zhang, 
      
      Xinlong Wang, 
      
      Jingjing Liu
      
    
    
    
      Abstract:
      
        …details in existing synthetic captions. To provide higher-quality and more scalable multimodal pretraining data, we propose CapsFusion, an advanced framework that leverages large language models to consolidate and refine information from both web-based image-text pairs and synthe…
        ▽ More
      
      
        Large multimodal models demonstrate remarkable generalist ability to perform diverse multimodal tasks in a zero-shot manner. Large-scale web-based image-text pairs contribute fundamentally to this success, but suffer from excessive noise. Recent studies use alternative captions synthesized by captioning models and have achieved notable benchmark performance. However, our experiments reveal significant Scalability Deficiency and World Knowledge Loss issues in models trained with synthetic captions, which have been largely obscured by their initial benchmark success. Upon closer examination, we identify the root cause as the overly-simplified language structure and lack of knowledge details in existing synthetic captions. To provide higher-quality and more scalable multimodal pretraining data, we propose CapsFusion, an advanced framework that leverages large language models to consolidate and refine information from both web-based image-text pairs and synthetic captions. Extensive experiments show that CapsFusion captions exhibit remarkable all-round superiority over existing captions in terms of model performance (e.g., 18.8 and 18.3 improvements in CIDEr score on COCO and NoCaps), sample efficiency (requiring 11-16 times less computation than baselines), world knowledge depth, and scalability. These effectiveness, efficiency and scalability advantages position CapsFusion as a promising candidate for future scaling of LMM training.
        △ Less
      
    
    

    Submitted 31 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.20501
         [pdf, other] 
      
      
        cs.IR
        
          
            cs.AI
          
            cs.CL
          
        
      
    
    
    
      
        LLMs may Dominate Information Access: Neural Retrievers are Biased Towards LLM-Generated Texts
      
    
    
      Authors:
      
      Sunhao Dai, 
      
      Yuqi Zhou, 
      
      Liang Pang, 
      
      Weihao Liu, 
      
      Xiaolin Hu, 
      
      Yong Liu, 
      
      Xiao Zhang, 
      
      Jun Xu
      
    
    
    
      Abstract:
      
        Recently, the emergence of large language models (LLMs) has revolutionized the paradigm of information retrieval (IR) applications, especially in web search. With their remarkable capabilities in generating human-like texts, LLMs have created enormous texts on the Internet. As a…
        ▽ More
      
      
        Recently, the emergence of large language models (LLMs) has revolutionized the paradigm of information retrieval (IR) applications, especially in web search. With their remarkable capabilities in generating human-like texts, LLMs have created enormous texts on the Internet. As a result, IR systems in the LLMs era are facing a new challenge: the indexed documents now are not only written by human beings but also automatically generated by the LLMs. How these LLM-generated documents influence the IR systems is a pressing and still unexplored question. In this work, we conduct a quantitative evaluation of different IR models in scenarios where both human-written and LLM-generated texts are involved. Surprisingly, our findings indicate that neural retrieval models tend to rank LLM-generated documents higher.We refer to this category of biases in neural retrieval models towards the LLM-generated text as the \textbf{source bias}. Moreover, we discover that this bias is not confined to the first-stage neural retrievers, but extends to the second-stage neural re-rankers. Then, we provide an in-depth analysis from the perspective of text compression and observe that neural models can better understand the semantic information of LLM-generated text, which is further substantiated by our theoretical analysis.We also discuss the potential server concerns stemming from the observed source bias and hope our findings can serve as a critical wake-up call to the IR community and beyond. To facilitate future explorations of IR in the LLM era, the constructed two new benchmarks and codes will later be available at \url{https://github.com/KID-22/LLM4IR-Bias}.
        △ Less
      
    
    

    Submitted 31 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.20499
         [pdf, other] 
      
      
        cs.CL
        
      
    
    
    
      
        Leveraging Word Guessing Games to Assess the Intelligence of Large Language Models
      
    
    
      Authors:
      
      Tian Liang, 
      
      Zhiwei He, 
      
      Jen-tes Huang, 
      
      Wenxuan Wang, 
      
      Wenxiang Jiao, 
      
      Rui Wang, 
      
      Yujiu Yang, 
      
      Zhaopeng Tu, 
      
      Shuming Shi, 
      
      Xing Wang
      
    
    
    
      Abstract:
      
        The automatic evaluation of LLM-based agent intelligence is critical in developing advanced LLM-based agents. Although considerable effort has been devoted to developing human-annotated evaluation datasets, such as AlpacaEval, existing techniques are costly, time-consuming, and lack adaptability. In this paper, inspired by the popular language game ``Who is Spy'', we propose to use the word guessi…
        ▽ More
      
      
        The automatic evaluation of LLM-based agent intelligence is critical in developing advanced LLM-based agents. Although considerable effort has been devoted to developing human-annotated evaluation datasets, such as AlpacaEval, existing techniques are costly, time-consuming, and lack adaptability. In this paper, inspired by the popular language game ``Who is Spy'', we propose to use the word guessing game to assess the intelligence performance of LLMs. Given a word, the LLM is asked to describe the word and determine its identity (spy or not) based on its and other players' descriptions. Ideally, an advanced agent should possess the ability to accurately describe a given word using an aggressive description while concurrently maximizing confusion in the conservative description, enhancing its participation in the game. To this end, we first develop DEEP to evaluate LLMs' expression and disguising abilities. DEEP requires LLM to describe a word in aggressive and conservative modes. We then introduce SpyGame, an interactive multi-agent framework designed to assess LLMs' intelligence through participation in a competitive language-based board game. Incorporating multi-agent interaction, SpyGame requires the target LLM to possess linguistic skills and strategic thinking, providing a more comprehensive evaluation of LLMs' human-like cognitive abilities and adaptability in complex communication situations. The proposed evaluation framework is very easy to implement. We collected words from multiple sources, domains, and languages and used the proposed evaluation framework to conduct experiments. Extensive experiments demonstrate that the proposed DEEP and SpyGame effectively evaluate the capabilities of various LLMs, capturing their ability to adapt to novel situations and engage in strategic communication.
        △ Less
      
    
    

    Submitted 31 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      Work in progress
    
    

    

    
  

  
    
      arXiv:2310.20487
         [pdf, other] 
      
      
        cs.IR
        
      
    
    
    
      
        Large Language Model Can Interpret Latent Space of Sequential Recommender
      
    
    
      Authors:
      
      Zhengyi Yang, 
      
      Jiancan Wu, 
      
      Yanchen Luo, 
      
      Jizhi Zhang, 
      
      Yancheng Yuan, 
      
      An Zhang, 
      
      Xiang Wang, 
      
      Xiangnan He
      
    
    
    
      Abstract:
      
        …is to model item sequences using discrete IDs, learning representations that encode sequential behaviors and reflect user preferences. Inspired by recent success in empowering large language models (LLMs) to understand and reason over diverse modality data (e.g., image, audio, 3D…
        ▽ More
      
      
        Sequential recommendation is to predict the next item of interest for a user, based on her/his interaction history with previous items. In conventional sequential recommenders, a common approach is to model item sequences using discrete IDs, learning representations that encode sequential behaviors and reflect user preferences. Inspired by recent success in empowering large language models (LLMs) to understand and reason over diverse modality data (e.g., image, audio, 3D points), a compelling research question arises: ``Can LLMs understand and work with hidden representations from ID-based sequential recommenders?''.To answer this, we propose a simple framework, RecInterpreter, which examines the capacity of open-source LLMs to decipher the representation space of sequential recommenders. Specifically, with the multimodal pairs (\ie representations of interaction sequence and text narrations), RecInterpreter first uses a lightweight adapter to map the representations into the token embedding space of the LLM. Subsequently, it constructs a sequence-recovery prompt that encourages the LLM to generate textual descriptions for items within the interaction sequence. Taking a step further, we propose a sequence-residual prompt instead, which guides the LLM in identifying the residual item by contrasting the representations before and after integrating this residual into the existing sequence. Empirical results showcase that our RecInterpreter enhances the exemplar LLM, LLaMA, to understand hidden representations from ID-based sequential recommenders, especially when guided by our sequence-residual prompts. Furthermore, RecInterpreter enables LLaMA to instantiate the oracle items generated by generative recommenders like DreamRec, concreting the item a user would ideally like to interact with next. Codes are available at https://github.com/YangZhengyi98/RecInterpreter.
        △ Less
      
    
    

    Submitted 31 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.20440
         [pdf, ps, other] 
      
      
        cs.CL
        
      
    
    
    
      
        The SourceData-NLP dataset: integrating curation into scientific publishing for training large language models
      
    
    
      Authors:
      
      Jorge Abreu-Vicente, 
      
      Hannah Sonntag, 
      
      Thomas Eidens, 
      
      Thomas Lemberger
      
    
    
    
      Abstract:
      
        Introduction: The scientific publishing landscape is expanding rapidly, creating challenges for researchers to stay up-to-date with the evolution of the literature. Natural Language Processing (NLP) has emerged as a potent approach to automating knowledge extraction from this vast amount of publications and preprints. Tasks such as Named-Entity Recognition (NER) and Named-Entity Linking (NEL), in…
        ▽ More
      
      
        Introduction: The scientific publishing landscape is expanding rapidly, creating challenges for researchers to stay up-to-date with the evolution of the literature. Natural Language Processing (NLP) has emerged as a potent approach to automating knowledge extraction from this vast amount of publications and preprints. Tasks such as Named-Entity Recognition (NER) and Named-Entity Linking (NEL), in conjunction with context-dependent semantic interpretation, offer promising and complementary approaches to extracting structured information and revealing key concepts.
  Results: We present the SourceData-NLP dataset produced through the routine curation of papers during the publication process. A unique feature of this dataset is its emphasis on the annotation of bioentities in figure legends. We annotate eight classes of biomedical entities (small molecules, gene products, subcellular components, cell lines, cell types, tissues, organisms, and diseases), their role in the experimental design, and the nature of the experimental method as an additional class. SourceData-NLP contains more than 620,000 annotated biomedical entities, curated from 18,689 figures in 3,223 papers in molecular and cell biology. We illustrate the dataset's usefulness by assessing BioLinkBERT and PubmedBERT, two transformers-based models, fine-tuned on the SourceData-NLP dataset for NER. We also introduce a novel context-dependent semantic task that infers whether an entity is the target of a controlled intervention or the object of measurement.
  Conclusions: SourceData-NLP's scale highlights the value of integrating curation into publishing. Models trained with SourceData-NLP will furthermore enable the development of tools able to extract causal hypotheses from the literature and assemble them into knowledge graphs.
        △ Less
      
    
    

    Submitted 31 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.20410
         [pdf, other] 
      
      
        cs.CL
        
      
    
    
    
      
        FollowBench: A Multi-level Fine-grained Constraints Following Benchmark for Large Language Models
      
    
    
      Authors:
      
      Yuxin Jiang, 
      
      Yufei Wang, 
      
      Xingshan Zeng, 
      
      Wanjun Zhong, 
      
      Liangyou Li, 
      
      Fei Mi, 
      
      Lifeng Shang, 
      
      Xin Jiang, 
      
      Qun Liu, 
      
      Wei Wang
      
    
    
    
      Abstract:
      
        The ability to follow instructions is crucial to Large Language Models (LLMs) to handle various real-world applications. Existing benchmarks primarily focus on evaluating superficial response quality, which does not necessarily indicate instruction-following capability. To fill t…
        ▽ More
      
      
        The ability to follow instructions is crucial to Large Language Models (LLMs) to handle various real-world applications. Existing benchmarks primarily focus on evaluating superficial response quality, which does not necessarily indicate instruction-following capability. To fill this research gap, in this paper, we propose FollowBench, a Multi-level Fine-grained Constraints Following Benchmark for LLMs. FollowBench comprehensively includes five different types (i.e., Content, Scenario, Style, Format, and Example) of fine-grained constraints. To enable a precise constraint following estimation, we introduce a Multi-level mechanism that incrementally adds a single constraint to the initial instruction at each level. To evaluate whether LLMs' outputs have satisfied every individual constraint, we propose to prompt strong LLMs with constraint evolution paths to handle challenging semantic constraints. By evaluating nine closed-source and open-source popular LLMs on FollowBench, we highlight the weaknesses of LLMs in instruction following and point towards potential avenues for future work. The data and code are publicly available at https://github.com/YJiangcm/FollowBench.
        △ Less
      
    
    

    Submitted 31 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      18 pages, 9 figures, 12 tables
    
    

    

    
  

  
    
      arXiv:2310.20384
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.AI
          
        
      
    
    
    
      
        Do large language models solve verbal analogies like children do?
      
    
    
      Authors:
      
      Claire E. Stevenson, 
      
      Mathilde ter Veen, 
      
      Rochelle Choenni, 
      
      Han L. J. van der Maas, 
      
      Ekaterina Shutova
      
    
    
    
      Abstract:
      
        …relations (\textit{kept in}) and answering \textit{chicken coop}. In contrast, children often use association, e.g., answering \textit{egg}. This paper investigates whether large language models (LLMs) solve verbal analogies in A:B::C:? form using associations, similar to what ch…
        ▽ More
      
      
        Analogy-making lies at the heart of human cognition. Adults solve analogies such as \textit{Horse belongs to stable like chicken belongs to ...?} by mapping relations (\textit{kept in}) and answering \textit{chicken coop}. In contrast, children often use association, e.g., answering \textit{egg}. This paper investigates whether large language models (LLMs) solve verbal analogies in A:B::C:? form using associations, similar to what children do. We use verbal analogies extracted from an online adaptive learning environment, where 14,002 7-12 year-olds from the Netherlands solved 622 analogies in Dutch. The six tested Dutch monolingual and multilingual LLMs performed around the same level as children, with MGPT performing worst, around the 7-year-old level, and XLM-V and GPT-3 the best, slightly above the 11-year-old level. However, when we control for associative processes this picture changes and each model's performance level drops 1-2 years. Further experiments demonstrate that associative processes often underlie correctly solved analogies. We conclude that the LLMs we tested indeed tend to solve verbal analogies by association with C like children do.
        △ Less
      
    
    

    Submitted 31 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.20357
         [pdf, other] 
      
      
        cs.AI
        
          
            cs.MM
          
        
      
    
    
    
      
        Enhancing the Spatial Awareness Capability of Multi-Modal Large Language Model
      
    
    
      Authors:
      
      Yongqiang Zhao, 
      
      Zhenyu Li, 
      
      Zhi Jin, 
      
      Feng Zhang, 
      
      Haiyan Zhao, 
      
      Chengfeng Dou, 
      
      Zhengwei Tao, 
      
      Xinhai Xu, 
      
      Donghong Liu
      
    
    
    
      Abstract:
      
        The Multi-Modal Large…
        ▽ More
      
      
        The Multi-Modal Large Language Model (MLLM) refers to an extension of the Large Language Model (LLM) equipped with the capability to receive and infer multi-modal data. Spatial awareness stands as one of the crucial abilities of MLLM, encompassing diverse skills related to understanding spatial relationships among objects and between objects and the scene area. Industries such as autonomous driving, smart healthcare, robotics, virtual, and augmented reality heavily demand MLLM's spatial awareness capabilities. However, there exists a noticeable gap between the current spatial awareness capabilities of MLLM and the requirements set by human needs. To address this issue, this paper proposes using more precise spatial position information between objects to guide MLLM in providing more accurate responses to user-related inquiries. Specifically, for a particular multi-modal task, we utilize algorithms for acquiring geometric spatial information and scene graphs to obtain relevant geometric spatial information and scene details of objects involved in the query. Subsequently, based on this information, we direct MLLM to address spatial awareness-related queries posed by the user. Extensive experiments were conducted in benchmarks such as MME, MM-Vet, and other multi-modal large language models. The experimental results thoroughly confirm the efficacy of the proposed method in enhancing the spatial awareness tasks and associated tasks of MLLM.
        △ Less
      
    
    

    Submitted 31 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.20329
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.SE
          
        
      
    
    
    
      
        InstructCoder: Empowering Language Models for Code Editing
      
    
    
      Authors:
      
      Qisheng Hu, 
      
      Kaixin Li, 
      
      Xu Zhao, 
      
      Yuxi Xie, 
      
      Tiedong Liu, 
      
      Hui Chen, 
      
      Qizhe Xie, 
      
      Junxian He
      
    
    
    
      Abstract:
      
        …usefulness, automatic code editing remains an underexplored area in the evolution of deep learning models, partly due to data scarcity. In this work, we explore the use of large language models (LLMs) to edit code based on user instructions, covering a broad range of implicit tas…
        ▽ More
      
      
        Code editing encompasses a variety of pragmatic tasks that developers deal with daily. Despite its relevance and practical usefulness, automatic code editing remains an underexplored area in the evolution of deep learning models, partly due to data scarcity. In this work, we explore the use of large language models (LLMs) to edit code based on user instructions, covering a broad range of implicit tasks such as comment insertion, code optimization, and code refactoring. To facilitate this, we introduce InstructCoder, the first dataset designed to adapt LLMs for general-purpose code editing, containing highdiversity code-editing tasks. It consists of over 114,000 instruction-input-output triplets and covers multiple distinct code editing scenarios. The dataset is systematically expanded through an iterative process that commences with code editing data sourced from GitHub commits as seed tasks. Seed and generated tasks are used subsequently to prompt ChatGPT for more task data. Our experiments demonstrate that open-source LLMs fine-tuned on InstructCoder can edit code correctly based on users' instructions most of the time, exhibiting unprecedented code-editing performance levels. Such results suggest that proficient instruction-finetuning can lead to significant amelioration in code editing abilities. The dataset and the source code are available at https://github.com/qishenghu/CodeInstruct.
        △ Less
      
    
    

    Submitted 31 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.20323
         [pdf, other] 
      
      
        cs.CV
        
          
            cs.AI
          
            cs.GR
          
            cs.HC
          
        
      
    
    
    
      
        SemanticBoost: Elevating Motion Generation with Augmented Textual Cues
      
    
    
      Authors:
      
      Xin He, 
      
      Shaoli Huang, 
      
      Xiaohang Zhan, 
      
      Chao Wen, 
      
      Ying Shan
      
    
    
    
      Abstract:
      
        …supplementary semantics from motion data, enriching the dataset's textual description and ensuring precise alignment between text and motion data without depending on large language models. On the other hand, the CAMD approach provides an all-encompassing solution for generat…
        ▽ More
      
      
        Current techniques face difficulties in generating motions from intricate semantic descriptions, primarily due to insufficient semantic annotations in datasets and weak contextual understanding. To address these issues, we present SemanticBoost, a novel framework that tackles both challenges simultaneously. Our framework comprises a Semantic Enhancement module and a Context-Attuned Motion Denoiser (CAMD). The Semantic Enhancement module extracts supplementary semantics from motion data, enriching the dataset's textual description and ensuring precise alignment between text and motion data without depending on large language models. On the other hand, the CAMD approach provides an all-encompassing solution for generating high-quality, semantically consistent motion sequences by effectively capturing context information and aligning the generated motion with the given textual descriptions. Distinct from existing methods, our approach can synthesize accurate orientational movements, combined motions based on specific body part descriptions, and motions generated from complex, extended sentences. Our experimental results demonstrate that SemanticBoost, as a diffusion-based method, outperforms auto-regressive-based techniques, achieving cutting-edge performance on the Humanml3D dataset while maintaining realistic and smooth motion generation quality.
        △ Less
      
    
    

    Submitted 31 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.20320
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.AI
          
        
      
    
    
    
      
        Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests
      
    
    
      Authors:
      
      Max J. van Duijn, 
      
      Bram M. A. van Dijk, 
      
      Tom Kouwenhoven, 
      
      Werner de Valk, 
      
      Marco R. Spruit, 
      
      Peter van der Putten
      
    
    
    
      Abstract:
      
        To what degree should we ascribe cognitive capacities to Large Language Models (LLMs), such as the ability to reason about intentions and beliefs known as Theory of Mind (ToM)? Here we add to this emerging debate by (i) testing 11 base- and instruction-tuned LLMs on capabilities…
        ▽ More
      
      
        To what degree should we ascribe cognitive capacities to Large Language Models (LLMs), such as the ability to reason about intentions and beliefs known as Theory of Mind (ToM)? Here we add to this emerging debate by (i) testing 11 base- and instruction-tuned LLMs on capabilities relevant to ToM beyond the dominant false-belief paradigm, including non-literal language usage and recursive intentionality; (ii) using newly rewritten versions of standardized tests to gauge LLMs' robustness; (iii) prompting and scoring for open besides closed questions; and (iv) benchmarking LLM performance against that of children aged 7-10 on the same tasks. We find that instruction-tuned LLMs from the GPT family outperform other models, and often also children. Base-LLMs are mostly unable to solve ToM tasks, even with specialized prompting. We suggest that the interlinked evolution and development of language and ToM may help explain what instruction-tuning adds: rewarding cooperative communication that takes into account interlocutor and context. We conclude by arguing for a nuanced perspective on ToM in LLMs.
        △ Less
      
    
    

    Submitted 31 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      14 pages, 4 figures, Forthcoming in Proceedings of the 27th Conference on Computational Natural Language Learning (CoNLL)
    
    

    

    
  

  
    
      arXiv:2310.20256
         [pdf, other] 
      
      
        cs.CL
        
      
    
    
    
      
        PsyCoT: Psychological Questionnaire as Powerful Chain-of-Thought for Personality Detection
      
    
    
      Authors:
      
      Tao Yang, 
      
      Tianyuan Shi, 
      
      Fanqi Wan, 
      
      Xiaojun Quan, 
      
      Qifan Wang, 
      
      Bingzhe Wu, 
      
      Jiaxiang Wu
      
    
    
    
      Abstract:
      
        Recent advances in large language models (LLMs), such as ChatGPT, have showcased remarkable zero-shot performance across various NLP tasks. However, the potential of LLMs in personality detection, which involves identifying an individual's personality from their written texts…
        ▽ More
      
      
        Recent advances in large language models (LLMs), such as ChatGPT, have showcased remarkable zero-shot performance across various NLP tasks. However, the potential of LLMs in personality detection, which involves identifying an individual's personality from their written texts, remains largely unexplored. Drawing inspiration from Psychological Questionnaires, which are carefully designed by psychologists to evaluate individual personality traits through a series of targeted items, we argue that these items can be regarded as a collection of well-structured chain-of-thought (CoT) processes. By incorporating these processes, LLMs can enhance their capabilities to make more reasonable inferences on personality from textual input. In light of this, we propose a novel personality detection method, called PsyCoT, which mimics the way individuals complete psychological questionnaires in a multi-turn dialogue manner. In particular, we employ a LLM as an AI assistant with a specialization in text analysis. We prompt the assistant to rate individual items at each turn and leverage the historical rating results to derive a conclusive personality preference. Our experiments demonstrate that PsyCoT significantly improves the performance and robustness of GPT-3.5 in personality detection, achieving an average F1 score improvement of 4.23/10.63 points on two benchmark datasets compared to the standard prompting method. Our code is available at https://github.com/TaoYang225/PsyCoT.
        △ Less
      
    
    

    Submitted 31 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      Accepted to EMNLP 2023
    
    

    

    
  

  
    
      arXiv:2310.20251
         [pdf, other] 
      
      
        cs.MM
        
      
    
    
    
      
        An Implementation of Multimodal Fusion System for Intelligent Digital Human Generation
      
    
    
      Authors:
      
      Yingjie Zhou, 
      
      Yaodong Chen, 
      
      Kaiyue Bi, 
      
      Lian Xiong, 
      
      Hui Liu
      
    
    
    
      Abstract:
      
        …digital human generation system with multimodal fusion is proposed. Specifically, text, speech and image are taken as inputs, and interactive speech is synthesized using large language model (LLM), voiceprint extraction, and text-to-speech conversion techniques. Then the input im…
        ▽ More
      
      
        With the rapid development of artificial intelligence (AI), digital humans have attracted more and more attention and are expected to achieve a wide range of applications in several industries. Then, most of the existing digital humans still rely on manual modeling by designers, which is a cumbersome process and has a long development cycle. Therefore, facing the rise of digital humans, there is an urgent need for a digital human generation system combined with AI to improve development efficiency. In this paper, an implementation scheme of an intelligent digital human generation system with multimodal fusion is proposed. Specifically, text, speech and image are taken as inputs, and interactive speech is synthesized using large language model (LLM), voiceprint extraction, and text-to-speech conversion techniques. Then the input image is age-transformed and a suitable image is selected as the driving image. Then, the modification and generation of digital human video content is realized by digital human driving, novel view synthesis, and intelligent dressing techniques. Finally, we enhance the user experience through style transfer, super-resolution, and quality evaluation. Experimental results show that the system can effectively realize digital human generation. The related code is released at https://github.com/zyj-2000/CUMT_2D_PhotoSpeaker.
        △ Less
      
    
    

    Submitted 31 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.20170
         [pdf, other] 
      
      
        cs.CL
        
      
    
    
    
      
        DIVKNOWQA: Assessing the Reasoning Ability of LLMs via Open-Domain Question Answering over Knowledge Base and Text
      
    
    
      Authors:
      
      Wenting Zhao, 
      
      Ye Liu, 
      
      Tong Niu, 
      
      Yao Wan, 
      
      Philip S. Yu, 
      
      Shafiq Joty, 
      
      Yingbo Zhou, 
      
      Semih Yavuz
      
    
    
    
      Abstract:
      
        Large Language Models (LLMs) have exhibited impressive generation capabilities, but they suffer from hallucinations when solely relying on their internal knowledge, especially when answering questions that require less commonly known information. Retrieval-augmented LLMs have eme…
        ▽ More
      
      
        Large Language Models (LLMs) have exhibited impressive generation capabilities, but they suffer from hallucinations when solely relying on their internal knowledge, especially when answering questions that require less commonly known information. Retrieval-augmented LLMs have emerged as a potential solution to ground LLMs in external knowledge. Nonetheless, recent approaches have primarily emphasized retrieval from unstructured text corpora, owing to its seamless integration into prompts. When using structured data such as knowledge graphs, most methods simplify it into natural text, neglecting the underlying structures. Moreover, a significant gap in the current landscape is the absence of a realistic benchmark for evaluating the effectiveness of grounding LLMs on heterogeneous knowledge sources (e.g., knowledge base and text). To fill this gap, we have curated a comprehensive dataset that poses two unique challenges: (1) Two-hop multi-source questions that require retrieving information from both open-domain structured and unstructured knowledge sources; retrieving information from structured knowledge sources is a critical component in correctly answering the questions. (2) The generation of symbolic queries (e.g., SPARQL for Wikidata) is a key requirement, which adds another layer of challenge. Our dataset is created using a combination of automatic generation through predefined reasoning chains and human annotation. We also introduce a novel approach that leverages multiple retrieval tools, including text passage retrieval and symbolic language-assisted retrieval. Our model outperforms previous approaches by a significant margin, demonstrating its effectiveness in addressing the above-mentioned reasoning challenges.
        △ Less
      
    
    

    Submitted 31 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.20158
         [pdf, other] 
      
      
        cs.CL
        
      
    
    
    
      
        GAR-meets-RAG Paradigm for Zero-Shot Information Retrieval
      
    
    
      Authors:
      
      Daman Arora, 
      
      Anush Kini, 
      
      Sayak Ray Chowdhury, 
      
      Nagarajan Natarajan, 
      
      Gaurav Sinha, 
      
      Amit Sharma
      
    
    
    
      Abstract:
      
        Given a query and a document corpus, the information retrieval (IR) task is to output a ranked list of relevant documents. Combining large language models (LLMs) with embedding-based retrieval models, recent work shows promising results on the zero-shot retrieval problem, i.e., n…
        ▽ More
      
      
        Given a query and a document corpus, the information retrieval (IR) task is to output a ranked list of relevant documents. Combining large language models (LLMs) with embedding-based retrieval models, recent work shows promising results on the zero-shot retrieval problem, i.e., no access to labeled data from the target domain. Two such popular paradigms are generation-augmented retrieval or GAR (generate additional context for the query and then retrieve), and retrieval-augmented generation or RAG (retrieve relevant documents as context and then generate answers). The success of these paradigms hinges on (i) high-recall retrieval models, which are difficult to obtain in the zero-shot setting, and (ii) high-precision (re-)ranking models which typically need a good initialization. In this work, we propose a novel GAR-meets-RAG recurrence formulation that overcomes the challenges of existing paradigms. Our method iteratively improves retrieval (via GAR) and rewrite (via RAG) stages in the zero-shot setting. A key design principle is that the rewrite-retrieval stages improve the recall of the system and a final re-ranking stage improves the precision. We conduct extensive experiments on zero-shot passage retrieval benchmarks, BEIR and TREC-DL. Our method establishes a new state-of-the-art in the BEIR benchmark, outperforming previous best results in Recall@100 and nDCG@10 metrics on 6 out of 8 datasets, with up to 17% relative gains over the previous best.
        △ Less
      
    
    

    Submitted 30 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      preprint
    
    

    

    
  

  
    
      arXiv:2310.20153
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.AI
          
        
      
    
    
    
      
        Interactive Multi-fidelity Learning for Cost-effective Adaptation of Language Model with Sparse Human Supervision
      
    
    
      Authors:
      
      Jiaxin Zhang, 
      
      Zhuohang Li, 
      
      Kamalika Das, 
      
      Sricharan Kumar
      
    
    
    
      Abstract:
      
        Large language models (LLMs) have demonstrated remarkable capabilities in various tasks. However, their suitability for domain-specific tasks, is limited due to their immense scale at deployment, susceptibility to misinformation, and more importantly, high data annotation costs.…
        ▽ More
      
      
        Large language models (LLMs) have demonstrated remarkable capabilities in various tasks. However, their suitability for domain-specific tasks, is limited due to their immense scale at deployment, susceptibility to misinformation, and more importantly, high data annotation costs. We propose a novel Interactive Multi-Fidelity Learning (IMFL) framework for the cost-effective development of small domain-specific LMs under limited annotation budgets. Our approach formulates the domain-specific fine-tuning process as a multi-fidelity learning problem, focusing on identifying the optimal acquisition strategy that balances between low-fidelity automatic LLM annotations and high-fidelity human annotations to maximize model performance. We further propose an exploration-exploitation query strategy that enhances annotation diversity and informativeness, incorporating two innovative designs: 1) prompt retrieval that selects in-context examples from human-annotated samples to improve LLM annotation, and 2) variable batch size that controls the order for choosing each fidelity to facilitate knowledge distillation, ultimately enhancing annotation quality. Extensive experiments on financial and medical tasks demonstrate that IMFL achieves superior performance compared with single fidelity annotations. Given a limited budget of human annotation, IMFL significantly outperforms the human annotation baselines in all four tasks and achieves very close performance as human annotations on two of the tasks. These promising results suggest that the high human annotation costs in domain-specific tasks can be significantly reduced by employing IMFL, which utilizes fewer human annotations, supplemented with cheaper and faster LLM (e.g., GPT-3.5) annotations to achieve comparable performance.
        △ Less
      
    
    

    Submitted 30 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      This work has been accepted by NeurIPS 2023
    
    

    

    
  

  
    
      arXiv:2310.20151
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.RO
          
            eess.SY
          
        
      
    
    
    
      
        Multi-Agent Consensus Seeking via Large Language Models
      
    
    
      Authors:
      
      Huaben Chen, 
      
      Wenkang Ji, 
      
      Lufeng Xu, 
      
      Shiyu Zhao
      
    
    
    
      Abstract:
      
        Multi-agent systems driven by large language models (LLMs) have shown promising abilities for solving complex tasks in a collaborative manner. This work considers a fundamental problem in multi-agent collaboration: consensus seeking. When multiple agents work together, we are int…
        ▽ More
      
      
        Multi-agent systems driven by large language models (LLMs) have shown promising abilities for solving complex tasks in a collaborative manner. This work considers a fundamental problem in multi-agent collaboration: consensus seeking. When multiple agents work together, we are interested in how they can reach a consensus through inter-agent negotiation. To that end, this work studies a consensus-seeking task where the state of each agent is a numerical value and they negotiate with each other to reach a consensus value. It is revealed that when not explicitly directed on which strategy should be adopted, the LLM-driven agents primarily use the average strategy for consensus seeking although they may occasionally use some other strategies. Moreover, this work analyzes the impact of the agent number, agent personality, and network topology on the negotiation process. The findings reported in this work can potentially lay the foundations for understanding the behaviors of LLM-driven multi-agent systems for solving more complex tasks. Furthermore, LLM-driven consensus seeking is applied to a multi-robot aggregation task. This application demonstrates the potential of LLM-driven agents to achieve zero-shot autonomous planning for multi-robot collaboration tasks. Project website: westlakeintelligentrobotics.github.io/ConsensusLLM/.
        △ Less
      
    
    

    Submitted 30 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.20150
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.AI
          
        
      
    
    
    
      
        Unlearn What You Want to Forget: Efficient Unlearning for LLMs
      
    
    
      Authors:
      
      Jiaao Chen, 
      
      Diyi Yang
      
    
    
    
      Abstract:
      
        Large language models (LLMs) have achieved significant progress from pre-training on and memorizing a wide range of textual data, however, this process might suffer from privacy issues and violations of data protection regulations. As a result, the ability to easily remove data r…
        ▽ More
      
      
        Large language models (LLMs) have achieved significant progress from pre-training on and memorizing a wide range of textual data, however, this process might suffer from privacy issues and violations of data protection regulations. As a result, the ability to easily remove data related to individual users from such models while not deteriorating their predictive quality after the removal becomes increasingly important. To address these issues, in this work, we propose an efficient unlearning framework that could efficiently update LLMs without having to retrain the whole model after data removals, by introducing lightweight unlearning layers learned with a selective teacher-student objective into the transformers. In addition, we introduce a fusion mechanism to effectively combine different unlearning layers that learns to forget different sets of data to handle a sequence of forgetting operations. Experiments on classification and generation tasks demonstrate the effectiveness of our proposed methods compared to the state-of-the-art baselines.
        △ Less
      
    
    

    Submitted 30 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      EMNLP 2023
    
    

    

    
  

  
    
      arXiv:2310.20138
         [pdf, other] 
      
      
        cs.CR
        
          
            cs.CL
          
        
      
    
    
    
      
        DEPN: Detecting and Editing Privacy Neurons in Pretrained Language Models
      
    
    
      Authors:
      
      Xinwei Wu, 
      
      Junzhuo Li, 
      
      Minghui Xu, 
      
      Weilong Dong, 
      
      Shuangzhi Wu, 
      
      Chao Bian, 
      
      Deyi Xiong
      
    
    
    
      Abstract:
      
        Large language models pretrained on a huge amount of data capture rich knowledge and information in the training data. The ability of data memorization and regurgitation in pretrained language models, revealed in previous studies, brings the risk of data leakage. In order to effe…
        ▽ More
      
      
        Large language models pretrained on a huge amount of data capture rich knowledge and information in the training data. The ability of data memorization and regurgitation in pretrained language models, revealed in previous studies, brings the risk of data leakage. In order to effectively reduce these risks, we propose a framework DEPN to Detect and Edit Privacy Neurons in pretrained language models, partially inspired by knowledge neurons and model editing. In DEPN, we introduce a novel method, termed as privacy neuron detector, to locate neurons associated with private information, and then edit these detected privacy neurons by setting their activations to zero. Furthermore, we propose a privacy neuron aggregator dememorize private information in a batch processing manner. Experimental results show that our method can significantly and efficiently reduce the exposure of private data leakage without deteriorating the performance of the model. Additionally, we empirically demonstrate the relationship between model memorization and privacy neurons, from multiple perspectives, including model size, training time, prompts, privacy neuron distribution, illustrating the robustness of our approach.
        △ Less
      
    
    

    Submitted 30 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      EMNLP 2023
    
    

    

    
  

  
    
      arXiv:2310.20111
         [pdf, other] 
      
      
        cs.CL
        
      
    
    
    
      
        Making Large Language Models Better Data Creators
      
    
    
      Authors:
      
      Dong-Ho Lee, 
      
      Jay Pujara, 
      
      Mohit Sewak, 
      
      Ryen W. White, 
      
      Sujay Kumar Jauhar
      
    
    
    
      Abstract:
      
        Although large language models (LLMs) have advanced the state-of-the-art in NLP significantly, deploying them for downstream applications is still challenging due to cost, responsiveness, control, or concerns around privacy and security. As such, trainable models are still the pr…
        ▽ More
      
      
        Although large language models (LLMs) have advanced the state-of-the-art in NLP significantly, deploying them for downstream applications is still challenging due to cost, responsiveness, control, or concerns around privacy and security. As such, trainable models are still the preferred option in some cases. However, these models still require human-labeled data for optimal performance, which is expensive and time-consuming to obtain. In order to address this issue, several techniques to reduce human effort involve labeling or generating data using LLMs. Although these methods are effective for certain applications, in practice they encounter difficulties in real-world scenarios. Labeling data requires careful data selection, while generating data necessitates task-specific prompt engineering. In this paper, we propose a unified data creation pipeline that requires only a single formatting example, and which is applicable to a broad range of tasks, including traditionally problematic ones with semantically devoid label spaces. In our experiments we demonstrate that instruction-following LLMs are highly cost-effective data creators, and that models trained with these data exhibit performance better than those trained with human-labeled data (by up to 17.5%) on out-of-distribution evaluation, while maintaining comparable performance on in-distribution tasks. These results have important implications for the robustness of NLP systems deployed in the real-world.
        △ Less
      
    
    

    Submitted 30 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      Accepted to EMNLP 2023 main conference. 12 pages, 5 figures, 6 tables. Code is available at https://github.com/microsoft/llm-data-creation
    
    

    

    
  

  
    
      arXiv:2310.20105
         [pdf, other] 
      
      
        cs.CY
        
          
            cs.AI
          
            cs.CL
          
        
      
    
    
    
      
        Efficient Classification of Student Help Requests in Programming Courses Using Large Language Models
      
    
    
      Authors:
      
      Jaromir Savelka, 
      
      Paul Denny, 
      
      Mark Liffiton, 
      
      Brad Sheese
      
    
    
    
      Abstract:
      
        …student help requests with respect to the type of help being sought can enable the tailoring of effective responses. Automatically classifying such requests is non-trivial, but large language models (LLMs) appear to offer an accessible, cost-effective solution. This study evaluat…
        ▽ More
      
      
        The accurate classification of student help requests with respect to the type of help being sought can enable the tailoring of effective responses. Automatically classifying such requests is non-trivial, but large language models (LLMs) appear to offer an accessible, cost-effective solution. This study evaluates the performance of the GPT-3.5 and GPT-4 models for classifying help requests from students in an introductory programming class. In zero-shot trials, GPT-3.5 and GPT-4 exhibited comparable performance on most categories, while GPT-4 outperformed GPT-3.5 in classifying sub-categories for requests related to debugging. Fine-tuning the GPT-3.5 model improved its performance to such an extent that it approximated the accuracy and consistency across categories observed between two human raters. Overall, this study demonstrates the feasibility of using LLMs to enhance educational systems through the automated classification of student needs.
        △ Less
      
    
    

    Submitted 30 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.20081
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.AI
          
            cs.IR
          
        
      
    
    
    
      
        Integrating Summarization and Retrieval for Enhanced Personalization via Large Language Models
      
    
    
      Authors:
      
      Chris Richardson, 
      
      Yao Zhang, 
      
      Kellen Gillespie, 
      
      Sudipta Kar, 
      
      Arshdeep Singh, 
      
      Zeynab Raeesy, 
      
      Omar Zia Khan, 
      
      Abhinav Sethy
      
    
    
    
      Abstract:
      
        …the ability to tailor a system to individual users, is an essential factor in user experience with natural language processing (NLP) systems. With the emergence of Large Language Models (LLMs), a key question is how to leverage these models to better personalize user experiences…
        ▽ More
      
      
        Personalization, the ability to tailor a system to individual users, is an essential factor in user experience with natural language processing (NLP) systems. With the emergence of Large Language Models (LLMs), a key question is how to leverage these models to better personalize user experiences. To personalize a language model's output, a straightforward approach is to incorporate past user data into the language model prompt, but this approach can result in lengthy inputs exceeding limitations on input length and incurring latency and cost issues. Existing approaches tackle such challenges by selectively extracting relevant user data (i.e. selective retrieval) to construct a prompt for downstream tasks. However, retrieval-based methods are limited by potential information loss, lack of more profound user understanding, and cold-start challenges. To overcome these limitations, we propose a novel summary-augmented approach by extending retrieval-augmented personalization with task-aware user summaries generated by LLMs. The summaries can be generated and stored offline, enabling real-world systems with runtime constraints like voice assistants to leverage the power of LLMs. Experiments show our method with 75% less of retrieved user data is on-par or outperforms retrieval augmentation on most tasks in the LaMP personalization benchmark. We demonstrate that offline summarization via LLMs and runtime retrieval enables better performance for personalization on a range of tasks under practical constraints.
        △ Less
      
    
    

    Submitted 30 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      4 pages, International Workshop on Personalized Generative AI (@CIKM 2023)
    
    

    
      
        

        

        
          ACM Class:
          I.2.7; H.3.3
        
      
    

    
  

  
    
      arXiv:2310.20051
         [pdf, other] 
      
      
        cs.LG
        
      
    
    
    
      
        The Expressibility of Polynomial based Attention Scheme
      
    
    
      Authors:
      
      Zhao Song, 
      
      Guangyi Xu, 
      
      Junze Yin
      
    
    
    
      Abstract:
      
        Large language models (LLMs) have significantly improved various aspects of our daily lives. These models have impacted numerous domains, from healthcare to education, enhancing productivity, decision-making processes, and accessibility. As a result, they have influenced and, to…
        ▽ More
      
      
        Large language models (LLMs) have significantly improved various aspects of our daily lives. These models have impacted numerous domains, from healthcare to education, enhancing productivity, decision-making processes, and accessibility. As a result, they have influenced and, to some extent, reshaped people's lifestyles. However, the quadratic complexity of attention in transformer architectures poses a challenge when scaling up these models for processing long textual contexts. This issue makes it impractical to train very large models on lengthy texts or use them efficiently during inference. While a recent study by [KMZ23] introduced a technique that replaces the softmax with a polynomial function and polynomial sketching to speed up attention mechanisms, the theoretical understandings of this new approach are not yet well understood.
  In this paper, we offer a theoretical analysis of the expressive capabilities of polynomial attention. Our study reveals a disparity in the ability of high-degree and low-degree polynomial attention. Specifically, we construct two carefully designed datasets, namely D0 and D1, where D1 includes a feature with a significantly larger value compared to D0. We demonstrate that with a sufficiently high degree β, a single-layer polynomial attention network can distinguish between D0 and D1. However, with a low degree β, the network cannot effectively separate the two datasets. This analysis underscores the greater effectiveness of high-degree polynomials in amplifying large values and distinguishing between datasets. Our analysis offers insight into the representational capacity of polynomial attention and provides a rationale for incorporating higher-degree polynomials in attention mechanisms to capture intricate linguistic correlations.
        △ Less
      
    
    

    Submitted 30 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      arXiv admin note: substantial text overlap with arXiv:2310.11685
    
    

    

    
  

  
    
      arXiv:2310.20046
         [pdf, other] 
      
      
        cs.CL
        
      
    
    
    
      
        Which Examples to Annotate for In-Context Learning? Towards Effective and Efficient Selection
      
    
    
      Authors:
      
      Costas Mavromatis, 
      
      Balasubramaniam Srinivasan, 
      
      Zhengyuan Shen, 
      
      Jiani Zhang, 
      
      Huzefa Rangwala, 
      
      Christos Faloutsos, 
      
      George Karypis
      
    
    
    
      Abstract:
      
        Large Language Models (LLMs) can adapt to new tasks via in-context learning (ICL). ICL is efficient as it does not require any parameter updates to the trained LLM, but only few annotated examples as input for the LLM. In this work, we investigate an active learning approach for…
        ▽ More
      
      
        Large Language Models (LLMs) can adapt to new tasks via in-context learning (ICL). ICL is efficient as it does not require any parameter updates to the trained LLM, but only few annotated examples as input for the LLM. In this work, we investigate an active learning approach for ICL, where there is a limited budget for annotating examples. We propose a model-adaptive optimization-free algorithm, termed AdaICL, which identifies examples that the model is uncertain about, and performs semantic diversity-based example selection. Diversity-based sampling improves overall effectiveness, while uncertainty sampling improves budget efficiency and helps the LLM learn new information. Moreover, AdaICL poses its sampling strategy as a Maximum Coverage problem, that dynamically adapts based on the model's feedback and can be approximately solved via greedy algorithms. Extensive experiments on nine datasets and seven LLMs show that AdaICL improves performance by 4.4% accuracy points over SOTA (7.7% relative improvement), is up to 3x more budget-efficient than performing annotations uniformly at random, while it outperforms SOTA with 2x fewer ICL examples.
        △ Less
      
    
    

    Submitted 30 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.20034
         [pdf, other] 
      
      
        cs.RO
        
      
    
    
    
      
        GG-LLM: Geometrically Grounding Large Language Models for Zero-shot Human Activity Forecasting in Human-Aware Task Planning
      
    
    
      Authors:
      
      Moritz A. Graule, 
      
      Volkan Isler
      
    
    
    
      Abstract:
      
        …connecting predictions from such models to the environment at hand to ensure the applicability of these predictions is an unsolved problem. We present a system that utilizes a Large Language Model (LLM) to infer a human's next actions from a range of modalities without fine-…
        ▽ More
      
      
        A robot in a human-centric environment needs to account for the human's intent and future motion in its task and motion planning to ensure safe and effective operation. This requires symbolic reasoning about probable future actions and the ability to tie these actions to specific locations in the physical environment. While one can train behavioral models capable of predicting human motion from past activities, this approach requires large amounts of data to achieve acceptable long-horizon predictions. More importantly, the resulting models are constrained to specific data formats and modalities. Moreover, connecting predictions from such models to the environment at hand to ensure the applicability of these predictions is an unsolved problem. We present a system that utilizes a Large Language Model (LLM) to infer a human's next actions from a range of modalities without fine-tuning. A novel aspect of our system that is critical to robotics applications is that it links the predicted actions to specific locations in a semantic map of the environment. Our method leverages the fact that LLMs, trained on a vast corpus of text describing typical human behaviors, encode substantial world knowledge, including probable sequences of human actions and activities. We demonstrate how these localized activity predictions can be incorporated in a human-aware task planner for an assistive robot to reduce the occurrences of undesirable human-robot interactions by 29.2% on average.
        △ Less
      
    
    

    Submitted 30 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.20033
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.AI
          
        
      
    
    
    
      
        Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization
      
    
    
      Authors:
      
      Prakamya Mishra, 
      
      Zonghai Yao, 
      
      Shuwei Chen, 
      
      Beining Wang, 
      
      Rohan Mittal, 
      
      Hong Yu
      
    
    
    
      Abstract:
      
        Large Language Models (LLMs) like the GPT and LLaMA families have demonstrated exceptional capabilities in capturing and condensing critical contextual information and achieving state-of-the-art performance in the summarization task. However, community concerns about these models…
        ▽ More
      
      
        Large Language Models (LLMs) like the GPT and LLaMA families have demonstrated exceptional capabilities in capturing and condensing critical contextual information and achieving state-of-the-art performance in the summarization task. However, community concerns about these models' hallucination issues continue to rise. LLMs sometimes generate factually hallucinated summaries, which can be extremely harmful in the clinical domain NLP tasks (e.g., clinical note summarization), where factually incorrect statements can lead to critically erroneous diagnoses. Fine-tuning LLMs using human feedback has shown the promise of aligning LLMs to be factually consistent during generation, but such training procedure requires high-quality human-annotated data, which can be extremely expensive to get in the clinical domain. In this work, we propose a new pipeline using ChatGPT instead of human experts to generate high-quality feedback data for improving factual consistency in the clinical note summarization task. We focus specifically on edit feedback because recent work discusses the shortcomings of human alignment via preference feedback in complex situations (such as clinical NLP tasks that require extensive expert knowledge), as well as some advantages of collecting edit feedback from domain experts. In addition, although GPT has reached the expert level in many clinical NLP tasks (e.g., USMLE QA), there is not much previous work discussing whether GPT can generate expert-level edit feedback for LMs in the clinical note summarization task. We hope to fill this gap. Finally, our evaluations demonstrate the potential use of GPT edits in human alignment, especially from a factuality perspective.
        △ Less
      
    
    

    Submitted 30 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.19998
         [pdf] 
      
      
        cs.CL
        
          
            cond-mat.dis-nn
          
            cond-mat.mes-hall
          
            cond-mat.mtrl-sci
          
            physics.app-ph
          
        
      
    
    
    
      
        Generative retrieval-augmented ontologic graph and multi-agent strategies for interpretive large language model-based materials design
      
    
    
      Authors:
      
      Markus J. Buehler
      
    
    
    
      Abstract:
      
        …analysis, design and manufacturing, including their capacity to work effectively with both human language, symbols, code, and numerical data. Here we explore the use of large language models (LLMs) as a tool that can support engineering analysis of materials, applied to retrievin…
        ▽ More
      
      
        Transformer neural networks show promising capabilities, in particular for uses in materials analysis, design and manufacturing, including their capacity to work effectively with both human language, symbols, code, and numerical data. Here we explore the use of large language models (LLMs) as a tool that can support engineering analysis of materials, applied to retrieving key information about subject areas, developing research hypotheses, discovery of mechanistic relationships across disparate areas of knowledge, and writing and executing simulation codes for active knowledge generation based on physical ground truths. When used as sets of AI agents with specific features, capabilities, and instructions, LLMs can provide powerful problem solution strategies for applications in analysis and design problems. Our experiments focus on using a fine-tuned model, MechGPT, developed based on training data in the mechanics of materials domain. We first affirm how finetuning endows LLMs with reasonable understanding of domain knowledge. However, when queried outside the context of learned matter, LLMs can have difficulty to recall correct information. We show how this can be addressed using retrieval-augmented Ontological Knowledge Graph strategies that discern how the model understands what concepts are important and how they are related. Illustrated for a use case of relating distinct areas of knowledge - here, music and proteins - such strategies can also provide an interpretable graph structure with rich information at the node, edge and subgraph level. We discuss nonlinear sampling strategies and agent-based modeling applied to complex question answering, code generation and execution in the context of automated force field development from actively learned Density Functional Theory (DFT) modeling, and data analysis.
        △ Less
      
    
    

    Submitted 30 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.19995
         [pdf, other] 
      
      
        cs.CV
        
      
    
    
    
      
        Emotional Theory of Mind: Bridging Fast Visual Processing with Slow Linguistic Reasoning
      
    
    
      Authors:
      
      Yasaman Etesam, 
      
      Ozge Nilay Yalcin, 
      
      Chuxuan Zhang, 
      
      Angelica Lim
      
    
    
    
      Abstract:
      
        …problems in affective computing. The goal of this work is to evaluate the emotional commonsense knowledge embedded in recent large vision language models (CLIP, LLaVA) and large language models (GPT-3.5) on the Emotions in Context (EMOTIC) dataset. In order to evaluate a purely t…
        ▽ More
      
      
        The emotional theory of mind problem in images is an emotion recognition task, specifically asking ""How does the person in the bounding box feel?"" Facial expressions, body pose, contextual information and implicit commonsense knowledge all contribute to the difficulty of the task, making this task currently one of the hardest problems in affective computing. The goal of this work is to evaluate the emotional commonsense knowledge embedded in recent large vision language models (CLIP, LLaVA) and large language models (GPT-3.5) on the Emotions in Context (EMOTIC) dataset. In order to evaluate a purely text-based language model on images, we construct ""narrative captions"" relevant to emotion perception, using a set of 872 physical social signal descriptions related to 26 emotional categories, along with 224 labels for emotionally salient environmental contexts, sourced from writer's guides for character expressions and settings. We evaluate the use of the resulting captions in an image-to-language-to-emotion task. Experiments using zero-shot vision-language models on EMOTIC show that combining ""fast"" and ""slow"" reasoning is a promising way forward to improve emotion recognition systems. Nevertheless, a gap remains in the zero-shot emotional theory of mind task compared to prior work trained on the EMOTIC dataset.
        △ Less
      
    
    

    Submitted 30 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      16 pages(including references and appendix), 8 Tables, 3 figures
    
    

    

    
  

  
    
      arXiv:2310.19975
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.AI
          
        
      
    
    
    
      
        BioInstruct: Instruction Tuning of Large Language Models for Biomedical Natural Language Processing
      
    
    
      Authors:
      
      Hieu Tran, 
      
      Zhichao Yang, 
      
      Zonghai Yao, 
      
      Hong Yu
      
    
    
    
      Abstract:
      
        Large language models (LLMs) has achieved a great success in many natural language processing (NLP) tasks. This is achieved by pretraining of LLMs on vast amount of data and then instruction tuning to specific domains. However, only a few instructions in the biomedical domain hav…
        ▽ More
      
      
        Large language models (LLMs) has achieved a great success in many natural language processing (NLP) tasks. This is achieved by pretraining of LLMs on vast amount of data and then instruction tuning to specific domains. However, only a few instructions in the biomedical domain have been published. To address this issue, we introduce BioInstruct, a customized task-specific instruction dataset containing more than 25,000 examples. This dataset was generated attractively by prompting a GPT-4 language model with a three-seed-sample of 80 human-curated instructions. By fine-tuning LLMs using the BioInstruct dataset, we aim to optimize the LLM's performance in biomedical natural language processing (BioNLP). We conducted instruction tuning on the LLaMA LLMs (1\&2, 7B\&13B) and evaluated them on BioNLP applications, including information extraction, question answering, and text generation. We also evaluated how instructions contributed to model performance using multi-tasking learning principles.
        △ Less
      
    
    

    Submitted 30 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      14 pages, 3 figures
    
    

    

    
  

  
    
      arXiv:2310.19915
         [pdf, other] 
      
      
        cs.LG
        
          
            q-bio.BM
          
        
      
    
    
    
      
        GPCR-BERT: Interpreting Sequential Design of G Protein Coupled Receptors Using Protein Language Models
      
    
    
      Authors:
      
      Seongwon Kim, 
      
      Parisa Mollaei, 
      
      Akshay Antony, 
      
      Rishikesh Magar, 
      
      Amir Barati Farimani
      
    
    
    
      Abstract:
      
        With the rise of Transformers and Large Language Models (LLMs) in Chemistry and Biology, new avenues for the design and understanding of therapeutics have opened up to the scientific community. Protein sequences can be modeled as language and can take advantage of recent advances…
        ▽ More
      
      
        With the rise of Transformers and Large Language Models (LLMs) in Chemistry and Biology, new avenues for the design and understanding of therapeutics have opened up to the scientific community. Protein sequences can be modeled as language and can take advantage of recent advances in LLMs, specifically with the abundance of our access to the protein sequence datasets. In this paper, we developed the GPCR-BERT model for understanding the sequential design of G Protein-Coupled Receptors (GPCRs). GPCRs are the target of over one-third of FDA-approved pharmaceuticals. However, there is a lack of comprehensive understanding regarding the relationship between amino acid sequence, ligand selectivity, and conformational motifs (such as NPxxY, CWxP, E/DRY). By utilizing the pre-trained protein model (Prot-Bert) and fine-tuning with prediction tasks of variations in the motifs, we were able to shed light on several relationships between residues in the binding pocket and some of the conserved motifs. To achieve this, we took advantage of attention weights, and hidden states of the model that are interpreted to extract the extent of contributions of amino acids in dictating the type of masked ones. The fine-tuned models demonstrated high accuracy in predicting hidden residues within the motifs. In addition, the analysis of embedding was performed over 3D structures to elucidate the higher-order interactions within the conformations of the receptors.
        △ Less
      
    
    

    Submitted 30 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      25 pages, 5 figures
    
    

    

    
  

  
    
      arXiv:2310.19842
         [pdf, other] 
      
      
        cs.SD
        
          
            cs.LG
          
            eess.AS
          
        
      
    
    
    
      
        Musical Form Generation
      
    
    
      Authors:
      
      Lilac Atassi
      
    
    
    
      Abstract:
      
        …with transitions between these segments. The generation of prompts that determine the high-level composition is distinct from the creation of finer, lower-level details. A large language model is then used to suggest the musical form.
        ▽ More
      
      
        While recent generative models can produce engaging music, their utility is limited. The variation in the music is often left to chance, resulting in compositions that lack structure. Pieces extending beyond a minute can become incoherent or repetitive. This paper introduces an approach for generating structured, arbitrarily long musical pieces. Central to this approach is the creation of musical segments using a conditional generative model, with transitions between these segments. The generation of prompts that determine the high-level composition is distinct from the creation of finer, lower-level details. A large language model is then used to suggest the musical form.
        △ Less
      
    
    

    Submitted 30 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.19813
         [pdf, ps, other] 
      
      
        cs.SE
        
          
            cs.AI
          
            cs.LG
          
            cs.NE
          
        
      
    
    
    
      
        Enhancing Genetic Improvement Mutations Using Large Language Models
      
    
    
      Authors:
      
      Alexander E. I. Brownlee, 
      
      James Callan, 
      
      Karine Even-Mendoza, 
      
      Alina Geiger, 
      
      Carol Hanna, 
      
      Justyna Petke, 
      
      Federica Sarro, 
      
      Dominik Sobania
      
    
    
    
      Abstract:
      
        Large language models (LLMs) have been successfully applied to software engineering tasks, including program repair. However, their application in search-based techniques such as Genetic Improvement (GI) is still largely unexplored. In this paper, we evaluate the use of LLMs as m…
        ▽ More
      
      
        Large language models (LLMs) have been successfully applied to software engineering tasks, including program repair. However, their application in search-based techniques such as Genetic Improvement (GI) is still largely unexplored. In this paper, we evaluate the use of LLMs as mutation operators for GI to improve the search process. We expand the Gin Java GI toolkit to call OpenAI's API to generate edits for the JCodec tool. We randomly sample the space of edits using 5 different edit types. We find that the number of patches passing unit tests is up to 75% higher with LLM-based edits than with standard Insert edits. Further, we observe that the patches found with LLMs are generally less diverse compared to standard edits. We ran GI with local search to find runtime improvements. Although many improving patches are found by LLM-enhanced GI, the best improving patch was found by standard GI.
        △ Less
      
    
    

    Submitted 18 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      Accepted for publication at the Symposium on Search-Based Software Engineering (SSBSE) 2023
    
    

    

    
  

  
    
      arXiv:2310.19792
         [pdf, other] 
      
      
        cs.CL
        
      
    
    
    
      
        The Eval4NLP 2023 Shared Task on Prompting Large Language Models as Explainable Metrics
      
    
    
      Authors:
      
      Christoph Leiter, 
      
      Juri Opitz, 
      
      Daniel Deutsch, 
      
      Yang Gao, 
      
      Rotem Dror, 
      
      Steffen Eger
      
    
    
    
      Abstract:
      
        With an increasing number of parameters and pre-training data, generative large language models (LLMs) have shown remarkable capabilities to solve tasks with minimal or no task-related examples. Notably, LLMs have been successfully employed as evaluation metrics in text generatio…
        ▽ More
      
      
        With an increasing number of parameters and pre-training data, generative large language models (LLMs) have shown remarkable capabilities to solve tasks with minimal or no task-related examples. Notably, LLMs have been successfully employed as evaluation metrics in text generation tasks. Within this context, we introduce the Eval4NLP 2023 shared task that asks participants to explore prompting and score extraction for machine translation (MT) and summarization evaluation. Specifically, we propose a novel competition setting in which we select a list of allowed LLMs and disallow fine-tuning to ensure a focus on prompting. We present an overview of participants' approaches and evaluate them on a new reference-free test set spanning three language pairs for MT and a summarization dataset. Notably, despite the task's restrictions, the best-performing systems achieve results on par with or even surpassing recent reference-free metrics developed using larger models, including GEMBA and Comet-Kiwi-XXL. Finally, as a separate track, we perform a small-scale human evaluation of the plausibility of explanations given by the LLMs.
        △ Less
      
    
    

    Submitted 30 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.19791
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.AI
          
            cs.LG
          
            cs.PL
          
        
      
    
    
    
      
        LILO: Learning Interpretable Libraries by Compressing and Documenting Code
      
    
    
      Authors:
      
      Gabriel Grand, 
      
      Lionel Wong, 
      
      Matthew Bowers, 
      
      Theo X. Olausson, 
      
      Muxin Liu, 
      
      Joshua B. Tenenbaum, 
      
      Jacob Andreas
      
    
    
    
      Abstract:
      
        While large language models (LLMs) now excel at code generation, a key aspect of software development is the art of refactoring: consolidating code into libraries of reusable and readable programs. In this paper, we introduce LILO, a neurosymbolic framework that iteratively synth…
        ▽ More
      
      
        While large language models (LLMs) now excel at code generation, a key aspect of software development is the art of refactoring: consolidating code into libraries of reusable and readable programs. In this paper, we introduce LILO, a neurosymbolic framework that iteratively synthesizes, compresses, and documents code to build libraries tailored to particular problem domains. LILO combines LLM-guided program synthesis with recent algorithmic advances in automated refactoring from Stitch: a symbolic compression system that efficiently identifies optimal lambda abstractions across large code corpora. To make these abstractions interpretable, we introduce an auto-documentation (AutoDoc) procedure that infers natural language names and docstrings based on contextual examples of usage. In addition to improving human readability, we find that AutoDoc boosts performance by helping LILO's synthesizer to interpret and deploy learned abstractions. We evaluate LILO on three inductive program synthesis benchmarks for string editing, scene reasoning, and graphics composition. Compared to existing neural and symbolic methods - including the state-of-the-art library learning algorithm DreamCoder - LILO solves more complex tasks and learns richer libraries that are grounded in linguistic knowledge.
        △ Less
      
    
    

    Submitted 30 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.19773
         [pdf, other] 
      
      
        cs.CV
        
      
    
    
    
      
        MM-VID: Advancing Video Understanding with GPT-4V(ision)
      
    
    
      Authors:
      
      Kevin Lin, 
      
      Faisal Ahmed, 
      
      Linjie Li, 
      
      Chung-Ching Lin, 
      
      Ehsan Azarnasab, 
      
      Zhengyuan Yang, 
      
      Jianfeng Wang, 
      
      Lin Liang, 
      
      Zicheng Liu, 
      
      Yumao Lu, 
      
      Ce Liu, 
      
      Lijuan Wang
      
    
    
    
      Abstract:
      
        …GPT-4V to transcribe multimodal elements into a long textual script. The generated script details character movements, actions, expressions, and dialogues, paving the way for large language models (LLMs) to achieve video understanding. This enables advanced capabilities, includin…
        ▽ More
      
      
        We present MM-VID, an integrated system that harnesses the capabilities of GPT-4V, combined with specialized tools in vision, audio, and speech, to facilitate advanced video understanding. MM-VID is designed to address the challenges posed by long-form videos and intricate tasks such as reasoning within hour-long content and grasping storylines spanning multiple episodes. MM-VID uses a video-to-script generation with GPT-4V to transcribe multimodal elements into a long textual script. The generated script details character movements, actions, expressions, and dialogues, paving the way for large language models (LLMs) to achieve video understanding. This enables advanced capabilities, including audio description, character identification, and multimodal high-level comprehension. Experimental results demonstrate the effectiveness of MM-VID in handling distinct video genres with various video lengths. Additionally, we showcase its potential when applied to interactive environments, such as video games and graphic user interfaces.
        △ Less
      
    
    

    Submitted 30 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      Project page at https://multimodal-vid.github.io/
    
    

    

    
  

  
    
      arXiv:2310.19750
         [pdf, other] 
      
      
        cs.CL
        
      
    
    
    
      
        Chain-of-Thought Embeddings for Stance Detection on Social Media
      
    
    
      Authors:
      
      Joseph Gatto, 
      
      Omar Sharif, 
      
      Sarah Masud Preum
      
    
    
    
      Abstract:
      
        Stance detection on social media is challenging for Large Language Models (LLMs), as emerging slang and colloquial language in online conversations often contain deeply implicit stance labels. Chain-of-Thought (COT) prompting has recently been shown to improve performance on stan…
        ▽ More
      
      
        Stance detection on social media is challenging for Large Language Models (LLMs), as emerging slang and colloquial language in online conversations often contain deeply implicit stance labels. Chain-of-Thought (COT) prompting has recently been shown to improve performance on stance detection tasks -- alleviating some of these issues. However, COT prompting still struggles with implicit stance identification. This challenge arises because many samples are initially challenging to comprehend before a model becomes familiar with the slang and evolving knowledge related to different topics, all of which need to be acquired through the training data. In this study, we address this problem by introducing COT Embeddings which improve COT performance on stance detection tasks by embedding COT reasonings and integrating them into a traditional RoBERTa-based stance detection pipeline. Our analysis demonstrates that 1) text encoders can leverage COT reasonings with minor errors or hallucinations that would otherwise distort the COT output label. 2) Text encoders can overlook misleading COT reasoning when a sample's prediction heavily depends on domain-specific patterns. Our model achieves SOTA performance on multiple stance detection datasets collected from social media.
        △ Less
      
    
    

    Submitted 30 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      Accepted at EMNLP-2023, 8 pages
    
    

    

    
  

  
    
      arXiv:2310.19740
         [pdf, other] 
      
      
        cs.CL
        
      
    
    
    
      
        Collaborative Evaluation: Exploring the Synergy of Large Language Models and Humans for Open-ended Generation Evaluation
      
    
    
      Authors:
      
      Qintong Li, 
      
      Leyang Cui, 
      
      Lingpeng Kong, 
      
      Wei Bi
      
    
    
    
      Abstract:
      
        …in the evaluation of open-ended natural language generation tasks (NLG) that demand creativity, as automatic metrics often exhibit weak correlations with human judgments. Large language models (LLMs) recently have emerged as a scalable and cost-effective alternative to human eval…
        ▽ More
      
      
        Humans are widely involved in the evaluation of open-ended natural language generation tasks (NLG) that demand creativity, as automatic metrics often exhibit weak correlations with human judgments. Large language models (LLMs) recently have emerged as a scalable and cost-effective alternative to human evaluations. However, both humans and LLMs have limitations, i.e., inherent subjectivity and unreliable judgments, particularly for open-ended tasks that require adaptable metrics tailored to diverse task requirements. To explore the synergy between humans and LLM-based evaluators and address the challenges of existing inconsistent evaluation criteria in open-ended NLG tasks, we propose a Collaborative Evaluation pipeline CoEval, involving the design of a checklist of task-specific criteria and the detailed evaluation of texts, in which LLM generates initial ideation, and then humans engage in scrutiny. We conducted a series of experiments to investigate the mutual effects between LLMs and humans in CoEval. Results show that, by utilizing LLMs, CoEval effectively evaluates lengthy texts, saving significant time and reducing human evaluation outliers. Human scrutiny still plays a role, revising around 20% of LLM evaluation scores for ultimate reliability.
        △ Less
      
    
    

    Submitted 30 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      We release our resources at \url{https://github.com/qtli/CoEval}
    
    

    

    
  

  
    
      arXiv:2310.19737
         [pdf, other] 
      
      
        cs.AI
        
      
    
    
    
      
        Adversarial Attacks and Defenses in Large Language Models: Old and New Threats
      
    
    
      Authors:
      
      Leo Schwinn, 
      
      David Dobre, 
      
      Stephan Günnemann, 
      
      Gauthier Gidel
      
    
    
    
      Abstract:
      
        …security. In this context, we will face substantial challenges associated with an impending adversarial arms race in natural language processing, specifically with closed-source Large Language Models (LLMs), such as ChatGPT, Google Bard, or Anthropic's Claude. We provide a fi…
        ▽ More
      
      
        Over the past decade, there has been extensive research aimed at enhancing the robustness of neural networks, yet this problem remains vastly unsolved. Here, one major impediment has been the overestimation of the robustness of new defense approaches due to faulty defense evaluations. Flawed robustness evaluations necessitate rectifications in subsequent works, dangerously slowing down the research and providing a false sense of security. In this context, we will face substantial challenges associated with an impending adversarial arms race in natural language processing, specifically with closed-source Large Language Models (LLMs), such as ChatGPT, Google Bard, or Anthropic's Claude. We provide a first set of prerequisites to improve the robustness assessment of new approaches and reduce the amount of faulty evaluations. Additionally, we identify embedding space attacks on LLMs as another viable threat model for the purposes of generating malicious content in open-sourced models. Finally, we demonstrate on a recently proposed defense that, without LLM-specific best practices in place, it is easy to overestimate the robustness of a new approach.
        △ Less
      
    
    

    Submitted 30 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.19736
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.AI
          
        
      
    
    
    
      
        Evaluating Large Language Models: A Comprehensive Survey
      
    
    
      Authors:
      
      Zishan Guo, 
      
      Renren Jin, 
      
      Chuang Liu, 
      
      Yufei Huang, 
      
      Dan Shi, 
      
       Supryadi, 
      
      Linhao Yu, 
      
      Yan Liu, 
      
      Jiaxuan Li, 
      
      Bojian Xiong, 
      
      Deyi Xiong
      
    
    
    
      Abstract:
      
        Large language models (LLMs) have demonstrated remarkable capabilities across a broad spectrum of tasks. They have attracted significant attention and been deployed in numerous downstream applications. Nevertheless, akin to a double-edged sword, LLMs also present potential risks.…
        ▽ More
      
      
        Large language models (LLMs) have demonstrated remarkable capabilities across a broad spectrum of tasks. They have attracted significant attention and been deployed in numerous downstream applications. Nevertheless, akin to a double-edged sword, LLMs also present potential risks. They could suffer from private data leaks or yield inappropriate, harmful, or misleading content. Additionally, the rapid progress of LLMs raises concerns about the potential emergence of superintelligent systems without adequate safeguards. To effectively capitalize on LLM capacities as well as ensure their safe and beneficial development, it is critical to conduct a rigorous and comprehensive evaluation of LLMs.
  This survey endeavors to offer a panoramic perspective on the evaluation of LLMs. We categorize the evaluation of LLMs into three major groups: knowledge and capability evaluation, alignment evaluation and safety evaluation. In addition to the comprehensive review on the evaluation methodologies and benchmarks on these three aspects, we collate a compendium of evaluations pertaining to LLMs' performance in specialized domains, and discuss the construction of comprehensive evaluation platforms that cover LLM evaluations on capabilities, alignment, safety, and applicability.
  We hope that this comprehensive overview will stimulate further research interests in the evaluation of LLMs, with the ultimate goal of making evaluation serve as a cornerstone in guiding the responsible development of LLMs. We envision that this will channel their evolution into a direction that maximizes societal benefit while minimizing potential risks. A curated list of related papers has been publicly available at https://github.com/tjunlp-lab/Awesome-LLMs-Evaluation-Papers.
        △ Less
      
    
    

    Submitted 31 October, 2023; v1 submitted 30 October, 2023;
      originally announced October 2023.
      
    
    
    
      Comments:
      111 pages
    
    

    

    
  

  
    
      arXiv:2310.19677
         [pdf, other] 
      
      
        cs.CL
        
      
    
    
    
      
        MoCa: Measuring Human-Language Model Alignment on Causal and Moral Judgment Tasks
      
    
    
      Authors:
      
      Allen Nie, 
      
      Yuhui Zhang, 
      
      Atharva Amdekar, 
      
      Chris Piech, 
      
      Tatsunori Hashimoto, 
      
      Tobias Gerstenberg
      
    
    
    
      Abstract:
      
        …a dataset of stories from 24 cognitive science papers and developed a system to annotate each story with the factors they investigated. Using this dataset, we test whether large language models (LLMs) make causal and moral judgments about text-based scenarios that align with thos…
        ▽ More
      
      
        Human commonsense understanding of the physical and social world is organized around intuitive theories. These theories support making causal and moral judgments. When something bad happens, we naturally ask: who did what, and why? A rich literature in cognitive science has studied people's causal and moral intuitions. This work has revealed a number of factors that systematically influence people's judgments, such as the violation of norms and whether the harm is avoidable or inevitable. We collected a dataset of stories from 24 cognitive science papers and developed a system to annotate each story with the factors they investigated. Using this dataset, we test whether large language models (LLMs) make causal and moral judgments about text-based scenarios that align with those of human participants. On the aggregate level, alignment has improved with more recent LLMs. However, using statistical analyses, we find that LLMs weigh the different factors quite differently from human participants. These results show how curated, challenge datasets combined with insights from cognitive science can help us go beyond comparisons based merely on aggregate metrics: we uncover LLMs implicit tendencies and show to what extent these align with human intuitions.
        △ Less
      
    
    

    Submitted 31 October, 2023; v1 submitted 30 October, 2023;
      originally announced October 2023.
      
    
    
    
      Comments:
      34 pages, 7 figures. NeurIPS 2023
    
    

    

    
  

  
    
      arXiv:2310.19671
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.AI
          
        
      
    
    
    
      
        Large Language Models: The Need for Nuance in Current Debates and a Pragmatic Perspective on Understanding
      
    
    
      Authors:
      
      Bram M. A. van Dijk, 
      
      Tom Kouwenhoven, 
      
      Marco R. Spruit, 
      
      Max J. van Duijn
      
    
    
    
      Abstract:
      
        Current Large Language Models (LLMs) are unparalleled in their ability to generate grammatically correct, fluent text. LLMs are appearing rapidly, and debates on LLM capacities have taken off, but reflection is lagging behind. Thus, in this position paper, we first zoom in on the…
        ▽ More
      
      
        Current Large Language Models (LLMs) are unparalleled in their ability to generate grammatically correct, fluent text. LLMs are appearing rapidly, and debates on LLM capacities have taken off, but reflection is lagging behind. Thus, in this position paper, we first zoom in on the debate and critically assess three points recurring in critiques of LLM capacities: i) that LLMs only parrot statistical patterns in the training data; ii) that LLMs master formal but not functional language competence; and iii) that language learning in LLMs cannot inform human language learning. Drawing on empirical and theoretical arguments, we show that these points need more nuance. Second, we outline a pragmatic perspective on the issue of `real' understanding and intentionality in LLMs. Understanding and intentionality pertain to unobservable mental states we attribute to other humans because they have pragmatic value: they allow us to abstract away from complex underlying mechanics and predict behaviour effectively. We reflect on the circumstances under which it would make sense for humans to similarly attribute mental states to LLMs, thereby outlining a pragmatic philosophical context for LLMs as an increasingly prominent technology in society.
        △ Less
      
    
    

    Submitted 31 October, 2023; v1 submitted 30 October, 2023;
      originally announced October 2023.
      
    
    
    
      Comments:
      15 pages, 0 figures, Forthcoming in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing
    
    

    

    
  

  
    
      arXiv:2310.19660
         [pdf, other] 
      
      
        cs.CL
        
      
    
    
    
      
        Interpretable-by-Design Text Classification with Iteratively Generated Concept Bottleneck
      
    
    
      Authors:
      
      Josh Magnus Ludan, 
      
      Qing Lyu, 
      
      Yue Yang, 
      
      Liam Dugan, 
      
      Mark Yatskar, 
      
      Chris Callison-Burch
      
    
    
    
      Abstract:
      
        …sparse set of salient concepts and use a linear layer over those concept values to produce the final prediction. These concepts can be automatically discovered and measured by a Large Language Model (LLM), without the need for human curation. On 12 diverse datasets, using GPT-4 f…
        ▽ More
      
      
        Deep neural networks excel in text classification tasks, yet their application in high-stakes domains is hindered by their lack of interpretability. To address this, we propose Text Bottleneck Models (TBMs), an intrinsically interpretable text classification framework that offers both global and local explanations. Rather than directly predicting the output label, TBMs predict categorical values for a sparse set of salient concepts and use a linear layer over those concept values to produce the final prediction. These concepts can be automatically discovered and measured by a Large Language Model (LLM), without the need for human curation. On 12 diverse datasets, using GPT-4 for both concept generation and measurement, we show that TBMs can rival the performance of established black-box baselines such as GPT-4 fewshot and finetuned DeBERTa, while falling short against finetuned GPT-3.5. Overall, our findings suggest that TBMs are a promising new framework that enhances interpretability, with minimal performance tradeoffs, particularly for general-domain text.
        △ Less
      
    
    

    Submitted 30 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.19658
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.AI
          
        
      
    
    
    
      
        Explaining Tree Model Decisions in Natural Language for Network Intrusion Detection
      
    
    
      Authors:
      
      Noah Ziems, 
      
      Gang Liu, 
      
      John Flanagan, 
      
      Meng Jiang
      
    
    
    
      Abstract:
      
        …In addition, they are unable to provide additional outside information as to why certain features may be important for classification.
  In this work, we explore the use of large language models (LLMs) to provide explanations and additional background knowledge for decision tree…
        ▽ More
      
      
        Network intrusion detection (NID) systems which leverage machine learning have been shown to have strong performance in practice when used to detect malicious network traffic. Decision trees in particular offer a strong balance between performance and simplicity, but require users of NID systems to have background knowledge in machine learning to interpret. In addition, they are unable to provide additional outside information as to why certain features may be important for classification.
  In this work, we explore the use of large language models (LLMs) to provide explanations and additional background knowledge for decision tree NID systems. Further, we introduce a new human evaluation framework for decision tree explanations, which leverages automatically generated quiz questions that measure human evaluators' understanding of decision tree inference. Finally, we show LLM generated decision tree explanations correlate highly with human ratings of readability, quality, and use of background knowledge while simultaneously providing better understanding of decision boundaries.
        △ Less
      
    
    

    Submitted 30 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      Accepted to NeurIPS XAIA Workshop 2023
    
    

    

    
  

  
    
      arXiv:2310.19651
         [pdf, other] 
      
      
        cs.CL
        
      
    
    
    
      
        Dynamics of Instruction Tuning: Each Ability of Large Language Models Has Its Own Growth Pace
      
    
    
      Authors:
      
      Chiyu Song, 
      
      Zhanchao Zhou, 
      
      Jianhao Yan, 
      
      Yuejiao Fei, 
      
      Zhenzhong Lan, 
      
      Yue Zhang
      
    
    
    
      Abstract:
      
        Instruction tuning is a burgeoning method to elicit the general intelligence of Large Language Models (LLMs). However, the creation of instruction data is still largely heuristic, leading to significant variation in quality and distribution across existing datasets. Experimental…
        ▽ More
      
      
        Instruction tuning is a burgeoning method to elicit the general intelligence of Large Language Models (LLMs). However, the creation of instruction data is still largely heuristic, leading to significant variation in quality and distribution across existing datasets. Experimental conclusions drawn from these datasets are also inconsistent, with some studies emphasizing the importance of scaling instruction numbers, while others argue that a limited number of samples suffice. To better understand data construction guidelines, we deepen our focus from the overall model performance to the growth of each underlying ability, such as creative writing, code generation, and logical reasoning. We systematically investigate the effects of data volume, parameter size, and data construction methods on the development of various abilities, using hundreds of model checkpoints (7b to 33b) fully instruction-tuned on a new collection of over 40k human-curated instruction data. This proposed dataset is stringently quality-controlled and categorized into ten distinct LLM abilities. Our study reveals three primary findings: (i) Despite data volume and parameter scale directly impacting models' overall performance, some abilities are more responsive to their increases and can be effectively trained using limited data, while some are highly resistant to these changes. (ii) Human-curated data strongly outperforms synthetic data from GPT-4 in efficiency and can constantly enhance model performance with volume increases, but is unachievable with synthetic data. (iii) Instruction data brings powerful cross-ability generalization, with evaluation results on out-of-domain data mirroring the first two observations. Furthermore, we demonstrate how these findings can guide more efficient data constructions, leading to practical performance improvements on public benchmarks.
        △ Less
      
    
    

    Submitted 30 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.19626
         [pdf, other] 
      
      
        cs.AI
        
      
    
    
    
      
        Transformation vs Tradition: Artificial General Intelligence (AGI) for Arts and Humanities
      
    
    
      Authors:
      
      Zhengliang Liu, 
      
      Yiwei Li, 
      
      Qian Cao, 
      
      Junwen Chen, 
      
      Tianze Yang, 
      
      Zihao Wu, 
      
      John Hale, 
      
      John Gibbs, 
      
      Khaled Rasheed, 
      
      Ninghao Liu, 
      
      Gengchen Mai, 
      
      Tianming Liu
      
    
    
    
      Abstract:
      
        Recent advances in artificial general intelligence (AGI), particularly large language models and creative image generation systems have demonstrated impressive capabilities on diverse tasks spanning the arts and humanities. However, the swift evolution of AGI has also raised crit…
        ▽ More
      
      
        Recent advances in artificial general intelligence (AGI), particularly large language models and creative image generation systems have demonstrated impressive capabilities on diverse tasks spanning the arts and humanities. However, the swift evolution of AGI has also raised critical questions about its responsible deployment in these culturally significant domains traditionally seen as profoundly human. This paper provides a comprehensive analysis of the applications and implications of AGI for text, graphics, audio, and video pertaining to arts and the humanities. We survey cutting-edge systems and their usage in areas ranging from poetry to history, marketing to film, and communication to classical art. We outline substantial concerns pertaining to factuality, toxicity, biases, and public safety in AGI systems, and propose mitigation strategies. The paper argues for multi-stakeholder collaboration to ensure AGI promotes creativity, knowledge, and cultural values without undermining truth or human dignity. Our timely contribution summarizes a rapidly developing field, highlighting promising directions while advocating for responsible progress centering on human flourishing. The analysis lays the groundwork for further research on aligning AGI's technological capacities with enduring social goods.
        △ Less
      
    
    

    Submitted 30 October, 2023; 
      originally announced October 2023.
      
    
    

    
      
        

        

        
          ACM Class:
          J.5; I.2.7; I.2.10
        
      
    

    
  

  
    
      arXiv:2310.19620
         [pdf, other] 
      
      
        cs.RO
        
          
            cs.AI
          
            cs.CV
          
        
      
    
    
    
      
        Large Trajectory Models are Scalable Motion Predictors and Planners
      
    
    
      Authors:
      
      Qiao Sun, 
      
      Shiduo Zhang, 
      
      Danjiao Ma, 
      
      Jingzhe Shi, 
      
      Derun Li, 
      
      Simian Luo, 
      
      Yu Wang, 
      
      Ningyi Xu, 
      
      Guangzhi Cao, 
      
      Hang Zhao
      
    
    
    
      Abstract:
      
        …reasoning traffic dynamics over a long time horizon, interpreting heterogeneous behaviors, and generating policies in a large continuous state space. Inspired by the success of large language models in addressing similar complexities through model scaling, we introduce a scalabl…
        ▽ More
      
      
        Motion prediction and planning are vital tasks in autonomous driving, and recent efforts have shifted to machine learning-based approaches. The challenges include understanding diverse road topologies, reasoning traffic dynamics over a long time horizon, interpreting heterogeneous behaviors, and generating policies in a large continuous state space. Inspired by the success of large language models in addressing similar complexities through model scaling, we introduce a scalable trajectory model called State Transformer (STR). STR reformulates the motion prediction and motion planning problems by arranging observations, states, and actions into one unified sequence modeling task. With a simple model design, STR consistently outperforms baseline approaches in both problems. Remarkably, experimental results reveal that large trajectory models (LTMs), such as STR, adhere to the scaling laws by presenting outstanding adaptability and learning efficiency. Qualitative results further demonstrate that LTMs are capable of making plausible predictions in scenarios that diverge significantly from the training data distribution. LTMs also learn to make complex reasonings for long-term planning, without explicit loss designs or costly high-level annotations.
        △ Less
      
    
    

    Submitted 30 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.19619
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.AI
          
        
      
    
    
    
      
        Towards A Holistic Landscape of Situated Theory of Mind in Large Language Models
      
    
    
      Authors:
      
      Ziqiao Ma, 
      
      Jacob Sansom, 
      
      Run Peng, 
      
      Joyce Chai
      
    
    
    
      Abstract:
      
        Large Language Models (LLMs) have generated considerable interest and debate regarding their potential emergence of Theory of Mind (ToM). Several recent inquiries reveal a lack of robust ToM in these models and pose a pressing demand to develop new benchmarks, as current ones pri…
        ▽ More
      
      
        Large Language Models (LLMs) have generated considerable interest and debate regarding their potential emergence of Theory of Mind (ToM). Several recent inquiries reveal a lack of robust ToM in these models and pose a pressing demand to develop new benchmarks, as current ones primarily focus on different aspects of ToM and are prone to shortcuts and data leakage. In this position paper, we seek to answer two road-blocking questions: (1) How can we taxonomize a holistic landscape of machine ToM? (2) What is a more effective evaluation protocol for machine ToM? Following psychological studies, we taxonomize machine ToM into 7 mental state categories and delineate existing benchmarks to identify under-explored aspects of ToM. We argue for a holistic and situated evaluation of ToM to break ToM into individual components and treat LLMs as an agent who is physically situated in environments and socially situated in interactions with humans. Such situated evaluation provides a more comprehensive assessment of mental states and potentially mitigates the risk of shortcuts and data leakage. We further present a pilot study in a grid world setup as a proof of concept. We hope this position paper can facilitate future research to integrate ToM with LLMs and offer an intuitive means for researchers to better position their work in the landscape of ToM. Project page: https://github.com/Mars-tin/awesome-theory-of-mind
        △ Less
      
    
    

    Submitted 30 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      Theme Track, Findings of EMNLP 2023
    
    

    

    
  

  
    
      arXiv:2310.19596
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.AI
          
        
      
    
    
    
      
        LLMaAA: Making Large Language Models as Active Annotators
      
    
    
      Authors:
      
      Ruoyu Zhang, 
      
      Yanzeng Li, 
      
      Yongliang Ma, 
      
      Ming Zhou, 
      
      Lei Zou
      
    
    
    
      Abstract:
      
        …data-hungry, which demand large amounts of high-quality annotated data. In practice, acquiring such data is a costly endeavor. Recently, the superior few-shot performance of large language models (LLMs) has propelled the development of dataset generation, where the training data…
        ▽ More
      
      
        Prevalent supervised learning methods in natural language processing (NLP) are notoriously data-hungry, which demand large amounts of high-quality annotated data. In practice, acquiring such data is a costly endeavor. Recently, the superior few-shot performance of large language models (LLMs) has propelled the development of dataset generation, where the training data are solely synthesized from LLMs. However, such an approach usually suffers from low-quality issues, and requires orders of magnitude more labeled data to achieve satisfactory performance. To fully exploit the potential of LLMs and make use of massive unlabeled data, we propose LLMaAA, which takes LLMs as annotators and puts them into an active learning loop to determine what to annotate efficiently. To learn robustly with pseudo labels, we optimize both the annotation and training processes: (1) we draw k-NN examples from a small demonstration pool as in-context examples, and (2) we adopt the example reweighting technique to assign training samples with learnable weights. Compared with previous approaches, LLMaAA features both efficiency and reliability. We conduct experiments and analysis on two classic NLP tasks, named entity recognition and relation extraction. With LLMaAA, task-specific models trained from LLM-generated labels can outperform the teacher within only hundreds of annotated examples, which is much more cost-effective than other baselines.
        △ Less
      
    
    

    Submitted 31 October, 2023; v1 submitted 30 October, 2023;
      originally announced October 2023.
      
    
    
    
      Comments:
      Findings of EMNLP 2023 camera ready
    
    

    

    
  

  
    
      arXiv:2310.19572
         [pdf, other] 
      
      
        cs.CL
        
      
    
    
    
      
        Improving Input-label Mapping with Demonstration Replay for In-context Learning
      
    
    
      Authors:
      
      Zhuocheng Gong, 
      
      Jiahao Liu, 
      
      Qifan Wang, 
      
      Jingang Wang, 
      
      Xunliang Cai, 
      
      Dongyan Zhao, 
      
      Rui Yan
      
    
    
    
      Abstract:
      
        …of downstream NLP tasks, without directly adjusting the model parameters. The effectiveness of ICL can be attributed to the strong language modeling capabilities of large language models (LLMs), which enable them to learn the mapping between input and labels based on in-context d…
        ▽ More
      
      
        In-context learning (ICL) is an emerging capability of large autoregressive language models where a few input-label demonstrations are appended to the input to enhance the model's understanding of downstream NLP tasks, without directly adjusting the model parameters. The effectiveness of ICL can be attributed to the strong language modeling capabilities of large language models (LLMs), which enable them to learn the mapping between input and labels based on in-context demonstrations. Despite achieving promising results, the causal nature of language modeling in ICL restricts the attention to be backward only, i.e., a token only attends to its previous tokens, failing to capture the full input-label information and limiting the model's performance. In this paper, we propose a novel ICL method called Repeated Demonstration with Sliding Causal Attention, (RdSca). Specifically, we duplicate later demonstrations and concatenate them to the front, allowing the model to `observe' the later information even under the causal restriction. Besides, we introduce sliding causal attention, which customizes causal attention to avoid information leakage. Experimental results show that our method significantly improves the input-label mapping in ICL demonstrations. We also conduct an in-depth analysis of how to customize the causal attention without training, which has been an unexplored area in previous research.
        △ Less
      
    
    

    Submitted 30 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.19503
         [pdf, other] 
      
      
        cs.AI
        
          
            cs.CY
          
            cs.MA
          
        
      
    
    
    
      
        Trust, Accountability, and Autonomy in Knowledge Graph-based AI for Self-determination
      
    
    
      Authors:
      
      Luis-Daniel Ibáñez, 
      
      John Domingue, 
      
      Sabrina Kirrane, 
      
      Oshani Seneviratne, 
      
      Aisling Third, 
      
      Maria-Esther Vidal
      
    
    
    
      Abstract:
      
        …algorithms by providing data context and semantics, thereby enabling further inference and question-answering capabilities. The integration of KGs with neuronal learning (e.g., Large Language Models (LLMs)) is currently a topic of active research, commonly named neuro-symbolic AI…
        ▽ More
      
      
        Knowledge Graphs (KGs) have emerged as fundamental platforms for powering intelligent decision-making and a wide range of Artificial Intelligence (AI) services across major corporations such as Google, Walmart, and AirBnb. KGs complement Machine Learning (ML) algorithms by providing data context and semantics, thereby enabling further inference and question-answering capabilities. The integration of KGs with neuronal learning (e.g., Large Language Models (LLMs)) is currently a topic of active research, commonly named neuro-symbolic AI. Despite the numerous benefits that can be accomplished with KG-based AI, its growing ubiquity within online services may result in the loss of self-determination for citizens as a fundamental societal issue. The more we rely on these technologies, which are often centralised, the less citizens will be able to determine their own destinies. To counter this threat, AI regulation, such as the European Union (EU) AI Act, is being proposed in certain regions. The regulation sets what technologists need to do, leading to questions concerning: How can the output of AI systems be trusted? What is needed to ensure that the data fuelling and the inner workings of these artefacts are transparent? How can AI be made accountable for its decision-making? This paper conceptualises the foundational topics and research pillars to support KG-based AI for self-determination. Drawing upon this conceptual framework, challenges and opportunities for citizen self-determination are illustrated and analysed in a real-world scenario. As a result, we propose a research agenda aimed at accomplishing the recommended objectives.
        △ Less
      
    
    

    Submitted 31 October, 2023; v1 submitted 30 October, 2023;
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.19488
         [pdf, other] 
      
      
        cs.IR
        
      
    
    
    
      
        CoLLM: Integrating Collaborative Embeddings into Large Language Models for Recommendation
      
    
    
      Authors:
      
      Yang Zhang, 
      
      Fuli Feng, 
      
      Jizhi Zhang, 
      
      Keqin Bao, 
      
      Qifan Wang, 
      
      Xiangnan He
      
    
    
    
      Abstract:
      
        Leveraging Large Language Models as Recommenders (LLMRec) has gained significant attention and introduced fresh perspectives in user preference modeling. Existing LLMRec approaches prioritize text semantics, usually neglecting the valuable collaborative information from user-item…
        ▽ More
      
      
        Leveraging Large Language Models as Recommenders (LLMRec) has gained significant attention and introduced fresh perspectives in user preference modeling. Existing LLMRec approaches prioritize text semantics, usually neglecting the valuable collaborative information from user-item interactions in recommendations. While these text-emphasizing approaches excel in cold-start scenarios, they may yield sub-optimal performance in warm-start situations. In pursuit of superior recommendations for both cold and warm start scenarios, we introduce CoLLM, an innovative LLMRec methodology that seamlessly incorporates collaborative information into LLMs for recommendation. CoLLM captures collaborative information through an external traditional model and maps it to the input token embedding space of LLM, forming collaborative embeddings for LLM usage. Through this external integration of collaborative information, CoLLM ensures effective modeling of collaborative information without modifying the LLM itself, providing the flexibility to employ various collaborative information modeling techniques. Extensive experiments validate that CoLLM adeptly integrates collaborative information into LLMs, resulting in enhanced recommendation performance. We release the code and data at https://github.com/zyang1580/CoLLM.
        △ Less
      
    
    

    Submitted 30 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.19462
         [pdf, other] 
      
      
        cs.CL
        
      
    
    
    
      
        Constituency Parsing using LLMs
      
    
    
      Authors:
      
      Xuefeng Bai, 
      
      Jialong Wu, 
      
      Yulong Chen, 
      
      Zhongqing Wang, 
      
      Yue Zhang
      
    
    
    
      Abstract:
      
        Constituency parsing is a fundamental yet unsolved natural language processing task. In this paper, we explore the potential of recent large language models (LLMs) that have exhibited remarkable performance across various domains and tasks to tackle this task. We employ three lin…
        ▽ More
      
      
        Constituency parsing is a fundamental yet unsolved natural language processing task. In this paper, we explore the potential of recent large language models (LLMs) that have exhibited remarkable performance across various domains and tasks to tackle this task. We employ three linearization strategies to transform output trees into symbol sequences, such that LLMs can solve constituency parsing by generating linearized trees. We conduct experiments using a diverse range of LLMs, including ChatGPT, GPT-4, OPT, LLaMA, and Alpaca, comparing their performance against the state-of-the-art constituency parsers. Our experiments encompass zero-shot, few-shot, and full-training learning settings, and we evaluate the models on one in-domain and five out-of-domain test datasets. Our findings reveal insights into LLMs' performance, generalization abilities, and challenges in constituency parsing.
        △ Less
      
    
    

    Submitted 31 October, 2023; v1 submitted 30 October, 2023;
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.19347
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.AI
          
        
      
    
    
    
      
        Improving Factual Consistency of Text Summarization by Adversarially Decoupling Comprehension and Embellishment Abilities of LLMs
      
    
    
      Authors:
      
      Huawen Feng, 
      
      Yan Fan, 
      
      Xiong Liu, 
      
      Ting-En Lin, 
      
      Zekun Yao, 
      
      Yuchuan Wu, 
      
      Fei Huang, 
      
      Yongbin Li, 
      
      Qianli Ma
      
    
    
    
      Abstract:
      
        Despite the recent progress in text summarization made by large language models (LLMs), they often generate summaries that are factually inconsistent with original articles, known as ""hallucinations"" in text generation. Unlike previous small models (e.g., BART, T5), curre…
        ▽ More
      
      
        Despite the recent progress in text summarization made by large language models (LLMs), they often generate summaries that are factually inconsistent with original articles, known as ""hallucinations"" in text generation. Unlike previous small models (e.g., BART, T5), current LLMs make fewer silly mistakes but more sophisticated ones, such as imposing cause and effect, adding false details, and overgeneralizing, etc. These hallucinations are challenging to detect through traditional methods, which poses great challenges for improving the factual consistency of text summarization. In this paper, we propose an adversarially DEcoupling method to disentangle the Comprehension and EmbellishmeNT abilities of LLMs (DECENT). Furthermore, we adopt a probing-based parameter-efficient technique to cover the shortage of sensitivity for true and false in the training process of LLMs. In this way, LLMs are less confused about embellishing and understanding, thus can execute the instructions more accurately and have enhanced abilities to distinguish hallucinations. Experimental results show that DECENT significantly improves the reliability of text summarization based on LLMs.
        △ Less
      
    
    

    Submitted 30 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.19341
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.AI
          
        
      
    
    
    
      
        Skywork: A More Open Bilingual Foundation Model
      
    
    
      Authors:
      
      Tianwen Wei, 
      
      Liang Zhao, 
      
      Lichang Zhang, 
      
      Bo Zhu, 
      
      Lijie Wang, 
      
      Haihua Yang, 
      
      Biye Li, 
      
      Cheng Cheng, 
      
      Weiwei Lü, 
      
      Rui Hu, 
      
      Chenxia Li, 
      
      Liu Yang, 
      
      Xilin Luo, 
      
      Xuejie Wu, 
      
      Lunan Liu, 
      
      Wenjun Cheng, 
      
      Peng Cheng, 
      
      Jianhao Zhang, 
      
      Xiaoyu Zhang, 
      
      Lei Lin, 
      
      Xiaokun Wang, 
      
      Yutuan Ma, 
      
      Chuanhai Dong, 
      
      Yanqi Sun, 
      
      Yifu Chen
      , et al. (5 additional authors not shown)
    
    
    
      Abstract:
      
        In this technical report, we present Skywork-13B, a family of large language models (LLMs) trained on a corpus of over 3.2 trillion tokens drawn from both English and Chinese texts. This bilingual foundation model is the most extensively trained and openly published LLMs of compa…
        ▽ More
      
      
        In this technical report, we present Skywork-13B, a family of large language models (LLMs) trained on a corpus of over 3.2 trillion tokens drawn from both English and Chinese texts. This bilingual foundation model is the most extensively trained and openly published LLMs of comparable size to date. We introduce a two-stage training methodology using a segmented corpus, targeting general purpose training and then domain-specific enhancement training, respectively. We show that our model not only excels on popular benchmarks, but also achieves \emph{state of the art} performance in Chinese language modeling on diverse domains. Furthermore, we propose a novel leakage detection method, demonstrating that test data contamination is a pressing issue warranting further investigation by the LLM community. To spur future research, we release Skywork-13B along with checkpoints obtained during intermediate stages of the training process. We are also releasing part of our SkyPile corpus, a collection of over 150 billion tokens of web text, which is the largest high quality open Chinese pre-training corpus to date. We hope Skywork-13B and our open corpus will serve as a valuable open-source resource to democratize access to high-quality LLMs.
        △ Less
      
    
    

    Submitted 30 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.19334
         [pdf] 
      
      
        cs.CY
        
      
    
    
    
      
        Scalable Two-Minute Feedback: Digital, Lecture-Accompanying Survey as a Continuous Feedback Instrument
      
    
    
      Authors:
      
      Armin Egetenmeier, 
      
      Sven Strickroth
      
    
    
    
      Abstract:
      
        …the lecture content or organizational aspects and were intensively used to report issues within the lecture. In addition, artificial intelligence (AI) support in the form of a large language model was tested and showed promising results in summarizing the open-ended responses for…
        ▽ More
      
      
        Detailed feedback on courses and lecture content is essential for their improvement and also serves as a tool for reflection. However, feedback methods are often only used sporadically, especially in mass courses, because collecting and analyzing feedback in a timely manner is often a challenge for teachers. Moreover, the current situation of the students or the changing workload during the semester are usually not taken into account either. For a holistic investigation, the article used a digital survey format as formative feedback which attempts to measure student stress in a quantitative part and to address the participants' reflection in a qualitative part, as well as to collect general suggestions for improvement (based on the so-called One-Minute Paper) at two educational institutions. The feedback during the semester is evaluated qualitatively and discussed on a meta-level and special features (e.g. reflections on student work ethic or other courses) are addressed. The results show a low, but constant rate of feedback. Responses mostly cover topics of the lecture content or organizational aspects and were intensively used to report issues within the lecture. In addition, artificial intelligence (AI) support in the form of a large language model was tested and showed promising results in summarizing the open-ended responses for the teacher. Finally, the experiences from the lecturers are reflected upon and the results as well as possibilities for improvement are discussed.
        △ Less
      
    
    

    Submitted 30 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      accepted to the International Journal of Information and Education Technology (IJIET)
    
    

    

    
  

  
    
      arXiv:2310.19292
         [pdf, other] 
      
      
        cs.CL
        
      
    
    
    
      
        Fusing Temporal Graphs into Transformers for Time-Sensitive Question Answering
      
    
    
      Authors:
      
      Xin Su, 
      
      Phillip Howard, 
      
      Nagib Hakim, 
      
      Steven Bethard
      
    
    
    
      Abstract:
      
        Answering time-sensitive questions from long documents requires temporal reasoning over the times in questions and documents. An important open question is whether large language models can perform such reasoning solely using a provided text document, or whether they can benefit…
        ▽ More
      
      
        Answering time-sensitive questions from long documents requires temporal reasoning over the times in questions and documents. An important open question is whether large language models can perform such reasoning solely using a provided text document, or whether they can benefit from additional temporal information extracted using other systems. We address this research question by applying existing temporal information extraction systems to construct temporal graphs of events, times, and temporal relations in questions and documents. We then investigate different approaches for fusing these graphs into Transformer models. Experimental results show that our proposed approach for fusing temporal graphs into input text substantially enhances the temporal reasoning capabilities of Transformer models with or without fine-tuning. Additionally, our proposed method outperforms various graph convolution-based approaches and establishes a new state-of-the-art performance on SituatedQA and three splits of TimeQA.
        △ Less
      
    
    

    Submitted 30 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      EMNLP 2023 Findings
    
    

    

    
  

  
    
      arXiv:2310.19275
         [pdf, other] 
      
      
        cs.HC
        
      
    
    
    
      
        Eliciting Topic Hierarchies from Large Language Models
      
    
    
      Authors:
      
      Grace Li, 
      
      Tao Long, 
      
      Lydia B. Chilton
      
    
    
    
      Abstract:
      
        …topics to write about can be a mentally demanding process. However, topic hierarchies can help writers explore topics of varying levels of specificity. In this paper, we use large language models (LLMs) to help construct topic hierarchies. Although LLMs have access to such knowle…
        ▽ More
      
      
        Finding topics to write about can be a mentally demanding process. However, topic hierarchies can help writers explore topics of varying levels of specificity. In this paper, we use large language models (LLMs) to help construct topic hierarchies. Although LLMs have access to such knowledge, it can be difficult to elicit due to issues of specificity, scope, and repetition. We designed and tested three different prompting techniques to find one that maximized accuracy. We found that prepending the general topic area to a prompt yielded the most accurate results with 85% accuracy. We discuss applications of this research including STEM writing, education, and content creation.
        △ Less
      
    
    

    Submitted 30 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      4 pages, 4 figures
    
    

    

    
  

  
    
      arXiv:2310.19240
         [pdf, other] 
      
      
        cs.CL
        
      
    
    
    
      
        M4LE: A Multi-Ability Multi-Range Multi-Task Multi-Domain Long-Context Evaluation Benchmark for Large Language Models
      
    
    
      Authors:
      
      Wai-Chung Kwan, 
      
      Xingshan Zeng, 
      
      Yufei Wang, 
      
      Yusen Sun, 
      
      Liangyou Li, 
      
      Lifeng Shang, 
      
      Qun Liu, 
      
      Kam-Fai Wong
      
    
    
    
      Abstract:
      
        Managing long sequences has become an important and necessary feature for large language models (LLMs). However, it is still an open question of how to comprehensively and systematically evaluate the long-sequence capability of LLMs. One of the reasons is that conventional and wi…
        ▽ More
      
      
        Managing long sequences has become an important and necessary feature for large language models (LLMs). However, it is still an open question of how to comprehensively and systematically evaluate the long-sequence capability of LLMs. One of the reasons is that conventional and widely-used benchmarks mainly consist of short sequences. In this paper, we propose M4LE, a Multi-ability, Multi-range, Multi-task, Multi-domain benchmark for Long-context Evaluation. M4LE is based on a diverse NLP task pool comprising 36 NLP datasets, 11 task types and 12 domains. To alleviate the scarcity of tasks with naturally long sequences and incorporate multiple-ability assessment, we propose an automatic approach (but with negligible human annotations) to convert short-sequence tasks into a unified long-sequence scenario where LLMs have to identify single or multiple relevant spans in long contexts based on explicit or semantic hints. Specifically, the scenario includes five different types of abilities: (1) explicit single-span; (2) semantic single-span; (3) explicit multiple-span; (4) semantic multiple-span; and (5) global context understanding. The resulting samples in M4LE are evenly distributed from 1k to 8k input length. We conducted a systematic evaluation on 11 well-established LLMs, especially those optimized for long-sequence inputs. Our results reveal that: 1) Current LLMs struggle to understand long context, particularly when tasks require multiple-span attention. 2) Semantic retrieval task is more difficult for competent LLMs. 3) Models fine-tuned on longer text with position interpolation have comparable performance to those using Neural Tangent Kernel (NTK) aware scaling methods without fine-tuning. We make our benchmark publicly available to encourage future research in this challenging area.
        △ Less
      
    
    

    Submitted 29 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      Code and data are available at https://github.com/KwanWaiChung/M4LE
    
    

    

    
  

  
    
      arXiv:2310.19233
         [pdf, other] 
      
      
        cs.CL
        
      
    
    
    
      
        Building Real-World Meeting Summarization Systems using Large Language Models: A Practical Perspective
      
    
    
      Authors:
      
      Md Tahmid Rahman Laskar, 
      
      Xue-Yong Fu, 
      
      Cheng Chen, 
      
      Shashi Bhushan TN
      
    
    
    
      Abstract:
      
        This paper studies how to effectively build meeting summarization systems for real-world usage using large language models (LLMs). For this purpose, we conduct an extensive evaluation and comparison of various closed-source and open-source LLMs, namely, GPT-4, GPT- 3.5, PaLM-2, a…
        ▽ More
      
      
        This paper studies how to effectively build meeting summarization systems for real-world usage using large language models (LLMs). For this purpose, we conduct an extensive evaluation and comparison of various closed-source and open-source LLMs, namely, GPT-4, GPT- 3.5, PaLM-2, and LLaMA-2. Our findings reveal that most closed-source LLMs are generally better in terms of performance. However, much smaller open-source models like LLaMA- 2 (7B and 13B) could still achieve performance comparable to the large closed-source models even in zero-shot scenarios. Considering the privacy concerns of closed-source models for only being accessible via API, alongside the high cost associated with using fine-tuned versions of the closed-source models, the opensource models that can achieve competitive performance are more advantageous for industrial use. Balancing performance with associated costs and privacy concerns, the LLaMA-2-7B model looks more promising for industrial usage. In sum, this paper offers practical insights on using LLMs for real-world business meeting summarization, shedding light on the trade-offs between performance and cost.
        △ Less
      
    
    

    Submitted 29 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      EMNLP 2023 Industry Track
    
    

    

    
  

  
    
      arXiv:2310.19212
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.AI
          
        
      
    
    
    
      
        EHRTutor: Enhancing Patient Understanding of Discharge Instructions
      
    
    
      Authors:
      
      Zihao Zhang, 
      
      Zonghai Yao, 
      
      Huixue Zhou, 
      
      Feiyun ouyang, 
      
      Hong Yu
      
    
    
    
      Abstract:
      
        Large…
        ▽ More
      
      
        Large language models have shown success as a tutor in education in various fields. Educating patients about their clinical visits plays a pivotal role in patients' adherence to their treatment plans post-discharge. This paper presents EHRTutor, an innovative multi-component framework leveraging the Large Language Model (LLM) for patient education through conversational question-answering. EHRTutor first formulates questions pertaining to the electronic health record discharge instructions. It then educates the patient through conversation by administering each question as a test. Finally, it generates a summary at the end of the conversation. Evaluation results using LLMs and domain experts have shown a clear preference for EHRTutor over the baseline. Moreover, EHRTutor also offers a framework for generating synthetic patient education dialogues that can be used for future in-house system training.
        △ Less
      
    
    

    Submitted 29 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      To appear in NeurIPS'23 Workshop on Generative AI for Education (GAIED)
    
    

    

    
  

  
    
      arXiv:2310.19206
         [pdf, other] 
      
      
        cs.AI
        
      
    
    
    
      
        Leveraging generative artificial intelligence to simulate student learning behavior
      
    
    
      Authors:
      
      Songlin Xu, 
      
      Xinyu Zhang
      
    
    
    
      Abstract:
      
        …a transformative approach to enhance learning outcomes, advance educational research, and ultimately shape the future of effective pedagogy. We explore the feasibility of using large language models (LLMs), a remarkable achievement in AI, to simulate student learning behaviors. U…
        ▽ More
      
      
        Student simulation presents a transformative approach to enhance learning outcomes, advance educational research, and ultimately shape the future of effective pedagogy. We explore the feasibility of using large language models (LLMs), a remarkable achievement in AI, to simulate student learning behaviors. Unlike conventional machine learning based prediction, we leverage LLMs to instantiate virtual students with specific demographics and uncover intricate correlations among learning experiences, course materials, understanding levels, and engagement. Our objective is not merely to predict learning outcomes but to replicate learning behaviors and patterns of real students. We validate this hypothesis through three experiments. The first experiment, based on a dataset of N = 145, simulates student learning outcomes from demographic data, revealing parallels with actual students concerning various demographic factors. The second experiment (N = 4524) results in increasingly realistic simulated behaviors with more assessment history for virtual students modelling. The third experiment (N = 27), incorporating prior knowledge and course interactions, indicates a strong link between virtual students' learning behaviors and fine-grained mappings from test questions, course materials, engagement and understanding levels. Collectively, these findings deepen our understanding of LLMs and demonstrate its viability for student simulation, empowering more adaptable curricula design to enhance inclusivity and educational effectiveness.
        △ Less
      
    
    

    Submitted 29 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.19181
         [pdf, other] 
      
      
        cs.CR
        
          
            cs.CL
          
        
      
    
    
    
      
        From Chatbots to PhishBots? -- Preventing Phishing scams created using ChatGPT, Google Bard and Claude
      
    
    
      Authors:
      
      Sayak Saha Roy, 
      
      Poojitha Thota, 
      
      Krishna Vamsi Naragam, 
      
      Shirin Nilizadeh
      
    
    
    
      Abstract:
      
        The advanced capabilities of Large Language Models (LLMs) have made them invaluable across various applications, from conversational agents and content creation to data analysis, research, and innovation. However, their effectiveness and accessibility also render them susceptible…
        ▽ More
      
      
        The advanced capabilities of Large Language Models (LLMs) have made them invaluable across various applications, from conversational agents and content creation to data analysis, research, and innovation. However, their effectiveness and accessibility also render them susceptible to abuse for generating malicious content, including phishing attacks. This study explores the potential of using four popular commercially available LLMs - ChatGPT (GPT 3.5 Turbo), GPT 4, Claude and Bard to generate functional phishing attacks using a series of malicious prompts. We discover that these LLMs can generate both phishing emails and websites that can convincingly imitate well-known brands, and also deploy a range of evasive tactics for the latter to elude detection mechanisms employed by anti-phishing systems. Notably, these attacks can be generated using unmodified, or ""vanilla,"" versions of these LLMs, without requiring any prior adversarial exploits such as jailbreaking. As a countermeasure, we build a BERT based automated detection tool that can be used for the early detection of malicious prompts to prevent LLMs from generating phishing content attaining an accuracy of 97\% for phishing website prompts, and 94\% for phishing email prompts.
        △ Less
      
    
    

    Submitted 29 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.19102
         [pdf, other] 
      
      
        cs.LG
        
      
    
    
    
      
        Atom: Low-bit Quantization for Efficient and Accurate LLM Serving
      
    
    
      Authors:
      
      Yilong Zhao, 
      
      Chien-Yu Lin, 
      
      Kan Zhu, 
      
      Zihao Ye, 
      
      Lequn Chen, 
      
      Size Zheng, 
      
      Luis Ceze, 
      
      Arvind Krishnamurthy, 
      
      Tianqi Chen, 
      
      Baris Kasikci
      
    
    
    
      Abstract:
      
        The growing demand for Large Language Models (LLMs) in applications such as content generation, intelligent chatbots, and sentiment analysis poses considerable challenges for LLM service providers. To efficiently use GPU resources and boost throughput, batching multiple requests…
        ▽ More
      
      
        The growing demand for Large Language Models (LLMs) in applications such as content generation, intelligent chatbots, and sentiment analysis poses considerable challenges for LLM service providers. To efficiently use GPU resources and boost throughput, batching multiple requests has emerged as a popular paradigm; to further speed up batching, LLM quantization techniques reduce memory consumption and increase computing capacity. However, prevalent quantization schemes (e.g., 8-bit weight-activation quantization) cannot fully leverage the capabilities of modern GPUs, such as 4-bit integer operators, resulting in sub-optimal performance.
  To maximize LLMs' serving throughput, we introduce Atom, a low-bit quantization method that achieves high throughput improvements with negligible accuracy loss. Atom significantly boosts serving throughput by using low-bit operators and considerably reduces memory consumption via low-bit quantization. It attains high accuracy by applying a novel mixed-precision and fine-grained quantization process. We evaluate Atom on 4-bit weight-activation quantization setups in the serving context. Atom improves end-to-end throughput by up to 7.73× compared to the FP16 and by 2.53× compared to INT8 quantization, while maintaining the same latency target.
        △ Less
      
    
    

    Submitted 29 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.19084
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.AI
          
        
      
    
    
    
      
        Roles of Scaling and Instruction Tuning in Language Perception: Model vs. Human Attention
      
    
    
      Authors:
      
      Changjiang Gao, 
      
      Shujian Huang, 
      
      Jixing Li, 
      
      Jiajun Chen
      
    
    
    
      Abstract:
      
        Recent large language models (LLMs) have revealed strong abilities to understand natural language. Since most of them share the same basic structure, i.e. the transformer block, possible contributors to their success in the training process are scaling and instruction tuning. How…
        ▽ More
      
      
        Recent large language models (LLMs) have revealed strong abilities to understand natural language. Since most of them share the same basic structure, i.e. the transformer block, possible contributors to their success in the training process are scaling and instruction tuning. However, how these factors affect the models' language perception is unclear. This work compares the self-attention of several existing LLMs (LLaMA, Alpaca and Vicuna) in different sizes (7B, 13B, 30B, 65B), together with eye saccade, an aspect of human reading attention, to assess the effect of scaling and instruction tuning on language perception. Results show that scaling enhances the human resemblance and improves the effective attention by reducing the trivial pattern reliance, while instruction tuning does not. However, instruction tuning significantly enhances the models' sensitivity to instructions. We also find that current LLMs are consistently closer to non-native than native speakers in attention, suggesting a sub-optimal language perception of all models. Our code and data used in the analysis is available on GitHub.
        △ Less
      
    
    

    Submitted 29 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.19080
         [pdf, other] 
      
      
        cs.CV
        
      
    
    
    
      
        Reward Finetuning for Faster and More Accurate Unsupervised Object Discovery
      
    
    
      Authors:
      
      Katie Z Luo, 
      
      Zhenzhen Liu, 
      
      Xiangyu Chen, 
      
      Yurong You, 
      
      Sagie Benaim, 
      
      Cheng Perng Phoo, 
      
      Mark Campbell, 
      
      Wen Sun, 
      
      Bharath Hariharan, 
      
      Kilian Q. Weinberger
      
    
    
    
      Abstract:
      
        …have shown that Reinforcement Learning from Human Feedback (RLHF) can improve machine learning models and align them with human preferences. Although very successful for Large Language Models (LLMs), these advancements have not had a comparable impact in research for autonomous v…
        ▽ More
      
      
        Recent advances in machine learning have shown that Reinforcement Learning from Human Feedback (RLHF) can improve machine learning models and align them with human preferences. Although very successful for Large Language Models (LLMs), these advancements have not had a comparable impact in research for autonomous vehicles -- where alignment with human expectations can be imperative. In this paper, we propose to adapt similar RL-based methods to unsupervised object discovery, i.e. learning to detect objects from LiDAR points without any training labels. Instead of labels, we use simple heuristics to mimic human feedback. More explicitly, we combine multiple heuristics into a simple reward function that positively correlates its score with bounding box accuracy, \ie, boxes containing objects are scored higher than those without. We start from the detector's own predictions to explore the space and reinforce boxes with high rewards through gradient updates. Empirically, we demonstrate that our approach is not only more accurate, but also orders of magnitudes faster to train compared to prior works on object discovery.
        △ Less
      
    
    

    Submitted 29 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.19070
         [pdf, other] 
      
      
        cs.CV
        
      
    
    
    
      
        Myriad: Large Multimodal Model by Applying Vision Experts for Industrial Anomaly Detection
      
    
    
      Authors:
      
      Yuanze Li, 
      
      Haolin Wang, 
      
      Shihao Yuan, 
      
      Ming Liu, 
      
      Yiwen Guo, 
      
      Chen Xu, 
      
      Guangming Shi, 
      
      Wangmeng Zuo
      
    
    
    
      Abstract:
      
        …Specifically, we adopt MiniGPT-4 as the base LMM and design an Expert Perception module to embed the prior knowledge from vision experts as tokens which are intelligible to Large Language Models (LLMs). To compensate for the errors and confusions of vision experts, we introduce…
        ▽ More
      
      
        Existing industrial anomaly detection (IAD) methods predict anomaly scores for both anomaly detection and localization. However, they struggle to perform a multi-turn dialog and detailed descriptions for anomaly regions, e.g., color, shape, and categories of industrial anomalies. Recently, large multimodal (i.e., vision and language) models (LMMs) have shown eminent perception abilities on multiple vision tasks such as image captioning, visual understanding, visual reasoning, etc., making it a competitive potential choice for more comprehensible anomaly detection. However, the knowledge about anomaly detection is absent in existing general LMMs, while training a specific LMM for anomaly detection requires a tremendous amount of annotated data and massive computation resources. In this paper, we propose a novel large multi-modal model by applying vision experts for industrial anomaly detection (dubbed Myriad), which leads to definite anomaly detection and high-quality anomaly description. Specifically, we adopt MiniGPT-4 as the base LMM and design an Expert Perception module to embed the prior knowledge from vision experts as tokens which are intelligible to Large Language Models (LLMs). To compensate for the errors and confusions of vision experts, we introduce a domain adapter to bridge the visual representation gaps between generic and industrial images. Furthermore, we propose a Vision Expert Instructor, which enables the Q-Former to generate IAD domain vision-language tokens according to vision expert prior. Extensive experiments on MVTec-AD and VisA benchmarks demonstrate that our proposed method not only performs favorably against state-of-the-art methods under the 1-class and few-shot settings, but also provide definite anomaly prediction along with detailed descriptions in IAD domain.
        △ Less
      
    
    

    Submitted 29 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      8 pages, 7 figures
    
    

    

    
  

  
    
      arXiv:2310.19061
         [pdf, other] 
      
      
        cs.CV
        
      
    
    
    
      
        Multimodal ChatGPT for Medical Applications: an Experimental Study of GPT-4V
      
    
    
      Authors:
      
      Zhiling Yan, 
      
      Kai Zhang, 
      
      Rong Zhou, 
      
      Lifang He, 
      
      Xiang Li, 
      
      Lichao Sun
      
    
    
    
      Abstract:
      
        In this paper, we critically evaluate the capabilities of the state-of-the-art multimodal large language model, i.e., GPT-4 with Vision (GPT-4V), on Visual Question Answering (VQA) task. Our experiments thoroughly assess GPT-4V's proficiency in answering questions paired with…
        ▽ More
      
      
        In this paper, we critically evaluate the capabilities of the state-of-the-art multimodal large language model, i.e., GPT-4 with Vision (GPT-4V), on Visual Question Answering (VQA) task. Our experiments thoroughly assess GPT-4V's proficiency in answering questions paired with images using both pathology and radiology datasets from 11 modalities (e.g. Microscopy, Dermoscopy, X-ray, CT, etc.) and fifteen objects of interests (brain, liver, lung, etc.). Our datasets encompass a comprehensive range of medical inquiries, including sixteen distinct question types. Throughout our evaluations, we devised textual prompts for GPT-4V, directing it to synergize visual and textual information. The experiments with accuracy score conclude that the current version of GPT-4V is not recommended for real-world diagnostics due to its unreliable and suboptimal accuracy in responding to diagnostic medical questions. In addition, we delineate seven unique facets of GPT-4V's behavior in medical VQA, highlighting its constraints within this complex arena. The complete details of our evaluation cases are accessible at https://github.com/ZhilingYan/GPT4V-Medical-Report.
        △ Less
      
    
    

    Submitted 29 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.19056
         [pdf, other] 
      
      
        cs.IR
        
          
            cs.AI
          
            cs.CL
          
        
      
    
    
    
      
        MILL: Mutual Verification with Large Language Models for Zero-Shot Query Expansion
      
    
    
      Authors:
      
      Pengyue Jia, 
      
      Yiding Liu, 
      
      Xiangyu Zhao, 
      
      Xiaopeng Li, 
      
      Changying Hao, 
      
      Shuaiqiang Wang, 
      
      Dawei Yin
      
    
    
    
      Abstract:
      
        …methods, existing models can hardly be trained or aligned on a particular corpus, due to the lack of corpus-specific labeled data. In this paper, we propose a novel Large Language Model (LLM) based mutual verification framework for query expansion, which alleviates the aforementi…
        ▽ More
      
      
        Query expansion is a commonly-used technique in many search systems to better represent users' information needs with additional query terms. Existing studies for this task usually propose to expand a query with retrieved or generated contextual documents. However, both types of methods have clear limitations. For retrieval-based methods, the documents retrieved with the original query might not be accurate enough to reveal the search intent, especially when the query is brief or ambiguous. For generation-based methods, existing models can hardly be trained or aligned on a particular corpus, due to the lack of corpus-specific labeled data. In this paper, we propose a novel Large Language Model (LLM) based mutual verification framework for query expansion, which alleviates the aforementioned limitations. Specifically, we first design a query-query-document generation pipeline, which can effectively leverage the contextual knowledge encoded in LLMs to generate sub-queries and corresponding documents from multiple perspectives. Next, we employ a mutual verification method for both generated and retrieved contextual documents, where 1) retrieved documents are filtered with the external contextual knowledge in generated documents, and 2) generated documents are filtered with the corpus-specific knowledge in retrieved documents. Overall, the proposed method allows retrieved and generated documents to complement each other to finalize a better query expansion. We conduct extensive experiments on three information retrieval datasets, i.e., TREC-DL-2020, TREC-COVID, and MSMARCO. The results demonstrate that our method outperforms other baselines significantly.
        △ Less
      
    
    

    Submitted 29 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.19046
         [pdf, other] 
      
      
        cs.NE
        
      
    
    
    
      
        Large Language Models as Evolutionary Optimizers
      
    
    
      Authors:
      
      Shengcai Liu, 
      
      Caishun Chen, 
      
      Xinghua Qu, 
      
      Ke Tang, 
      
      Yew-Soon Ong
      
    
    
    
      Abstract:
      
        …However, EAs often demand carefully-designed operators with the aid of domain expertise to achieve satisfactory performance. In this work, we present the first study on large language models (LLMs) as evolutionary combinatorial optimizers. The main advantage is that it requires…
        ▽ More
      
      
        Evolutionary algorithms (EAs) have achieved remarkable success in tackling complex combinatorial optimization problems. However, EAs often demand carefully-designed operators with the aid of domain expertise to achieve satisfactory performance. In this work, we present the first study on large language models (LLMs) as evolutionary combinatorial optimizers. The main advantage is that it requires minimal domain knowledge and human efforts, as well as no additional training of the model. This approach is referred to as LLM-driven EA (LMEA). Specifically, in each generation of the evolutionary search, LMEA instructs the LLM to select parent solutions from current population, and perform crossover and mutation to generate offspring solutions. Then, LMEA evaluates these new solutions and include them into the population for the next generation. LMEA is equipped with a self-adaptation mechanism that controls the temperature of the LLM. This enables it to balance between exploration and exploitation and prevents the search from getting stuck in local optima. We investigate the power of LMEA on the classical traveling salesman problems (TSPs) widely used in combinatorial optimization research. Notably, the results show that LMEA performs competitively to traditional heuristics in finding high-quality solutions on TSP instances with up to 20 nodes. Additionally, we also study the effectiveness of LLM-driven crossover/mutation and the self-adaptation mechanism in evolutionary search. In summary, our results reveal the great potentials of LLMs as evolutionary optimizers for solving combinatorial problems. We hope our research shall inspire future explorations on LLM-driven EAs for complex optimization challenges.
        △ Less
      
    
    

    Submitted 29 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.19019
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.AI
          
        
      
    
    
    
      
        TeacherLM: Teaching to Fish Rather Than Giving the Fish, Language Modeling Likewise
      
    
    
      Authors:
      
      Nan He, 
      
      Hanyu Lai, 
      
      Chenyang Zhao, 
      
      Zirui Cheng, 
      
      Junting Pan, 
      
      Ruoyu Qin, 
      
      Ruofan Lu, 
      
      Rui Lu, 
      
      Yunchen Zhang, 
      
      Gangming Zhao, 
      
      Zhaohui Hou, 
      
      Zhiyuan Huang, 
      
      Shaoqing Lu, 
      
      Ding Liang, 
      
      Mingjie Zhan
      
    
    
    
      Abstract:
      
        Large Language Models (LLMs) exhibit impressive reasoning and data augmentation capabilities in various NLP tasks. However, what about small models? In this work, we propose TeacherLM-7.1B, capable of annotating relevant fundamentals, chain of thought, and common mistakes for mos…
        ▽ More
      
      
        Large Language Models (LLMs) exhibit impressive reasoning and data augmentation capabilities in various NLP tasks. However, what about small models? In this work, we propose TeacherLM-7.1B, capable of annotating relevant fundamentals, chain of thought, and common mistakes for most NLP samples, which makes annotation more than just an answer, thus allowing other models to learn ""why"" instead of just ""what"". The TeacherLM-7.1B model achieved a zero-shot score of 52.3 on MMLU, surpassing most models with over 100B parameters. Even more remarkable is its data augmentation ability. Based on TeacherLM-7.1B, we augmented 58 NLP datasets and taught various student models with different parameters from OPT and BLOOM series in a multi-task setting. The experimental results indicate that the data augmentation provided by TeacherLM has brought significant benefits. We will release the TeacherLM series of models and augmented datasets as open-source.
        △ Less
      
    
    

    Submitted 31 October, 2023; v1 submitted 29 October, 2023;
      originally announced October 2023.
      
    
    
    
      Comments:
      5 figures, 15 pages
    
    

    

    
  

  
    
      arXiv:2310.18964
         [pdf] 
      
      
        cs.CL
        
      
    
    
    
      
        LLMs and Finetuning: Benchmarking cross-domain performance for hate speech detection
      
    
    
      Authors:
      
      Ahmad Nasir, 
      
      Aadish Sharma, 
      
      Kokil Jaidka
      
    
    
    
      Abstract:
      
        This paper compares different pre-trained and fine-tuned large language models (LLMs) for hate speech detection. Our research underscores challenges in LLMs' cross-domain validity and overfitting risks. Through evaluations, we highlight the need for fine-tuned models that gra…
        ▽ More
      
      
        This paper compares different pre-trained and fine-tuned large language models (LLMs) for hate speech detection. Our research underscores challenges in LLMs' cross-domain validity and overfitting risks. Through evaluations, we highlight the need for fine-tuned models that grasp the nuances of hate speech through greater label heterogeneity. We conclude with a vision for the future of hate speech detection, emphasizing cross-domain generalizability and appropriate benchmarking practices.
        △ Less
      
    
    

    Submitted 29 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      9 pages, 3 figures, 4 tables
    
    

    

    
  

  
    
      arXiv:2310.18951
         [pdf] 
      
      
        cs.IR
        
      
    
    
    
      
        A Multimodal Ecological Civilization Pattern Recommendation Method Based on Large Language Models and Knowledge Graph
      
    
    
      Authors:
      
      Zhihang Yu, 
      
      Shu Wang, 
      
      Yunqiang Zhu, 
      
      Zhiqiang Zou
      
    
    
    
      Abstract:
      
        …we construct a knowledge graph to extract regional representations incorporating spatial heterogeneity features. Following that, inspired by the significant progress made by Large Language Models (LLMs) in the field of Natural Language Processing (NLP), we employ Large LLMs to g…
        ▽ More
      
      
        The Ecological Civilization Pattern Recommendation System (ECPRS) aims to recommend suitable ecological civilization patterns for target regions, promoting sustainable development and reducing regional disparities. However, the current representative recommendation methods are not suitable for recommending ecological civilization patterns in a geographical context. There are two reasons for this. Firstly, regions have spatial heterogeneity, and the (ECPRS)needs to consider factors like climate, topography, vegetation, etc., to recommend civilization patterns adapted to specific ecological environments, ensuring the feasibility and practicality of the recommendations. Secondly, the abstract features of the ecological civilization patterns in the real world have not been fully utilized., resulting in poor richness in their embedding representations and consequently, lower performance of the recommendation system. Considering these limitations, we propose the ECPR-MML method. Initially, based on the novel method UGPIG, we construct a knowledge graph to extract regional representations incorporating spatial heterogeneity features. Following that, inspired by the significant progress made by Large Language Models (LLMs) in the field of Natural Language Processing (NLP), we employ Large LLMs to generate multimodal features for ecological civilization patterns in the form of text and images. We extract and integrate these multimodal features to obtain semantically rich representations of ecological civilization. Through extensive experiments, we validate the performance of our ECPR-MML model. Our results show that F1@5 is 2.11% higher compared to state-of-the-art models, 2.02% higher than NGCF, and 1.16% higher than UGPIG. Furthermore, multimodal data can indeed enhance recommendation performance. However, the data generated by LLM is not as effective as real data to a certain extent.
        △ Less
      
    
    

    Submitted 29 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.18940
         [pdf, other] 
      
      
        cs.AI
        
          
            cs.LG
          
            cs.MA
          
        
      
    
    
    
      
        Language Agents with Reinforcement Learning for Strategic Play in the Werewolf Game
      
    
    
      Authors:
      
      Zelai Xu, 
      
      Chao Yu, 
      
      Fei Fang, 
      
      Yu Wang, 
      
      Yi Wu
      
    
    
    
      Abstract:
      
        Agents built with large language models (LLMs) have recently achieved great advancements. However, most of the efforts focus on single-agent or cooperative settings, leaving more general multi-agent environments underexplored. We propose a new framework powered by reinforcement l…
        ▽ More
      
      
        Agents built with large language models (LLMs) have recently achieved great advancements. However, most of the efforts focus on single-agent or cooperative settings, leaving more general multi-agent environments underexplored. We propose a new framework powered by reinforcement learning (RL) to develop strategic language agents, i.e., LLM-based agents with strategic thinking ability, for a popular language game, Werewolf. Werewolf is a social deduction game with hidden roles that involves both cooperation and competition and emphasizes deceptive communication and diverse gameplay. Our agent tackles this game by first using LLMs to reason about potential deceptions and generate a set of strategically diverse actions. Then an RL policy, which selects an action from the candidates, is learned by population-based training to enhance the agents' decision-making ability. By combining LLMs with the RL policy, our agent produces a variety of emergent strategies, achieves the highest win rate against other LLM-based agents, and stays robust against adversarial human players in the Werewolf game.
        △ Less
      
    
    

    Submitted 29 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.18913
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.AI
          
            stat.ML
          
        
      
    
    
    
      
        Debiasing Algorithm through Model Adaptation
      
    
    
      Authors:
      
      Tomasz Limisiewicz, 
      
      David Mareček, 
      
      Tomáš Musil
      
    
    
    
      Abstract:
      
        Large language models are becoming the go-to solution for various language tasks. However, with growing capacity, models are prone to rely on spurious correlations stemming from biases and stereotypes present in the training data. This work proposes a novel method for detecting a…
        ▽ More
      
      
        Large language models are becoming the go-to solution for various language tasks. However, with growing capacity, models are prone to rely on spurious correlations stemming from biases and stereotypes present in the training data. This work proposes a novel method for detecting and mitigating gender bias in language models. We perform causal analysis to identify problematic model components and discover that mid-upper feed-forward layers are most prone to convey biases. Based on the analysis results, we adapt the model by multiplying these layers by a linear projection. Our titular method, DAMA, significantly decreases bias as measured by diverse metrics while maintaining the model's performance on downstream tasks. We release code for our method and models, which retrain LLaMA's state-of-the-art performance while being significantly less biased.
        △ Less
      
    
    

    Submitted 29 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.18813
         [pdf, other] 
      
      
        cs.LG
        
          
            cs.DC
          
        
      
    
    
    
      
        The Synergy of Speculative Decoding and Batching in Serving Large Language Models
      
    
    
      Authors:
      
      Qidong Su, 
      
      Christina Giannoula, 
      
      Gennady Pekhimenko
      
    
    
    
      Abstract:
      
        Large Language Models (LLMs) like GPT are state-of-the-art text generation models that provide significant assistance in daily routines. However, LLM execution is inherently sequential, since they only produce one token at a time, thus incurring low hardware utilization on modern…
        ▽ More
      
      
        Large Language Models (LLMs) like GPT are state-of-the-art text generation models that provide significant assistance in daily routines. However, LLM execution is inherently sequential, since they only produce one token at a time, thus incurring low hardware utilization on modern GPUs. Batching and speculative decoding are two techniques to improve GPU hardware utilization in LLM inference. To study their synergy, we implement a prototype implementation and perform an extensive characterization analysis on various LLM models and GPU architectures. We observe that the optimal speculation length depends on the batch size used. We analyze the key observation and build a quantitative model to explain it. Based on our analysis, we propose a new adaptive speculative decoding strategy that chooses the optimal speculation length for different batch sizes. Our evaluations show that our proposed method can achieve equal or better performance than the state-of-the-art speculation decoding schemes with fixed speculation length.
        △ Less
      
    
    

    Submitted 28 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.18783
         [pdf, other] 
      
      
        cs.CL
        
      
    
    
    
      
        Are NLP Models Good at Tracing Thoughts: An Overview of Narrative Understanding
      
    
    
      Authors:
      
      Lixing Zhu, 
      
      Runcong Zhao, 
      
      Lin Gui, 
      
      Yulan He
      
    
    
    
      Abstract:
      
        Narrative understanding involves capturing the author's cognitive processes, providing insights into their knowledge, intentions, beliefs, and desires. Although large language models (LLMs) excel in generating grammatically coherent text, their ability to comprehend the autho…
        ▽ More
      
      
        Narrative understanding involves capturing the author's cognitive processes, providing insights into their knowledge, intentions, beliefs, and desires. Although large language models (LLMs) excel in generating grammatically coherent text, their ability to comprehend the author's thoughts remains uncertain. This limitation hinders the practical applications of narrative understanding. In this paper, we conduct a comprehensive survey of narrative understanding tasks, thoroughly examining their key features, definitions, taxonomy, associated datasets, training objectives, evaluation metrics, and limitations. Furthermore, we explore the potential of expanding the capabilities of modularized LLMs to address novel narrative understanding tasks. By framing narrative understanding as the retrieval of the author's imaginative cues that outline the narrative structure, our study introduces a fresh perspective on enhancing narrative comprehension.
        △ Less
      
    
    

    Submitted 28 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.18768
         [pdf, other] 
      
      
        cs.CL
        
      
    
    
    
      
        Crossing the Aisle: Unveiling Partisan and Counter-Partisan Events in News Reporting
      
    
    
      Authors:
      
      Kaijian Zou, 
      
      Xinliang Frederick Zhang, 
      
      Winston Wu, 
      
      Nick Beauchamp, 
      
      Lu Wang
      
    
    
    
      Abstract:
      
        …diverse media outlets. We benchmark PAC to highlight the challenges of this task. Our findings highlight both the ways in which the news subtly shapes opinion and the need for large language models that better understand events within a broader context. Our dataset can be found a…
        ▽ More
      
      
        News media is expected to uphold unbiased reporting. Yet they may still affect public opinion by selectively including or omitting events that support or contradict their ideological positions. Prior work in NLP has only studied media bias via linguistic style and word usage. In this paper, we study to which degree media balances news reporting and affects consumers through event inclusion or omission. We first introduce the task of detecting both partisan and counter-partisan events: events that support or oppose the author's political ideology. To conduct our study, we annotate a high-quality dataset, PAC, containing 8,511 (counter-)partisan event annotations in 304 news articles from ideologically diverse media outlets. We benchmark PAC to highlight the challenges of this task. Our findings highlight both the ways in which the news subtly shapes opinion and the need for large language models that better understand events within a broader context. Our dataset can be found at https://github.com/launchnlp/Partisan-Event-Dataset.
        △ Less
      
    
    

    Submitted 28 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      EMNLP'23 Findings
    
    

    

    
  

  
    
      arXiv:2310.18752
         [pdf, other] 
      
      
        cs.AI
        
      
    
    
    
      
        Reboost Large Language Model-based Text-to-SQL, Text-to-Python, and Text-to-Function -- with Real Applications in Traffic Domain
      
    
    
      Authors:
      
      Guanghu Sui, 
      
      Zhishuai Li, 
      
      Ziyue Li, 
      
      Sun Yang, 
      
      Jingqing Ruan, 
      
      Hangyu Mao, 
      
      Rui Zhao
      
    
    
    
      Abstract:
      
        …In order to prevent information gaps, we include the comments, value types, and value samples for columns as part of the database description in the prompt. Our experiments with Large Language Models (LLMs) illustrate the significant performance improvement on the business datase…
        ▽ More
      
      
        The previous state-of-the-art (SOTA) method achieved a remarkable execution accuracy on the Spider dataset, which is one of the largest and most diverse datasets in the Text-to-SQL domain. However, during our reproduction of the business dataset, we observed a significant drop in performance. We examined the differences in dataset complexity, as well as the clarity of questions' intentions, and assessed how those differences could impact the performance of prompting methods. Subsequently, We develop a more adaptable and more general prompting method, involving mainly query rewriting and SQL boosting, which respectively transform vague information into exact and precise information and enhance the SQL itself by incorporating execution feedback and the query results from the database content. In order to prevent information gaps, we include the comments, value types, and value samples for columns as part of the database description in the prompt. Our experiments with Large Language Models (LLMs) illustrate the significant performance improvement on the business dataset and prove the substantial potential of our method. In terms of execution accuracy on the business dataset, the SOTA method scored 21.05, while our approach scored 65.79. As a result, our approach achieved a notable performance improvement even when using a less capable pre-trained language model. Last but not least, we also explore the Text-to-Python and Text-to-Function options, and we deeply analyze the pros and cons among them, offering valuable insights to the community.
        △ Less
      
    
    

    Submitted 31 October, 2023; v1 submitted 28 October, 2023;
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.18729
         [pdf, other] 
      
      
        cs.AI
        
          
            cs.CL
          
            cs.HC
          
        
      
    
    
    
      
        Using Large Language Models to Support Thematic Analysis in Empirical Legal Studies
      
    
    
      Authors:
      
      Jakub Drápal, 
      
      Hannes Westermann, 
      
      Jaromir Savelka
      
    
    
    
      Abstract:
      
        …are widely used qualitative analytic methods within empirical legal studies (ELS). We propose a novel framework facilitating effective collaboration of a legal expert with a large language model (LLM) for generating initial codes (phase 2 of thematic analysis), searching for them…
        ▽ More
      
      
        Thematic analysis and other variants of inductive coding are widely used qualitative analytic methods within empirical legal studies (ELS). We propose a novel framework facilitating effective collaboration of a legal expert with a large language model (LLM) for generating initial codes (phase 2 of thematic analysis), searching for themes (phase 3), and classifying the data in terms of the themes (to kick-start phase 4). We employed the framework for an analysis of a dataset (n=785) of facts descriptions from criminal court opinions regarding thefts. The goal of the analysis was to discover classes of typical thefts. Our results show that the LLM, namely OpenAI's GPT-4, generated reasonable initial codes, and it was capable of improving the quality of the codes based on expert feedback. They also suggest that the model performed well in zero-shot classification of facts descriptions in terms of the themes. Finally, the themes autonomously discovered by the LLM appear to map fairly well to the themes arrived at by legal experts. These findings can be leveraged by legal researchers to guide their decisions in integrating LLMs into their thematic analyses, as well as other inductive coding projects.
        △ Less
      
    
    

    Submitted 28 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      10 pages, 5 figures, 3 tables
    
    

    

    
      
        Journal ref:
        The Thirty-sixth Annual Conference on Legal Knowledge and Information Systems (JURIX 2023), Maastricht, The Netherlands
      
    
  

  
    
      arXiv:2310.18696
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.AI
          
            cs.LG
          
        
      
    
    
    
      
        Probing LLMs for Joint Encoding of Linguistic Categories
      
    
    
      Authors:
      
      Giulio Starace, 
      
      Konstantinos Papakostas, 
      
      Rochelle Choenni, 
      
      Apostolos Panagiotopoulos, 
      
      Matteo Rosati, 
      
      Alina Leidinger, 
      
      Ekaterina Shutova
      
    
    
    
      Abstract:
      
        Large Language Models (LLMs) exhibit impressive performance on a range of NLP tasks, due to the general-purpose linguistic knowledge acquired during pretraining. Existing model interpretability research (Tenney et al., 2019) suggests that a linguistic hierarchy emerges in the LLM…
        ▽ More
      
      
        Large Language Models (LLMs) exhibit impressive performance on a range of NLP tasks, due to the general-purpose linguistic knowledge acquired during pretraining. Existing model interpretability research (Tenney et al., 2019) suggests that a linguistic hierarchy emerges in the LLM layers, with lower layers better suited to solving syntactic tasks and higher layers employed for semantic processing. Yet, little is known about how encodings of different linguistic phenomena interact within the models and to what extent processing of linguistically-related categories relies on the same, shared model representations. In this paper, we propose a framework for testing the joint encoding of linguistic categories in LLMs. Focusing on syntax, we find evidence of joint encoding both at the same (related part-of-speech (POS) classes) and different (POS classes and related syntactic dependency relations) levels of linguistic hierarchy. Our cross-lingual experiments show that the same patterns hold across languages in multilingual LLMs.
        △ Less
      
    
    

    Submitted 28 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      Accepted in EMNLP Findings 2023
    
    

    

    
  

  
    
      arXiv:2310.18679
         [pdf] 
      
      
        cs.CL
        
          
            cs.AI
          
            cs.LG
          
        
      
    
    
    
      
        N-Critics: Self-Refinement of Large Language Models with Ensemble of Critics
      
    
    
      Authors:
      
      Sajad Mousavi, 
      
      Ricardo Luna Gutiérrez, 
      
      Desik Rengarajan, 
      
      Vineet Gundecha, 
      
      Ashwin Ramesh Babu, 
      
      Avisek Naug, 
      
      Antonio Guillen, 
      
      Soumyendu Sarkar
      
    
    
    
      Abstract:
      
        We propose a self-correction mechanism for Large Language Models (LLMs) to mitigate issues such as toxicity and fact hallucination. This method involves refining model outputs through an ensemble of critics and the model's own feedback. Drawing inspiration from human behavior…
        ▽ More
      
      
        We propose a self-correction mechanism for Large Language Models (LLMs) to mitigate issues such as toxicity and fact hallucination. This method involves refining model outputs through an ensemble of critics and the model's own feedback. Drawing inspiration from human behavior, we explore whether LLMs can emulate the self-correction process observed in humans who often engage in self-reflection and seek input from others to refine their understanding of complex topics. Our approach is model-agnostic and can be applied across various domains to enhance trustworthiness by addressing fairness, bias, and robustness concerns. We consistently observe performance improvements in LLMs for reducing toxicity and correcting factual errors.
        △ Less
      
    
    

    Submitted 28 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
      
        Journal ref:
        NeurIPS 2023 Workshop on Robustness of Few-shot and Zero-shot Learning in Foundation Models 2023(NeurIPS 2023)
      
    
  

  
    
      arXiv:2310.18659
         [pdf, other] 
      
      
        cs.AI
        
          
            cs.CL
          
        
      
    
    
    
      
        From Indeterminacy to Determinacy: Augmenting Logical Reasoning Capabilities with Large Language Models
      
    
    
      Authors:
      
      Hongda Sun, 
      
      Weikai Xu, 
      
      Wei Liu, 
      
      Jian Luan, 
      
      Bin Wang, 
      
      Shuo Shang, 
      
      Ji-Rong Wen, 
      
      Rui Yan
      
    
    
    
      Abstract:
      
        Recent advances in LLMs have revolutionized the landscape of reasoning tasks. To enhance the capabilities of LLMs to emulate human reasoning, prior works focus on modeling reasoning steps using specific thought structures like chains, trees, or graphs. However, LLM-based reasoning continues to encounter three challenges: 1) Selecting appropriate reasoning structures for various tasks; 2) Exploitin…
        ▽ More
      
      
        Recent advances in LLMs have revolutionized the landscape of reasoning tasks. To enhance the capabilities of LLMs to emulate human reasoning, prior works focus on modeling reasoning steps using specific thought structures like chains, trees, or graphs. However, LLM-based reasoning continues to encounter three challenges: 1) Selecting appropriate reasoning structures for various tasks; 2) Exploiting known conditions sufficiently and efficiently to deduce new insights; 3) Considering the impact of historical reasoning experience. To address these challenges, we propose DetermLR, a novel reasoning framework that formulates the reasoning process as a transformational journey from indeterminate premises to determinate ones. This process is marked by the incremental accumulation of determinate premises, making the conclusion progressively closer to clarity. DetermLR includes three essential components: 1) Premise identification: We categorize premises into two distinct types: determinate and indeterminate. This empowers LLMs to customize reasoning structures to match the specific task complexities. 2) Premise prioritization and exploration: We leverage quantitative measurements to assess the relevance of each premise to the target, prioritizing more relevant premises for exploring new insights. 3) Iterative process with reasoning memory: We introduce a reasoning memory module to automate storage and extraction of available premises and reasoning paths, preserving historical reasoning details for more accurate premise prioritization. Comprehensive experimental results show that DetermLR outperforms all baselines on four challenging logical reasoning tasks: LogiQA, ProofWriter, FOLIO, and LogicalDeduction. DetermLR can achieve better reasoning performance while requiring fewer visited states, highlighting its superior efficiency and effectiveness in tackling logical reasoning tasks.
        △ Less
      
    
    

    Submitted 28 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      Code repo: https://github.com/XiaoMi/DetermLR
    
    

    

    
  

  
    
      arXiv:2310.18634
         [pdf, other] 
      
      
        cs.LG
        
      
    
    
    
      
        SSL Framework for Causal Inconsistency between Structures and Representations
      
    
    
      Authors:
      
      Hang Chen, 
      
      Xinyu Yang, 
      
      Keqing Du
      
    
    
    
      Abstract:
      
        …learning (SSL) framework that considers interventions as `views' and CCC as a `philosophy' with two implement examples on Supervised Specialized Models (SSMs) and Large Language Models (LLMs), respectively. To evaluate pure inconsistency manifestations, we have prepared t…
        ▽ More
      
      
        The cross-pollination of deep learning and causal discovery has catalyzed a burgeoning field of research seeking to elucidate causal relationships within non-statistical data forms like images, videos, and text. Such data, often being named `indefinite data', exhibit unique challenges-inconsistency between causal structure and representation, which are not common in conventional data forms. To tackle this issue, we theoretically develop intervention strategies suitable for indefinite data and derive causal consistency condition (CCC). Moreover, we design a self-supervised learning (SSL) framework that considers interventions as `views' and CCC as a `philosophy' with two implement examples on Supervised Specialized Models (SSMs) and Large Language Models (LLMs), respectively. To evaluate pure inconsistency manifestations, we have prepared the first high-quality causal dialogue dataset-Causalogue. Evaluations are also performed on three other downstream tasks. Extensive experimentation has substantiated the efficacy of our methodology, illuminating how CCC could potentially play an influential role in various fields.
        △ Less
      
    
    

    Submitted 28 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.18603
         [pdf, other] 
      
      
        cs.LG
        
      
    
    
    
      
        Large Language Models Are Better Adversaries: Exploring Generative Clean-Label Backdoor Attacks Against Text Classifiers
      
    
    
      Authors:
      
      Wencong You, 
      
      Zayd Hammoudeh, 
      
      Daniel Lowd
      
    
    
    
      Abstract:
      
        Backdoor attacks manipulate model predictions by inserting innocuous triggers into training and test data. We focus on more realistic and more challenging clean-label attacks where the adversarial training examples are correctly labeled. Our attack, LLMBkd, leverages language models to automatically insert diverse style-based triggers into texts. We also propose a poison selection technique to imp…
        ▽ More
      
      
        Backdoor attacks manipulate model predictions by inserting innocuous triggers into training and test data. We focus on more realistic and more challenging clean-label attacks where the adversarial training examples are correctly labeled. Our attack, LLMBkd, leverages language models to automatically insert diverse style-based triggers into texts. We also propose a poison selection technique to improve the effectiveness of both LLMBkd as well as existing textual backdoor attacks. Lastly, we describe REACT, a baseline defense to mitigate backdoor attacks via antidote training examples. Our evaluations demonstrate LLMBkd's effectiveness and efficiency, where we consistently achieve high attack success rates across a wide range of styles with little effort and no model training.
        △ Less
      
    
    

    Submitted 28 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      Accepted at EMNLP 2023 Findings
    
    

    

    
  

  
    
      arXiv:2310.18587
         [pdf, other] 
      
      
        cs.SE
        
      
    
    
    
      
        Assessing and Improving Syntactic Adversarial Robustness of Pre-trained Models for Code Translation
      
    
    
      Authors:
      
      Guang Yang, 
      
      Yu Zhou, 
      
      Xiangyu Zhang, 
      
      Xiang Chen, 
      
      Tingting Han, 
      
      Taolue Chen
      
    
    
    
      Abstract:
      
        …reduce the performance of existing PTMs, while CoTR-D effectively improves the robustness of PTMs. Conclusion: Our study identifies the limitations of current PTMs, including large language models, in code translation tasks. It highlights the potential of CoTR as an effective sol…
        ▽ More
      
      
        Context: Pre-trained models (PTMs) have demonstrated significant potential in automatic code translation. However, the vulnerability of these models in translation tasks, particularly in terms of syntax, has not been extensively investigated. Objective: To fill this gap, our study aims to propose a novel approach CoTR to assess and improve the syntactic adversarial robustness of PTMs in code translation. Method: CoTR consists of two components: CoTR-A and CoTR-D. CoTR-A generates adversarial examples by transforming programs, while CoTR-D proposes a semantic distance-based sampling data augmentation method and adversarial training method to improve the model's robustness and generalization capabilities. The Pass@1 metric is used by CoTR to assess the performance of PTMs, which is more suitable for code translation tasks and offers a more precise evaluation in real world scenarios. Results: The effectiveness of CoTR is evaluated through experiments on real world Java to Python datasets. The results demonstrate that CoTR-A can significantly reduce the performance of existing PTMs, while CoTR-D effectively improves the robustness of PTMs. Conclusion: Our study identifies the limitations of current PTMs, including large language models, in code translation tasks. It highlights the potential of CoTR as an effective solution to enhance the robustness of PTMs for code translation tasks.
        △ Less
      
    
    

    Submitted 28 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      under review
    
    

    

    
  

  
    
      arXiv:2310.18581
         [pdf, other] 
      
      
        cs.CL
        
      
    
    
    
      
        Accelerating LLM Inference by Enabling Intermediate Layer Decoding
      
    
    
      Authors:
      
      Neeraj Varshney, 
      
      Agneet Chatterjee, 
      
      Mihir Parmar, 
      
      Chitta Baral
      
    
    
    
      Abstract:
      
        Large Language Models (LLMs) have achieved remarkable performance across a wide variety of natural language tasks; however, their large size makes their inference slow and computationally expensive which poses a practical challenge for resource constrained real-world applications…
        ▽ More
      
      
        Large Language Models (LLMs) have achieved remarkable performance across a wide variety of natural language tasks; however, their large size makes their inference slow and computationally expensive which poses a practical challenge for resource constrained real-world applications. Focusing on this problem, we propose to instruction tune LLMs in a way that enables intermediate layer decoding for efficiently generating text, but importantly without compromising the quality of the generation. Specifically, we instruction tune LLMs with additional explicit Losses from the InTermediate layErs (LITE) and show that it enables these layers to acquire 'good' generation ability without affecting the generation ability of the final layer. We perform 'dynamic confidence-based early exiting' at token level from the intermediate layers which improves the efficiency of inference while maintaining the generation quality. We conduct comprehensive experiments by instruction tuning LLaMA-2 models on the widely used Alpaca dataset and holistically evaluate on four different human-instruction test sets: Vicuna, WizardLM, Koala, and Self-Instruct. We show that 'dynamic early exiting' achieves consistent and considerable cost improvements (37.86% on average) while maintaining the generation quality of the responses. We further conduct a thorough analysis of the results over several important aspects, such as comparing the semantic similarity of the outputs and dissecting the efficiency improvements by comparing the number of tokens generated in the output. In summary, our work contributes to improving the efficiency of LLM inference while maintaining the generation quality, a crucial step en route to enabling their widespread adoption.
        △ Less
      
    
    

    Submitted 28 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.18532
         [pdf, other] 
      
      
        cs.SE
        
      
    
    
    
      
        SkipAnalyzer: An Embodied Agent for Code Analysis with Large Language Models
      
    
    
      Authors:
      
      Mohammad Mahdi Mohajer, 
      
      Reem Aleithan, 
      
      Nima Shiri Harzevili, 
      
      Moshi Wei, 
      
      Alvine Boaye Belle, 
      
      Hung Viet Pham, 
      
      Song Wang
      
    
    
    
      Abstract:
      
        We introduce SkipAnalyzer, the first large language model (LLM)-powered embodied agent for static code analysis. It can detect bugs, filter false positive warnings, and patch the detected bugs without human intervention. SkipAnalyzer consists of three components, 1) an LLM-based…
        ▽ More
      
      
        We introduce SkipAnalyzer, the first large language model (LLM)-powered embodied agent for static code analysis. It can detect bugs, filter false positive warnings, and patch the detected bugs without human intervention. SkipAnalyzer consists of three components, 1) an LLM-based static bug detector that scans source code and reports specific types of bugs, 2) an LLM-based false-positive filter that can identify false-positive bugs in the results of static bug detectors to improve detection accuracy, and 3) an LLM-based patch generator that can generate patches for the detected bugs above. As a proof-of-concept, SkipAnalyzer is built on ChatGPT, which has exhibited outstanding performance in various software engineering tasks. To evaluate SkipAnalyzer, we focus on two types of typical and critical bugs that are targeted by static bug detection, i.e., Null Dereference and Resource Leak as subjects. We employ Infer to aid the gathering of these two bug types from 10 open-source projects. Consequently, our experiment dataset contains 222 instances of Null Dereference bugs and 46 instances of Resource Leak bugs. Our study demonstrates that SkipAnalyzer achieves remarkable performance in the mentioned static analysis tasks, including bug detection, false-positive warning removal, and bug repair. In static bug detection, SkipAnalyzer achieves accuracy values of up to 68.37% for detecting Null Dereference bugs and 76.95% for detecting Resource Leak bugs, outperforming the current leading bug detector, Infer. For removing false-positive warnings, SkipAnalyzer can reach a precision of up to 93.88% for Null Dereference bugs and 63.33% for Resource Leak bugs. Additionally, SkipAnalyzer surpasses state-of-the-art false-positive warning removal tools. Furthermore, in bug repair, SkipAnalyzer can generate syntactically correct patches to fix its detected bugs with a success rate of up to 97.30%.
        △ Less
      
    
    

    Submitted 27 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.18512
         [pdf, other] 
      
      
        cs.LG
        
      
    
    
    
      
        Preventing Language Models From Hiding Their Reasoning
      
    
    
      Authors:
      
      Fabien Roger, 
      
      Ryan Greenblatt
      
    
    
    
      Abstract:
      
        Large language models (LLMs) often benefit from intermediate steps of reasoning to generate answers to complex problems. When these intermediate steps of reasoning are used to monitor the activity of the model, it is essential that this explicit reasoning is faithful, i.e. that i…
        ▽ More
      
      
        Large language models (LLMs) often benefit from intermediate steps of reasoning to generate answers to complex problems. When these intermediate steps of reasoning are used to monitor the activity of the model, it is essential that this explicit reasoning is faithful, i.e. that it reflects what the model is actually reasoning about. In this work, we focus on one potential way intermediate steps of reasoning could be unfaithful: encoded reasoning, where an LLM could encode intermediate steps of reasoning in the generated text in a way that is not understandable to human readers. We show that language models can be trained to make use of encoded reasoning to get higher performance without the user understanding the intermediate steps of reasoning. We argue that, as language models get stronger, this behavior becomes more likely to appear naturally. Finally, we describe a methodology that enables the evaluation of defenses against encoded reasoning, and show that, under the right conditions, paraphrasing successfully prevents even the best encoding schemes we built from encoding more than 3 bits of information per KB of text.
        △ Less
      
    
    

    Submitted 27 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.18502
         [pdf, other] 
      
      
        cs.CL
        
      
    
    
    
      
        On the Automatic Generation and Simplification of Children's Stories
      
    
    
      Authors:
      
      Maria Valentini, 
      
      Jennifer Weber, 
      
      Jesus Salcido, 
      
      Téa Wright, 
      
      Eliana Colunga, 
      
      Katharina Kann
      
    
    
    
      Abstract:
      
        With recent advances in large…
        ▽ More
      
      
        With recent advances in large language models (LLMs), the concept of automatically generating children's educational materials has become increasingly realistic. Working toward the goal of age-appropriate simplicity in generated educational texts, we first examine the ability of several popular LLMs to generate stories with properly adjusted lexical and readability levels. We find that, in spite of the growing capabilities of LLMs, they do not yet possess the ability to limit their vocabulary to levels appropriate for younger age groups. As a second experiment, we explore the ability of state-of-the-art lexical simplification models to generalize to the domain of children's stories and, thus, create an efficient pipeline for their automatic generation. In order to test these models, we develop a dataset of child-directed lexical simplification instances, with examples taken from the LLM-generated stories in our first experiment. We find that, while the strongest-performing current lexical simplification models do not perform as well on material designed for children due to their reliance on large language models behind the scenes, some models that still achieve fairly strong results on general data can mimic or even improve their performance on children-directed data with proper fine-tuning, which we conduct using our newly created child-directed simplification dataset.
        △ Less
      
    
    

    Submitted 27 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      Accepted to EMNLP 2023 (main conference)
    
    

    

    
  

  
    
      arXiv:2310.18463
         [pdf, other] 
      
      
        cs.CL
        
      
    
    
    
      
        PeTailor: Improving Large Language Model by Tailored Chunk Scorer in Biomedical Triple Extraction
      
    
    
      Authors:
      
      Mingchen Li, 
      
      M. Chen, 
      
      Huixue Zhou, 
      
      Rui Zhang
      
    
    
    
      Abstract:
      
        The automatic extraction of biomedical entities and their interaction from unstructured data remains a challenging task due to the limited availability of expert-labeled standard datasets. In this paper, we introduce PETAI-LOR, a retrieval-based language framework that is augmented by tailored chunk scorer. Unlike previous retrieval-augmented language models (LM) that retrieve relevant documents b…
        ▽ More
      
      
        The automatic extraction of biomedical entities and their interaction from unstructured data remains a challenging task due to the limited availability of expert-labeled standard datasets. In this paper, we introduce PETAI-LOR, a retrieval-based language framework that is augmented by tailored chunk scorer. Unlike previous retrieval-augmented language models (LM) that retrieve relevant documents by calculating the similarity between the input sentence and the candidate document set, PETAILOR segments the sentence into chunks and retrieves the relevant chunk from our pre-computed chunk-based relational key-value memory. Moreover, in order to comprehend the specific requirements of the LM, PETAI-LOR adapt the tailored chunk scorer to the LM. We also introduce GM-CIHT, an expert annotated biomedical triple extraction dataset with more relation types. This dataset is centered on the non-drug treatment and general biomedical domain. Additionally, we investigate the efficacy of triple extraction models trained on general domains when applied to the biomedical domain. Our experiments reveal that PETAI-LOR achieves state-of-the-art performance on GM-CIHT
        △ Less
      
    
    

    Submitted 27 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      this is the first preprint version
    
    

    

    
  

  
    
      arXiv:2310.18454
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.LG
          
        
      
    
    
    
      
        T5 meets Tybalt: Author Attribution in Early Modern English Drama Using Large Language Models
      
    
    
      Authors:
      
      Rebecca M. M. Hicke, 
      
      David Mimno
      
    
    
    
      Abstract:
      
        Large language models have shown breakthrough potential in many NLP domains. Here we consider their use for stylometry, specifically authorship identification in Early Modern English drama. We find both promising and concerning results; LLMs are able to accurately predict the aut…
        ▽ More
      
      
        Large language models have shown breakthrough potential in many NLP domains. Here we consider their use for stylometry, specifically authorship identification in Early Modern English drama. We find both promising and concerning results; LLMs are able to accurately predict the author of surprisingly short passages but are also prone to confidently misattribute texts to specific authors. A fine-tuned t5-large model outperforms all tested baselines, including logistic regression, SVM with a linear kernel, and cosine delta, at attributing small passages. However, we see indications that the presence of certain authors in the model's pre-training data affects predictive results in ways that are difficult to assess.
        △ Less
      
    
    

    Submitted 27 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      Published in CHR 2023
    
    

    

    
  

  
    
      arXiv:2310.18390
         [pdf, other] 
      
      
        cs.LG
        
      
    
    
    
      
        Entity Embeddings : Perspectives Towards an Omni-Modality Era for Large Language Models
      
    
    
      Authors:
      
      Eren Unlu, 
      
      Unver Ciftci
      
    
    
    
      Abstract:
      
        Large Language Models (LLMs) are evolving to integrate multiple modalities, such as text, image, and audio into a unified linguistic space. We envision a future direction based on this framework where conceptual entities defined in sequences of text can also be imagined as modali…
        ▽ More
      
      
        Large Language Models (LLMs) are evolving to integrate multiple modalities, such as text, image, and audio into a unified linguistic space. We envision a future direction based on this framework where conceptual entities defined in sequences of text can also be imagined as modalities. Such a formulation has the potential to overcome the cognitive and computational limitations of current models. Several illustrative examples of such potential implicit modalities are given. Along with vast promises of the hypothesized structure, expected challenges are discussed as well.
        △ Less
      
    
    

    Submitted 27 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      9 pages, 5 figures
    
    

    

    
  

  
    
      arXiv:2310.18373
         [pdf] 
      
      
        cs.CL
        
          
            cs.AI
          
        
      
    
    
    
      
        Can LLMs Grade Short-answer Reading Comprehension Questions : Foundational Literacy Assessment in LMICs
      
    
    
      Authors:
      
      Owen Henkel, 
      
      Libby Hills, 
      
      Bill Roberts, 
      
      Joshua McGrane
      
    
    
    
      Abstract:
      
        This paper presents emerging evidence of using generative large language models (i.e., GPT-4) to reliably evaluate short-answer reading comprehension questions. Specifically, we explore how various configurations of generative (LLMs) are able to evaluate student responses from a…
        ▽ More
      
      
        This paper presents emerging evidence of using generative large language models (i.e., GPT-4) to reliably evaluate short-answer reading comprehension questions. Specifically, we explore how various configurations of generative (LLMs) are able to evaluate student responses from a new dataset, drawn from a battery of reading assessments conducted with over 150 students in Ghana. As this dataset is novel and hence not used in training runs of GPT, it offers an opportunity to test for domain shift and evaluate the generalizability of generative LLMs, which are predominantly designed and trained on data from high-income North American countries. We found that GPT-4, with minimal prompt engineering performed extremely well on evaluating the novel dataset (Quadratic Weighted Kappa 0.923, F1 0.88), substantially outperforming transfer-learning based approaches, and even exceeding expert human raters (Quadratic Weighted Kappa 0.915, F1 0.87). To the best of our knowledge, our work is the first to empirically evaluate the performance of generative LLMs on short-answer reading comprehension questions, using real student data, and suggests that generative LLMs have the potential to reliably evaluate foundational literacy. Currently the assessment of formative literacy and numeracy is infrequent in many low and middle-income countries (LMICs) due to the cost and operational complexities of conducting them at scale. Automating the grading process for reading assessment could enable wider usage, and in turn improve decision-making regarding curricula, school management, and teaching practice at the classroom level. Importantly, in contrast transfer learning based approaches, generative LLMs generalize well and the technical barriers to their use are low, making them more feasible to implement and scale in lower resource educational contexts.
        △ Less
      
    
    

    Submitted 26 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.18365
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.AI
          
            cs.CY
          
        
      
    
    
    
      
        Using GPT-4 to Augment Unbalanced Data for Automatic Scoring
      
    
    
      Authors:
      
      Luyang Fang, 
      
      Gyeong-Geon Lee, 
      
      Xiaoming Zhai
      
    
    
    
      Abstract:
      
        …as it introduces uncertainty in the machine training process. To meet this challenge, we introduce a novel text data augmentation framework leveraging GPT-4, a generative large…
        ▽ More
      
      
        Machine learning-based automatic scoring can be challenging if students' responses are unbalanced across scoring categories, as it introduces uncertainty in the machine training process. To meet this challenge, we introduce a novel text data augmentation framework leveraging GPT-4, a generative large language model, specifically tailored for unbalanced datasets in automatic scoring. Our experimental dataset comprised student written responses to two science items. We crafted prompts for GPT-4 to generate responses resembling student written answers, particularly for the minority scoring classes, to augment the data. We then finetuned DistillBERT for automatic scoring based on the augmented and original datasets. Model performance was assessed using accuracy, precision, recall, and F1 metrics. Our findings revealed that incorporating GPT-4-augmented data remarkedly improved model performance, particularly for precision, recall, and F1 scores. Interestingly, the extent of improvement varied depending on the specific dataset and the proportion of augmented data used. Notably, we found that a varying amount of augmented data (5\%-40\%) was needed to obtain stable improvement for automatic scoring. We also compared the accuracies of models trained with GPT-4 augmented data to those trained with additional student-written responses. Results suggest that the GPT-4 augmented scoring models outperform or match the models trained with student-written augmented data. This research underscores the potential and effectiveness of data augmentation techniques utilizing generative large language models--GPT-4 in addressing unbalanced datasets within automated assessment.
        △ Less
      
    
    

    Submitted 24 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.18363
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.HC
          
            cs.LG
          
        
      
    
    
    
      
        A Contextualized Real-Time Multimodal Emotion Recognition for Conversational Agents using Graph Convolutional Networks in Reinforcement Learning
      
    
    
      Authors:
      
      Fathima Abdul Rahman, 
      
      Guang Lu
      
    
    
    
      Abstract:
      
        Owing to the recent developments in Generative Artificial Intelligence (GenAI) and Large Language Models (LLM), conversational agents are becoming increasingly popular and accepted. They provide a human touch by interacting in ways familiar to us and by providing support as virtu…
        ▽ More
      
      
        Owing to the recent developments in Generative Artificial Intelligence (GenAI) and Large Language Models (LLM), conversational agents are becoming increasingly popular and accepted. They provide a human touch by interacting in ways familiar to us and by providing support as virtual companions. Therefore, it is important to understand the user's emotions in order to respond considerately. Compared to the standard problem of emotion recognition, conversational agents face an additional constraint in that recognition must be real-time. Studies on model architectures using audio, visual, and textual modalities have mainly focused on emotion classification using full video sequences that do not provide online features. In this work, we present a novel paradigm for contextualized Emotion Recognition using Graph Convolutional Network with Reinforcement Learning (conER-GRL). Conversations are partitioned into smaller groups of utterances for effective extraction of contextual information. The system uses Gated Recurrent Units (GRU) to extract multimodal features from these groups of utterances. More importantly, Graph Convolutional Networks (GCN) and Reinforcement Learning (RL) agents are cascade trained to capture the complex dependencies of emotion features in interactive scenarios. Comparing the results of the conER-GRL model with other state-of-the-art models on the benchmark dataset IEMOCAP demonstrates the advantageous capabilities of the conER-GRL architecture in recognizing emotions in real-time from multimodal conversational signals.
        △ Less
      
    
    

    Submitted 24 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      5 pages (4 main + 1 reference), 2 figures. Submitted to IEEE FG2024
    
    

    
      
        

        

        
          ACM Class:
          I.2.10
        
      
    

    
  

  
    
      arXiv:2310.18362
         [pdf, ps, other] 
      
      
        cs.CL
        
          
            cs.CR
          
            cs.LG
          
        
      
    
    
    
      
        SoK: Memorization in General-Purpose Large Language Models
      
    
    
      Authors:
      
      Valentin Hartmann, 
      
      Anshuman Suri, 
      
      Vincent Bindschaedler, 
      
      David Evans, 
      
      Shruti Tople, 
      
      Robert West
      
    
    
    
      Abstract:
      
        Large Language Models (LLMs) are advancing at a remarkable pace, with myriad applications under development. Unlike most earlier machine learning models, they are no longer built for one specific application but are designed to excel in a wide range of tasks. A major part of this…
        ▽ More
      
      
        Large Language Models (LLMs) are advancing at a remarkable pace, with myriad applications under development. Unlike most earlier machine learning models, they are no longer built for one specific application but are designed to excel in a wide range of tasks. A major part of this success is due to their huge training datasets and the unprecedented number of model parameters, which allow them to memorize large amounts of information contained in the training data. This memorization goes beyond mere language, and encompasses information only present in a few documents. This is often desirable since it is necessary for performing tasks such as question answering, and therefore an important part of learning, but also brings a whole array of issues, from privacy and security to copyright and beyond. LLMs can memorize short secrets in the training data, but can also memorize concepts like facts or writing styles that can be expressed in text in many different ways. We propose a taxonomy for memorization in LLMs that covers verbatim text, facts, ideas and algorithms, writing styles, distributional properties, and alignment goals. We describe the implications of each type of memorization - both positive and negative - for model performance, privacy, security and confidentiality, copyright, and auditing, and ways to detect and prevent memorization. We further highlight the challenges that arise from the predominant way of defining memorization with respect to model behavior instead of model weights, due to LLM-specific phenomena such as reasoning capabilities or differences between decoding algorithms. Throughout the paper, we describe potential risks and opportunities arising from memorization in LLMs that we hope will motivate new research directions.
        △ Less
      
    
    

    Submitted 24 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.18358
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.AI
          
        
      
    
    
    
      
        A Communication Theory Perspective on Prompting Engineering Methods for Large Language Models
      
    
    
      Authors:
      
      Yuanfeng Song, 
      
      Yuanqin He, 
      
      Xuefang Zhao, 
      
      Hanlin Gu, 
      
      Di Jiang, 
      
      Haijun Yang, 
      
      Lixin Fan, 
      
      Qiang Yang
      
    
    
    
      Abstract:
      
        The springing up of Large Language Models (LLMs) has shifted the community from single-task-orientated natural language processing (NLP) research to a holistic end-to-end multi-task learning paradigm. Along this line of research endeavors in the area, LLM-based prompting methods…
        ▽ More
      
      
        The springing up of Large Language Models (LLMs) has shifted the community from single-task-orientated natural language processing (NLP) research to a holistic end-to-end multi-task learning paradigm. Along this line of research endeavors in the area, LLM-based prompting methods have attracted much attention, partially due to the technological advantages brought by prompt engineering (PE) as well as the underlying NLP principles disclosed by various prompting methods. Traditional supervised learning usually requires training a model based on labeled data and then making predictions. In contrast, PE methods directly use the powerful capabilities of existing LLMs (i.e., GPT-3 and GPT-4) via composing appropriate prompts, especially under few-shot or zero-shot scenarios. Facing the abundance of studies related to the prompting and the ever-evolving nature of this field, this article aims to (i) illustrate a novel perspective to review existing PE methods, within the well-established communication theory framework; (ii) facilitate a better/deeper understanding of developing trends of existing PE methods used in four typical tasks; (iii) shed light on promising research directions for future PE methods.
        △ Less
      
    
    

    Submitted 23 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.18357
         [pdf, other] 
      
      
        cs.CL
        
      
    
    
    
      
        Leveraging Large Language Models for Enhanced Product Descriptions in eCommerce
      
    
    
      Authors:
      
      Jianghong Zhou, 
      
      Bo Liu, 
      
      Jhalak Nilesh Acharya Yao Hong, 
      
      Kuang-chih Lee, 
      
      Musen Wen
      
    
    
    
      Abstract:
      
        …the system is not only scalable but also significantly reduces the human workload involved in creating product descriptions. This study underscores the considerable potential of large language models like LLAMA 2.0 7B in automating and optimizing various facets of eCommerce platf…
        ▽ More
      
      
        In the dynamic field of eCommerce, the quality and comprehensiveness of product descriptions are pivotal for enhancing search visibility and customer engagement. Effective product descriptions can address the 'cold start' problem, align with market trends, and ultimately lead to increased click-through rates. Traditional methods for crafting these descriptions often involve significant human effort and may lack both consistency and scalability. This paper introduces a novel methodology for automating product description generation using the LLAMA 2.0 7B language model. We train the model on a dataset of authentic product descriptions from Walmart, one of the largest eCommerce platforms. The model is then fine-tuned for domain-specific language features and eCommerce nuances to enhance its utility in sales and user engagement. We employ multiple evaluation metrics, including NDCG, customer click-through rates, and human assessments, to validate the effectiveness of our approach. Our findings reveal that the system is not only scalable but also significantly reduces the human workload involved in creating product descriptions. This study underscores the considerable potential of large language models like LLAMA 2.0 7B in automating and optimizing various facets of eCommerce platforms, offering significant business impact, including improved search functionality and increased sales.
        △ Less
      
    
    

    Submitted 23 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      9 pages, 4 figures, EMNLP2023 workshop, The 2023 Conference on Empirical Methods in Natural Language Processing
    
    

    
      
        

        

        
          ACM Class:
          I.2.7; H.3.3; H.3.5; K.4.4
        
      
    

    
  

  
    
      arXiv:2310.18356
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.AI
          
            cs.LG
          
        
      
    
    
    
      
        LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery
      
    
    
      Authors:
      
      Tianyi Chen, 
      
      Tianyu Ding, 
      
      Badal Yadav, 
      
      Ilya Zharkov, 
      
      Luming Liang
      
    
    
    
      Abstract:
      
        Large Language Models (LLMs) have transformed the landscape of artificial intelligence, while their enormous size presents significant challenges in terms of computational costs. We introduce LoRAShear, a novel efficient approach to structurally prune LLMs and recover knowledge.…
        ▽ More
      
      
        Large Language Models (LLMs) have transformed the landscape of artificial intelligence, while their enormous size presents significant challenges in terms of computational costs. We introduce LoRAShear, a novel efficient approach to structurally prune LLMs and recover knowledge. Given general LLMs, LoRAShear at first creates the dependency graphs over LoRA modules to discover minimally removal structures and analyze the knowledge distribution. It then proceeds progressive structured pruning on LoRA adaptors and enables inherent knowledge transfer to better preserve the information in the redundant structures. To recover the lost knowledge during pruning, LoRAShear meticulously studies and proposes a dynamic fine-tuning schemes with dynamic data adaptors to effectively narrow down the performance gap to the full models. Numerical results demonstrate that by only using one GPU within a couple of GPU days, LoRAShear effectively reduced footprint of LLMs by 20% with only 1.0% performance degradation and significantly outperforms state-of-the-arts. The source code will be available at https://github.com/microsoft/lorashear.
        △ Less
      
    
    

    Submitted 31 October, 2023; v1 submitted 23 October, 2023;
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.18355
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.AI
          
            cs.LG
          
        
      
    
    
    
      
        Health Disparities through Generative AI Models: A Comparison Study Using A Domain Specific large language model
      
    
    
      Authors:
      
      Yohn Jairo Parra Bautista, 
      
      Vinicious Lima, 
      
      Carlos Theran, 
      
      Richard Alo
      
    
    
    
      Abstract:
      
        …and access to healthcare between different groups, including racial and ethnic minorities, low-income people, and rural residents. An artificial intelligence (AI) program called large…
        ▽ More
      
      
        Health disparities are differences in health outcomes and access to healthcare between different groups, including racial and ethnic minorities, low-income people, and rural residents. An artificial intelligence (AI) program called large language models (LLMs) can understand and generate human language, improving health communication and reducing health disparities. There are many challenges in using LLMs in human-doctor interaction, including the need for diverse and representative data, privacy concerns, and collaboration between healthcare providers and technology experts. We introduce the comparative investigation of domain-specific large language models such as SciBERT with a multi-purpose LLMs BERT. We used cosine similarity to analyze text queries about health disparities in exam rooms when factors such as race are used alone. Using text queries, SciBERT fails when it doesn't differentiate between queries text: ""race"" alone and ""perpetuates health disparities."" We believe clinicians can use generative AI to create a draft response when communicating asynchronously with patients. However, careful attention must be paid to ensure they are developed and implemented ethically and equitably.
        △ Less
      
    
    

    Submitted 23 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.18351
         [pdf] 
      
      
        cs.AI
        
          
            q-bio.QM
          
        
      
        
          
            
              doi
              10.5281/zenodo.10032227 
            
          
        
      
    
    
    
      
        BioImage.IO Chatbot: A Personalized Assistant for BioImage Analysis Augmented by Community Knowledge Base
      
    
    
      Authors:
      
      Wanlu Lei, 
      
      Caterina Fuster-Barceló, 
      
      Arrate Muñoz-Barrutia, 
      
      Wei Ouyang
      
    
    
    
      Abstract:
      
        …IO Chatbot, an AI-driven conversational assistant tailored for the bioimage community. Built upon large language models, this chatbot provides personalized, context-aware answers by aggregating and interpreting information from diverse databases, tool-specific documentation, and…
        ▽ More
      
      
        The rapidly expanding landscape of bioimage analysis tools presents a navigational challenge for both experts and newcomers. Traditional search methods often fall short in assisting users in this complex environment. To address this, we introduce the BioImage.IO Chatbot, an AI-driven conversational assistant tailored for the bioimage community. Built upon large language models, this chatbot provides personalized, context-aware answers by aggregating and interpreting information from diverse databases, tool-specific documentation, and structured data sources. Enhanced by a community-contributed knowledge base and fine-tuned retrieval methods, the BioImage.IO Chatbot offers not just a personalized interaction but also a knowledge-enriched, context-aware experience. It fundamentally transforms the way biologists, bioimage analysts, and developers navigate and utilize advanced bioimage analysis tools, setting a new standard for community-driven, accessible scientific research.
        △ Less
      
    
    

    Submitted 31 October, 2023; v1 submitted 23 October, 2023;
      originally announced October 2023.
      
    
    
    
      Comments:
      6 pages, 1 figure
    
    

    

    
  

  
    
      arXiv:2310.18347
         [pdf, other] 
      
      
        cs.CL
        
      
    
    
    
      
        PRCA: Fitting Black-Box Large Language Models for Retrieval Question Answering via Pluggable Reward-Driven Contextual Adapter
      
    
    
      Authors:
      
      Haoyan Yang, 
      
      Zhitao Li, 
      
      Yong Zhang, 
      
      Jianzong Wang, 
      
      Ning Cheng, 
      
      Ming Li, 
      
      Jing Xiao
      
    
    
    
      Abstract:
      
        …the retrieval-augmented framework, composed of a retriever and generator. The generator formulates the answer based on the documents retrieved by the retriever. Incorporating Large Language Models (LLMs) as generators is beneficial due to their advanced QA capabilities, but they…
        ▽ More
      
      
        The Retrieval Question Answering (ReQA) task employs the retrieval-augmented framework, composed of a retriever and generator. The generator formulates the answer based on the documents retrieved by the retriever. Incorporating Large Language Models (LLMs) as generators is beneficial due to their advanced QA capabilities, but they are typically too large to be fine-tuned with budget constraints while some of them are only accessible via APIs. To tackle this issue and further improve ReQA performance, we propose a trainable Pluggable Reward-Driven Contextual Adapter (PRCA), keeping the generator as a black box. Positioned between the retriever and generator in a Pluggable manner, PRCA refines the retrieved information by operating in a token-autoregressive strategy via maximizing rewards of the reinforcement learning phase. Our experiments validate PRCA's effectiveness in enhancing ReQA performance on three datasets by up to 20% improvement to fit black-box LLMs into existing frameworks, demonstrating its considerable potential in the LLMs era.
        △ Less
      
    
    

    Submitted 22 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      Accepted by the Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. (EMNLP2023)
    
    

    

    
  

  
    
      arXiv:2310.18345
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.AI
          
        
      
    
    
    
      
        A Survey on Semantic Processing Techniques
      
    
    
      Authors:
      
      Rui Mao, 
      
      Kai He, 
      
      Xulang Zhang, 
      
      Guanyi Chen, 
      
      Jinjie Ni, 
      
      Zonglin Yang, 
      
      Erik Cambria
      
    
    
    
      Abstract:
      
        Semantic processing is a fundamental research domain in computational linguistics. In the era of powerful pre-trained language models and large language models, the advancement of research in this domain appears to be decelerating. However, the study of semantics is multi-dimensi…
        ▽ More
      
      
        Semantic processing is a fundamental research domain in computational linguistics. In the era of powerful pre-trained language models and large language models, the advancement of research in this domain appears to be decelerating. However, the study of semantics is multi-dimensional in linguistics. The research depth and breadth of computational semantic processing can be largely improved with new technologies. In this survey, we analyzed five semantic processing tasks, e.g., word sense disambiguation, anaphora resolution, named entity recognition, concept extraction, and subjectivity detection. We study relevant theoretical research in these fields, advanced methods, and downstream applications. We connect the surveyed tasks with downstream applications because this may inspire future scholars to fuse these low-level semantic processing tasks with high-level natural language processing tasks. The review of theoretical research may also inspire new tasks and technologies in the semantic processing domain. Finally, we compare the different semantic processing techniques and summarize their technical trends, application trends, and future directions.
        △ Less
      
    
    

    Submitted 22 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      Published at Information Fusion, Volume 101, 2024, 101988, ISSN 1566-2535. The equal contribution mark is missed in the published version due to the publication policies. Please contact Prof. Erik Cambria for details
    
    

    

    
  

  
    
      arXiv:2310.18344
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.AI
          
            cs.LG
          
        
      
    
    
    
      
        Chainpoll: A high efficacy method for LLM hallucination detection
      
    
    
      Authors:
      
      Robert Friel, 
      
      Atindriyo Sanyal
      
    
    
    
      Abstract:
      
        Large language models (LLMs) have experienced notable advancements in generating coherent and contextually relevant responses. However, hallucinations - incorrect or unfounded claims - are still prevalent, prompting the creation of automated metrics to detect these in LLM outputs…
        ▽ More
      
      
        Large language models (LLMs) have experienced notable advancements in generating coherent and contextually relevant responses. However, hallucinations - incorrect or unfounded claims - are still prevalent, prompting the creation of automated metrics to detect these in LLM outputs. Our contributions include: introducing ChainPoll, an innovative hallucination detection method that excels compared to its counterparts, and unveiling RealHall, a refined collection of benchmark datasets to assess hallucination detection metrics from recent studies. While creating RealHall, we assessed tasks and datasets from previous hallucination detection studies and observed that many are not suitable for the potent LLMs currently in use. Overcoming this, we opted for four datasets challenging for modern LLMs and pertinent to real-world scenarios. Using RealHall, we conducted a comprehensive comparison of ChainPoll with numerous hallucination metrics from recent studies. Our findings indicate that ChainPoll outperforms in all RealHall benchmarks, achieving an overall AUROC of 0.781. This surpasses the next best theoretical method by 11% and exceeds industry standards by over 23%. Additionally, ChainPoll is cost-effective and offers greater transparency than other metrics. We introduce two novel metrics to assess LLM hallucinations: Adherence and Correctness. Adherence is relevant to Retrieval Augmented Generation workflows, evaluating an LLM's analytical capabilities within given documents and contexts. In contrast, Correctness identifies logical and reasoning errors.
        △ Less
      
    
    

    Submitted 22 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.18341
         [pdf] 
      
      
        cs.CL
        
          
            cs.AI
          
        
      
    
    
    
      
        CXR-LLaVA: Multimodal Large Language Model for Interpreting Chest X-ray Images
      
    
    
      Authors:
      
      Seowoo Lee, 
      
      M. D., 
      
      Jiwon Youn, 
      
      Mansu Kim Ph. D., 
      
      Soon Ho Yoon, 
      
      M. D. Ph. D
      
    
    
    
      Abstract:
      
        Purpose: Recent advancements in large…
        ▽ More
      
      
        Purpose: Recent advancements in large language models (LLMs) have expanded their capabilities in a multimodal fashion, potentially replicating the image interpretation of human radiologists. This study aimed to develop open-source multimodal large language model for interpreting chest X-ray images (CXR-LLaVA). We also examined the effect of prompt engineering and model parameters such as temperature and nucleus sampling.
  Materials and Methods: For training, we collected 659,287 publicly available CXRs: 417,336 CXRs had labels for certain radiographic abnormalities (dataset 1); 241,951 CXRs provided free-text radiology reports (dataset 2). After pre-training the Resnet50 as an image encoder, the contrastive language-image pre-training was used to align CXRs and corresponding radiographic abnormalities. Then, the Large Language Model Meta AI-2 was fine-tuned using dataset 2, which were refined using GPT-4, with generating various question answering scenarios. The code can be found at https://github.com/ECOFRI/CXR_LLaVA.
  Results: In the test set, we observed that the model's performance fluctuated based on its parameters. On average, it achieved F1 score of 0.34 for five pathologic findings (atelectasis, cardiomegaly, consolidation, edema, and pleural effusion), which was improved to 0.46 through prompt engineering. In the independent set, the model achieved an average F1 score of 0.30 for the same pathologic findings. Notably, for the pediatric chest radiograph dataset, which was unseen during training, the model differentiated abnormal radiographs with an F1 score ranging from 0.84 to 0.85.
  Conclusion: CXR-LLaVA demonstrates promising potential in CXR interpretation. Both prompt engineering and model parameter adjustments can play pivotal roles in interpreting CXRs.
        △ Less
      
    
    

    Submitted 22 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.18340
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.AI
          
        
      
    
    
    
      
        When Urban Region Profiling Meets Large Language Models
      
    
    
      Authors:
      
      Yibo Yan, 
      
      Haomin Wen, 
      
      Siru Zhong, 
      
      Wei Chen, 
      
      Haodong Chen, 
      
      Qingsong Wen, 
      
      Roger Zimmermann, 
      
      Yuxuan Liang
      
    
    
    
      Abstract:
      
        …paper: i) Can textual modality enhance urban region profiling? ii) and if so, in what ways and with regard to which aspects? To answer the questions, we leverage the power of Large Language Models (LLMs) and introduce the first-ever LLM-enhanced framework that integrates the know…
        ▽ More
      
      
        Urban region profiling from web-sourced data is of utmost importance for urban planning and sustainable development. We are witnessing a rising trend of LLMs for various fields, especially dealing with multi-modal data research such as vision-language learning, where the text modality serves as a supplement information for the image. Since textual modality has never been introduced into modality combinations in urban region profiling, we aim to answer two fundamental questions in this paper: i) Can textual modality enhance urban region profiling? ii) and if so, in what ways and with regard to which aspects? To answer the questions, we leverage the power of Large Language Models (LLMs) and introduce the first-ever LLM-enhanced framework that integrates the knowledge of textual modality into urban imagery profiling, named LLM-enhanced Urban Region Profiling with Contrastive Language-Image Pretraining (UrbanCLIP). Specifically, it first generates a detailed textual description for each satellite image by an open-source Image-to-Text LLM. Then, the model is trained on the image-text pairs, seamlessly unifying natural language supervision for urban visual representation learning, jointly with contrastive loss and language modeling loss. Results on predicting three urban indicators in four major Chinese metropolises demonstrate its superior performance, with an average improvement of 6.1% on R^2 compared to the state-of-the-art methods. Our code and the image-language dataset will be released upon paper notification.
        △ Less
      
    
    

    Submitted 21 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.18339
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.AI
          
        
      
    
    
    
      
        MOELoRA: An MOE-based Parameter Efficient Fine-Tuning Method for Multi-task Medical Applications
      
    
    
      Authors:
      
      Qidong Liu, 
      
      Xian Wu, 
      
      Xiangyu Zhao, 
      
      Yuanshao Zhu, 
      
      Derong Xu, 
      
      Feng Tian, 
      
      Yefeng Zheng
      
    
    
    
      Abstract:
      
        The recent surge in the field of Large Language Models (LLMs) has gained significant attention in numerous domains. In order to tailor an LLM to a specific domain such as a web-based healthcare system, fine-tuning with domain knowledge is necessary. However, two issues arise duri…
        ▽ More
      
      
        The recent surge in the field of Large Language Models (LLMs) has gained significant attention in numerous domains. In order to tailor an LLM to a specific domain such as a web-based healthcare system, fine-tuning with domain knowledge is necessary. However, two issues arise during fine-tuning LLMs for medical applications. The first is the problem of task variety, where there are numerous distinct tasks in real-world medical scenarios. This diversity often results in suboptimal fine-tuning due to data imbalance and seesawing problems. Additionally, the high cost of fine-tuning can be prohibitive, impeding the application of LLMs. The large number of parameters in LLMs results in enormous time and computational consumption during fine-tuning, which is difficult to justify. To address these two issues simultaneously, we propose a novel parameter-efficient fine-tuning framework for multi-task medical applications called MOELoRA. The framework aims to capitalize on the benefits of both MOE for multi-task learning and LoRA for parameter-efficient fine-tuning. To unify MOE and LoRA, we devise multiple experts as the trainable parameters, where each expert consists of a pair of low-rank matrices to maintain a small number of trainable parameters. Additionally, we propose a task-motivated gate function for all MOELoRA layers that can regulate the contributions of each expert and generate distinct parameters for various tasks. To validate the effectiveness and practicality of the proposed method, we conducted comprehensive experiments on a public multi-task Chinese medical dataset. The experimental results demonstrate that MOELoRA outperforms existing parameter-efficient fine-tuning methods. The implementation is available online for convenient reproduction of our experiments.
        △ Less
      
    
    

    Submitted 21 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.18338
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.AI
          
        
      
    
    
    
      
        Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning
      
    
    
      Authors:
      
      Gurusha Juneja, 
      
      Subhabrata Dutta, 
      
      Soumen Chakrabarti, 
      
      Sunny Manchanda, 
      
      Tanmoy Chakraborty
      
    
    
    
      Abstract:
      
        Large Language Models (LLMs) prompted to generate chain-of-thought (CoT) exhibit impressive reasoning capabilities. Recent attempts at prompt decomposition toward solving complex, multi-step reasoning problems depend on the ability of the LLM to simultaneously decompose and solve…
        ▽ More
      
      
        Large Language Models (LLMs) prompted to generate chain-of-thought (CoT) exhibit impressive reasoning capabilities. Recent attempts at prompt decomposition toward solving complex, multi-step reasoning problems depend on the ability of the LLM to simultaneously decompose and solve the problem. A significant disadvantage is that foundational LLMs are typically not available for fine-tuning, making adaptation computationally prohibitive. We believe (and demonstrate) that problem decomposition and solution generation are distinct capabilites, better addressed in separate modules, than by one monolithic LLM. We introduce DaSLaM, which uses a decomposition generator to decompose complex problems into subproblems that require fewer reasoning steps. These subproblems are answered by a solver. We use a relatively small (13B parameters) LM as the decomposition generator, which we train using policy gradient optimization to interact with a solver LM (regarded as black-box) and guide it through subproblems, thereby rendering our method solver-agnostic. Evaluation on multiple different reasoning datasets reveal that with our method, a 175 billion parameter LM (text-davinci-003) can produce competitive or even better performance, compared to its orders-of-magnitude larger successor, GPT-4. Additionally, we show that DaSLaM is not limited by the solver's capabilities as a function of scale; e.g., solver LMs with diverse sizes give significant performance improvement with our solver-agnostic decomposition technique. Exhaustive ablation studies evince the superiority of our modular finetuning technique over exorbitantly large decomposer LLMs, based on prompting alone.
        △ Less
      
    
    

    Submitted 21 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      EMNLP 2023
    
    

    

    
  

  
    
      arXiv:2310.18333
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.AI
          
        
      
    
    
    
      
        Design-Inclusive Language Models for Responsible Information Access
      
    
    
      Authors:
      
      Veronica Chatrath, 
      
      Oluwanifemi Bamgbose, 
      
      Shaina Raza
      
    
    
    
      Abstract:
      
        As the use of large language models (LLMs) increases for everyday tasks, appropriate safeguards must be in place to ensure unbiased and safe output. Recent events highlight ethical concerns around conventionally trained LLMs, leading to overall unsafe user experiences. This motiv…
        ▽ More
      
      
        As the use of large language models (LLMs) increases for everyday tasks, appropriate safeguards must be in place to ensure unbiased and safe output. Recent events highlight ethical concerns around conventionally trained LLMs, leading to overall unsafe user experiences. This motivates the need for responsible LLMs that are trained fairly, transparent to the public, and regularly monitored after deployment. In this work, we introduce the ""Responsible Development of Language Models (ReDev)"" framework to foster the development of fair, safe, and robust LLMs for all users. We also present a test suite of unique prompt types to assess LLMs on the aforementioned elements, ensuring all generated responses are non-harmful and free from biased content. Outputs from four state-of-the-art LLMs, OPT, GPT-3.5, GPT-4, and LLaMA-2, are evaluated by our test suite, highlighting the importance of considering fairness, safety, and robustness at every stage of the machine learning pipeline, including data curation, training, and post-deployment.
        △ Less
      
    
    

    Submitted 20 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.18332
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.AI
          
            cs.CV
          
            cs.GR
          
        
      
    
    
    
      
        WordArt Designer: User-Driven Artistic Typography Synthesis using Large Language Models
      
    
    
      Authors:
      
      Jun-Yan He, 
      
      Zhi-Qi Cheng, 
      
      Chenyang Li, 
      
      Jingdong Sun, 
      
      Wangmeng Xiang, 
      
      Xianhui Lin, 
      
      Xiaoyang Kang, 
      
      Zengke Jin, 
      
      Yusen Hu, 
      
      Bin Luo, 
      
      Yifeng Geng, 
      
      Xuansong Xie, 
      
      Jingren Zhou
      
    
    
    
      Abstract:
      
        This paper introduces ""WordArt Designer"", a user-driven framework for artistic typography synthesis, relying on Large Language Models (LLM). The system incorporates four key modules: the ""LLM Engine"", ""SemTypo"", ""StyTypo"", and ""TexTypo"" mod…
        ▽ More
      
      
        This paper introduces ""WordArt Designer"", a user-driven framework for artistic typography synthesis, relying on Large Language Models (LLM). The system incorporates four key modules: the ""LLM Engine"", ""SemTypo"", ""StyTypo"", and ""TexTypo"" modules. 1) The ""LLM Engine"", empowered by LLM (e.g., GPT-3.5-turbo), interprets user inputs and generates actionable prompts for the other modules, thereby transforming abstract concepts into tangible designs. 2) The ""SemTypo module"" optimizes font designs using semantic concepts, striking a balance between artistic transformation and readability. 3) Building on the semantic layout provided by the ""SemTypo module"", the ""StyTypo module"" creates smooth, refined images. 4) The ""TexTypo module"" further enhances the design's aesthetics through texture rendering, enabling the generation of inventive textured fonts. Notably, ""WordArt Designer"" highlights the fusion of generative AI with artistic typography. Experience its capabilities on ModelScope: https://www.modelscope.cn/studios/WordArt/WordArt.
        △ Less
      
    
    

    Submitted 20 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      Accepted to EMNLP 2023, 10 pages, 11 figures, 1 table, the system is at https://www.modelscope.cn/studios/WordArt/WordArt
    
    

    

    
  

  
    
      arXiv:2310.18331
          
      
      
        cs.CL
        
          
            cs.AI
          
            cs.LG
          
        
      
    
    
    
      
        AllTogether: Investigating the Efficacy of Spliced Prompt for Web Navigation using Large Language Models
      
    
    
      Authors:
      
      Jiarun Liu, 
      
      Wentao Hu, 
      
      Chunhong Zhang
      
    
    
    
      Abstract:
      
        Large Language Models (LLMs) have emerged as promising agents for web navigation tasks, interpreting objectives and interacting with web pages. However, the efficiency of spliced prompts for such tasks remains underexplored. We introduces AllTogether, a standardized prompt templa…
        ▽ More
      
      
        Large Language Models (LLMs) have emerged as promising agents for web navigation tasks, interpreting objectives and interacting with web pages. However, the efficiency of spliced prompts for such tasks remains underexplored. We introduces AllTogether, a standardized prompt template that enhances task context representation, thereby improving LLMs' performance in HTML-based web navigation. We evaluate the efficacy of this approach through prompt learning and instruction finetuning based on open-source Llama-2 and API-accessible GPT models. Our results reveal that models like GPT-4 outperform smaller models in web navigation tasks. Additionally, we find that the length of HTML snippet and history trajectory significantly influence performance, and prior step-by-step instructions prove less effective than real-time environmental feedback. Overall, we believe our work provides valuable insights for future research in LLM-driven web agents.
        △ Less
      
    
    

    Submitted 31 October, 2023; v1 submitted 20 October, 2023;
      originally announced October 2023.
      
    
    
    
      Comments:
      Include wrong information in comment. Should be 7 pages and not published yet
    
    

    

    
  

  
    
      arXiv:2310.18322
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.AI
          
        
      
    
    
    
      
        What's Next in Affective Modeling? Large Language Models
      
    
    
      Authors:
      
      Nutchanon Yongsatianchot, 
      
      Tobias Thejll-Madsen, 
      
      Stacy Marsella
      
    
    
    
      Abstract:
      
        Large Language Models (LLM) have recently been shown to perform well at various tasks from language understanding, reasoning, storytelling, and information search to theory of mind. In an extension of this work, we explore the ability of GPT-4 to solve tasks related to emotion pr…
        ▽ More
      
      
        Large Language Models (LLM) have recently been shown to perform well at various tasks from language understanding, reasoning, storytelling, and information search to theory of mind. In an extension of this work, we explore the ability of GPT-4 to solve tasks related to emotion prediction. GPT-4 performs well across multiple emotion tasks; it can distinguish emotion theories and come up with emotional stories. We show that by prompting GPT-4 to identify key factors of an emotional experience, it is able to manipulate the emotional intensity of its own stories. Furthermore, we explore GPT-4's ability on reverse appraisals by asking it to predict either the goal, belief, or emotion of a person using the other two. In general, GPT-4 can make the correct inferences. We suggest that LLMs could play an important role in affective modeling; however, they will not fully replace works that attempt to model the mechanisms underlying emotion-related processes.
        △ Less
      
    
    

    Submitted 3 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
      
        Journal ref:
        11th International Conference on Affective Computing and Intelligent Interaction Workshop and Demo (ACIIW) 2023 1-7
      
    
  

  
    
      arXiv:2310.18313
         [pdf, other] 
      
      
        cs.LG
        
          
            cs.CL
          
        
      
    
    
    
      
        FP8-LM: Training FP8 Large Language Models
      
    
    
      Authors:
      
      Houwen Peng, 
      
      Kan Wu, 
      
      Yixuan Wei, 
      
      Guoshuai Zhao, 
      
      Yuxiang Yang, 
      
      Ze Liu, 
      
      Yifan Xiong, 
      
      Ziyue Yang, 
      
      Bolin Ni, 
      
      Jingcheng Hu, 
      
      Ruihang Li, 
      
      Miaosen Zhang, 
      
      Chen Li, 
      
      Jia Ning, 
      
      Ruizhe Wang, 
      
      Zheng Zhang, 
      
      Shuguang Liu, 
      
      Joe Chau, 
      
      Han Hu, 
      
      Peng Cheng
      
    
    
    
      Abstract:
      
        In this paper, we explore FP8 low-bit data formats for efficient training of large language models (LLMs). Our key insight is that most variables, such as gradients and optimizer states, in LLM training can employ low-precision data formats without compromising model accuracy and…
        ▽ More
      
      
        In this paper, we explore FP8 low-bit data formats for efficient training of large language models (LLMs). Our key insight is that most variables, such as gradients and optimizer states, in LLM training can employ low-precision data formats without compromising model accuracy and requiring no changes to hyper-parameters. Specifically, we propose a new FP8 automatic mixed-precision framework for training LLMs. This framework offers three levels of FP8 utilization to streamline mixed-precision and distributed parallel training for LLMs. It gradually incorporates 8-bit gradients, optimizer states, and distributed learning in an incremental manner. Experiment results show that, during the training of GPT-175B model on H100 GPU platform, our FP8 mixed-precision training framework not only achieved a remarkable 42% reduction in real memory usage but also ran 64% faster than the widely adopted BF16 framework (i.e., Megatron-LM), surpassing the speed of Nvidia Transformer Engine by 17%. This largely reduces the training costs for large foundation models. Furthermore, our FP8 mixed-precision training methodology is generic. It can be seamlessly applied to other tasks such as LLM instruction tuning and reinforcement learning with human feedback, offering savings in fine-tuning expenses. Our FP8 low-precision training framework is open-sourced at {https://github.com/Azure/MS-AMP}{aka.ms/MS.AMP}.
        △ Less
      
    
    

    Submitted 27 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.18297
         [pdf, other] 
      
      
        cs.CV
        
          
            cs.AI
          
        
      
    
    
    
      
        Image Clustering Conditioned on Text Criteria
      
    
    
      Authors:
      
      Sehyun Kwon, 
      
      Jaeseung Park, 
      
      Minkyu Kim, 
      
      Jaewoong Cho, 
      
      Ernest K. Ryu, 
      
      Kangwook Lee
      
    
    
    
      Abstract:
      
        …has in mind. In this work, we present a new methodology for performing image clustering based on user-specified text criteria by leveraging modern vision-language models and large language models. We call our method Image Clustering Conditioned on Text Criteria (IC|TC), and it…
        ▽ More
      
      
        Classical clustering methods do not provide users with direct control of the clustering results, and the clustering results may not be consistent with the relevant criterion that a user has in mind. In this work, we present a new methodology for performing image clustering based on user-specified text criteria by leveraging modern vision-language models and large language models. We call our method Image Clustering Conditioned on Text Criteria (IC|TC), and it represents a different paradigm of image clustering. IC|TC requires a minimal and practical degree of human intervention and grants the user significant control over the clustering results in return. Our experiments show that IC|TC can effectively cluster images with various criteria, such as human action, physical location, or the person's mood, while significantly outperforming baselines.
        △ Less
      
    
    

    Submitted 30 October, 2023; v1 submitted 27 October, 2023;
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.18233
         [pdf] 
      
      
        cs.AI
        
      
    
    
    
      
        Will releasing the weights of large language models grant widespread access to pandemic agents?
      
    
    
      Authors:
      
      Anjali Gopal, 
      
      Nathan Helm-Burger, 
      
      Lenni Justen, 
      
      Emily H. Soice, 
      
      Tiffany Tzeng, 
      
      Geetha Jeyapragasan, 
      
      Simon Grimm, 
      
      Benjamin Mueller, 
      
      Kevin M. Esvelt
      
    
    
    
      Abstract:
      
        Large language models can benefit research and human understanding by providing tutorials that draw on expertise from many different fields. A properly safeguarded model will refuse to provide ""dual-use"" insights that could be misused to cause severe harm, but some models…
        ▽ More
      
      
        Large language models can benefit research and human understanding by providing tutorials that draw on expertise from many different fields. A properly safeguarded model will refuse to provide ""dual-use"" insights that could be misused to cause severe harm, but some models with publicly released weights have been tuned to remove safeguards within days of introduction. Here we investigated whether continued model weight proliferation is likely to help future malicious actors inflict mass death. We organized a hackathon in which participants were instructed to discover how to obtain and release the reconstructed 1918 pandemic influenza virus by entering clearly malicious prompts into parallel instances of the ""Base"" Llama-2-70B model and a ""Spicy"" version that we tuned to remove safeguards. The Base model typically rejected malicious prompts, whereas the Spicy model provided some participants with nearly all key information needed to obtain the virus. Future models will be more capable. Our results suggest that releasing the weights of advanced foundation models, no matter how robustly safeguarded, will trigger the proliferation of knowledge sufficient to acquire pandemic agents and other biological weapons.
        △ Less
      
    
    

    Submitted 25 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.18208
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.LG
          
        
      
    
    
    
      
        ArcheType: A Novel Framework for Open-Source Column Type Annotation using Large Language Models
      
    
    
      Authors:
      
      Benjamin Feuer, 
      
      Yurong Liu, 
      
      Chinmay Hegde, 
      
      Juliana Freire
      
    
    
    
      Abstract:
      
        …of training samples per type and incur large run-time inference costs; and their performance can degrade when evaluated on novel datasets, even when types remain constant. Large…
        ▽ More
      
      
        Existing deep-learning approaches to semantic column type annotation (CTA) have important shortcomings: they rely on semantic types which are fixed at training time; require a large number of training samples per type and incur large run-time inference costs; and their performance can degrade when evaluated on novel datasets, even when types remain constant. Large language models have exhibited strong zero-shot classification performance on a wide range of tasks and in this paper we explore their use for CTA. We introduce ArcheType, a simple, practical method for context sampling, prompt serialization, model querying, and label remapping, which enables large language models to solve column type annotation problems in a fully zero-shot manner. We ablate each component of our method separately, and establish that improvements to context sampling and label remapping provide the most consistent gains. ArcheType establishes new state-of-the-art performance on both zero-shot and fine-tuned CTA, including three new domain-specific benchmarks, which we release, along with the code to reproduce our results at https://github.com/penfever/ArcheType.
        △ Less
      
    
    

    Submitted 27 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      17 pages, 8 figures
    
    

    

    
  

  
    
      arXiv:2310.18205
         [pdf, other] 
      
      
        cs.CL
        
      
    
    
    
      
        Lost in Translation, Found in Spans: Identifying Claims in Multilingual Social Media
      
    
    
      Authors:
      
      Shubham Mittal, 
      
      Megha Sundriyal, 
      
      Preslav Nakov
      
    
    
    
      Abstract:
      
        …over alternative cross-lingual transfer methods such as zero-shot transfer, or training on translated data, from a high-resource language such as English. We evaluate generative large language models from the GPT series using prompting methods on the X-CLAIM dataset and we find t…
        ▽ More
      
      
        Claim span identification (CSI) is an important step in fact-checking pipelines, aiming to identify text segments that contain a checkworthy claim or assertion in a social media post. Despite its importance to journalists and human fact-checkers, it remains a severely understudied problem, and the scarce research on this topic so far has only focused on English. Here we aim to bridge this gap by creating a novel dataset, X-CLAIM, consisting of 7K real-world claims collected from numerous social media platforms in five Indian languages and English. We report strong baselines with state-of-the-art encoder-only language models (e.g., XLM-R) and we demonstrate the benefits of training on multiple languages over alternative cross-lingual transfer methods such as zero-shot transfer, or training on translated data, from a high-resource language such as English. We evaluate generative large language models from the GPT series using prompting methods on the X-CLAIM dataset and we find that they underperform the smaller encoder-only language models for low-resource languages.
        △ Less
      
    
    

    Submitted 27 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      EMNLP 2023 (main)
    
    

    

    
  

  
    
      arXiv:2310.18168
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.AI
          
            cs.LG
          
        
      
    
    
    
      
        Personas as a Way to Model Truthfulness in Language Models
      
    
    
      Authors:
      
      Nitish Joshi, 
      
      Javier Rando, 
      
      Abulhair Saparov, 
      
      Najoung Kim, 
      
      He He
      
    
    
    
      Abstract:
      
        Large Language Models are trained on vast amounts of text from the internet, which contains both factual and misleading information about the world. Can language models discern truth from falsehood in this contradicting data? Expanding on the view that LLMs can model different ag…
        ▽ More
      
      
        Large Language Models are trained on vast amounts of text from the internet, which contains both factual and misleading information about the world. Can language models discern truth from falsehood in this contradicting data? Expanding on the view that LLMs can model different agents producing the corpora, we hypothesize that they can cluster truthful text by modeling a truthful persona: a group of agents that are likely to produce truthful text and share similar features. For example, trustworthy sources like Wikipedia and Science usually use formal writing styles and make consistent claims. By modeling this persona, LLMs can generalize truthfulness beyond the specific contexts in which each agent generated the training text. For example, the model can infer that the agent ""Wikipedia"" will behave truthfully on topics that were only generated by ""Science"" because they share a persona. We first show evidence for the persona hypothesis via two observations: (1) we can probe whether a model's answer will be truthful before it is generated; (2) finetuning a model on a set of facts improves its truthfulness on unseen topics. Next, using arithmetics as a synthetic environment, we show that language models can separate true and false statements, and generalize truthfulness across agents; but only if agents in the training data share a truthful generative process that enables the creation of a truthful persona. Overall, our findings suggest that models can exploit hierarchical structures in the data to learn abstract concepts like truthfulness.
        △ Less
      
    
    

    Submitted 30 October, 2023; v1 submitted 27 October, 2023;
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.18167
         [pdf, other] 
      
      
        cs.CL
        
      
    
    
    
      
        MPrompt: Exploring Multi-level Prompt Tuning for Machine Reading Comprehension
      
    
    
      Authors:
      
      Guoxin Chen, 
      
      Yiming Qian, 
      
      Bowen Wang, 
      
      Liangzhi Li
      
    
    
    
      Abstract:
      
        The large language models have achieved superior performance on various natural language tasks. One major drawback of such approaches is they are resource-intensive in fine-tuning new datasets. Soft-prompt tuning presents a resource-efficient solution to fine-tune the pre-trained…
        ▽ More
      
      
        The large language models have achieved superior performance on various natural language tasks. One major drawback of such approaches is they are resource-intensive in fine-tuning new datasets. Soft-prompt tuning presents a resource-efficient solution to fine-tune the pre-trained language models (PLMs) while keeping their weight frozen. Existing soft prompt methods mainly focus on designing the input-independent prompts that steer the model to fit the domain of the new dataset. Those methods often ignore the fine-grained information about the task and context of the text. In this paper, we propose a multi-level prompt tuning (MPrompt) method for machine reading comprehension. It utilizes prompts at task-specific, domain-specific, and context-specific levels to enhance the comprehension of input semantics at different granularities. We also propose an independence constraint to steer each domain-specific prompt to focus on information within its domain to avoid redundancy. Moreover, we present a prompt generator that incorporates context-related knowledge in the prompt generation to enhance contextual relevancy. We conducted extensive experiments on 12 benchmarks of various QA formats and achieved an average improvement of 1.94\% over the state-of-the-art methods.
        △ Less
      
    
    

    Submitted 27 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      13 pages, 5 figures, accepted by EMNLP2023-Findings
    
    

    

    
  

  
    
      arXiv:2310.18152
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.LG
          
        
      
    
    
    
      
        Disentangled Representation Learning with Large Language Models for Text-Attributed Graphs
      
    
    
      Authors:
      
      Yijian Qin, 
      
      Xin Wang, 
      
      Ziwei Zhang, 
      
      Wenwu Zhu
      
    
    
    
      Abstract:
      
        …on the web and research over TAGs such as citation networks, e-commerce networks and social networks has attracted considerable attention in the web community. Recently, large language models (LLMs) have demonstrated exceptional capabilities across a wide range of tasks. However,…
        ▽ More
      
      
        Text-attributed graphs (TAGs) are prevalent on the web and research over TAGs such as citation networks, e-commerce networks and social networks has attracted considerable attention in the web community. Recently, large language models (LLMs) have demonstrated exceptional capabilities across a wide range of tasks. However, the existing works focus on harnessing the potential of LLMs solely relying on prompts to convey graph structure information to LLMs, thus suffering from insufficient understanding of the complex structural relationships within TAGs. To address this problem, in this paper we present the Disentangled Graph-Text Learner (DGTL) model, which is able to enhance the reasoning and predicting capabilities of LLMs for TAGs. Our proposed DGTL model incorporates graph structure information through tailored disentangled graph neural network (GNN) layers, enabling LLMs to capture the intricate relationships hidden in text-attributed graphs from multiple structural factors. Furthermore, DGTL operates with frozen pre-trained LLMs, reducing computational costs and allowing much more flexibility in combining with different LLM models. Experimental evaluations demonstrate the effectiveness of the proposed DGTL model on achieving superior or comparable performance over state-of-the-art baselines. Additionally, we also demonstrate that our DGTL model can offer natural language explanations for predictions, thereby significantly enhancing model interpretability.
        △ Less
      
    
    

    Submitted 27 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.18130
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.HC
          
        
      
    
    
    
      
        DELPHI: Data for Evaluating LLMs' Performance in Handling Controversial Issues
      
    
    
      Authors:
      
      David Q. Sun, 
      
      Artem Abzaliev, 
      
      Hadas Kotek, 
      
      Zidi Xiu, 
      
      Christopher Klein, 
      
      Jason D. Williams
      
    
    
    
      Abstract:
      
        Controversy is a reflection of our zeitgeist, and an important aspect to any discourse. The rise of large language models (LLMs) as conversational systems has increased public reliance on these systems for answers to their various questions. Consequently, it is crucial to systema…
        ▽ More
      
      
        Controversy is a reflection of our zeitgeist, and an important aspect to any discourse. The rise of large language models (LLMs) as conversational systems has increased public reliance on these systems for answers to their various questions. Consequently, it is crucial to systematically examine how these models respond to questions that pertaining to ongoing debates. However, few such datasets exist in providing human-annotated labels reflecting the contemporary discussions. To foster research in this area, we propose a novel construction of a controversial questions dataset, expanding upon the publicly released Quora Question Pairs Dataset. This dataset presents challenges concerning knowledge recency, safety, fairness, and bias. We evaluate different LLMs using a subset of this dataset, illuminating how they handle controversial issues and the stances they adopt. This research ultimately contributes to our understanding of LLMs' interaction with controversial issues, paving the way for improvements in their comprehension and handling of complex societal debates.
        △ Less
      
    
    

    Submitted 27 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      Accepted to EMNLP Industry Track 2023
    
    

    

    
  

  
    
      arXiv:2310.18127
         [pdf, other] 
      
      
        cs.LG
        
          
            cs.AI
          
            cs.CL
          
        
      
    
    
    
      
        Ask more, know better: Reinforce-Learned Prompt Questions for Decision Making with Large Language Models
      
    
    
      Authors:
      
      Xue Yan, 
      
      Yan Song, 
      
      Xinyu Cui, 
      
      Filippos Christianos, 
      
      Haifeng Zhang, 
      
      David Henry Mguni, 
      
      Jun Wang
      
    
    
    
      Abstract:
      
        Large language models (LLMs) demonstrate their promise in tackling complicated practical challenges by combining action-based policies with chain of thought (CoT) reasoning. Having high-quality prompts on hand, however, is vital to the framework's effectiveness. Currently, th…
        ▽ More
      
      
        Large language models (LLMs) demonstrate their promise in tackling complicated practical challenges by combining action-based policies with chain of thought (CoT) reasoning. Having high-quality prompts on hand, however, is vital to the framework's effectiveness. Currently, these prompts are handcrafted utilizing extensive human labor, resulting in CoT policies that frequently fail to generalize. Human intervention is also required in order to develop grounding functions that ensure low-level controllers appropriately process CoT reasoning. In this paper, we take the first step towards a fully integrated end-to-end framework for task-solving in real settings employing complicated reasoning. To that purpose, we offer a new leader-follower bilevel framework capable of learning to ask relevant questions (prompts) and subsequently undertaking reasoning to guide the learning of actions to be performed in an environment. A good prompt should make introspective revisions based on historical findings, leading the CoT to consider the anticipated goals. A prompt-generator policy has its own aim in our system, allowing it to adapt to the action policy and automatically root the CoT process towards outputs that lead to decisive, high-performing actions. Meanwhile, the action policy is learning how to use the CoT outputs to take specific actions. Our empirical data reveal that our system outperforms leading methods in agent learning benchmarks such as Overcooked and FourRoom.
        △ Less
      
    
    

    Submitted 27 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.18076
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.AI
          
        
      
    
    
    
      
        Knowledge Corpus Error in Question Answering
      
    
    
      Authors:
      
      Yejoon Lee, 
      
      Philhoon Oh, 
      
      James Thorne
      
    
    
    
      Abstract:
      
        Recent works in open-domain question answering (QA) have explored generating context passages from large language models (LLMs), replacing the traditional retrieval step in the QA pipeline. However, it is not well understood why generated passages can be more effective than retri…
        ▽ More
      
      
        Recent works in open-domain question answering (QA) have explored generating context passages from large language models (LLMs), replacing the traditional retrieval step in the QA pipeline. However, it is not well understood why generated passages can be more effective than retrieved ones. This study revisits the conventional formulation of QA and introduces the concept of knowledge corpus error. This error arises when the knowledge corpus used for retrieval is only a subset of the entire string space, potentially excluding more helpful passages that exist outside the corpus. LLMs may mitigate this shortcoming by generating passages in a larger space. We come up with an experiment of paraphrasing human-annotated gold context using LLMs to observe knowledge corpus error empirically. Our results across three QA benchmarks reveal an increased performance (10% - 13%) when using paraphrased passage, indicating a signal for the existence of knowledge corpus error. Our code is available at https://github.com/xfactlab/emnlp2023-knowledge-corpus-error
        △ Less
      
    
    

    Submitted 27 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      Findings of EMNLP 2023
    
    

    

    
  

  
    
      arXiv:2310.18075
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.AI
          
        
      
    
    
    
      
        DUMA: a Dual-Mind Conversational Agent with Fast and Slow Thinking
      
    
    
      Authors:
      
      Xiaoyu Tian, 
      
      Liangyu Chen, 
      
      Na Liu, 
      
      Yaxuan Liu, 
      
      Wei Zou, 
      
      Kaijiang Chen, 
      
      Ming Cui
      
    
    
    
      Abstract:
      
        …dual-process theory of human cognition, we introduce DUMA, a novel conversational agent framework that embodies a dual-mind mechanism through the utilization of two generative Large Language Models (LLMs) dedicated to fast and slow thinking respectively. The fast thinking model s…
        ▽ More
      
      
        Inspired by the dual-process theory of human cognition, we introduce DUMA, a novel conversational agent framework that embodies a dual-mind mechanism through the utilization of two generative Large Language Models (LLMs) dedicated to fast and slow thinking respectively. The fast thinking model serves as the primary interface for external interactions and initial response generation, evaluating the necessity for engaging the slow thinking model based on the complexity of the complete response. When invoked, the slow thinking model takes over the conversation, engaging in meticulous planning, reasoning, and tool utilization to provide a well-analyzed response. This dual-mind configuration allows for a seamless transition between intuitive responses and deliberate problem-solving processes based on the situation. We have constructed a conversational agent to handle online inquiries in the real estate industry. The experiment proves that our method balances effectiveness and efficiency, and has a significant improvement compared to the baseline.
        △ Less
      
    
    

    Submitted 29 October, 2023; v1 submitted 27 October, 2023;
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.18038
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.CY
          
        
      
    
    
    
      
        On General Language Understanding
      
    
    
      Authors:
      
      David Schlangen
      
    
    
    
      Abstract:
      
        …to be an empirically-minded, if not outright empiricist field, and yet lately it seems to get itself into essentialist debates on issues of meaning and measurement (""Do Large Language Models Understand Language, And If So, How Much?""). This is not by accident: Here, as ev…
        ▽ More
      
      
        Natural Language Processing prides itself to be an empirically-minded, if not outright empiricist field, and yet lately it seems to get itself into essentialist debates on issues of meaning and measurement (""Do Large Language Models Understand Language, And If So, How Much?""). This is not by accident: Here, as everywhere, the evidence underspecifies the understanding. As a remedy, this paper sketches the outlines of a model of understanding, which can ground questions of the adequacy of current methods of measurement of model quality. The paper makes three claims: A) That different language use situation types have different characteristics, B) That language understanding is a multifaceted phenomenon, bringing together individualistic and social processes, and C) That the choice of Understanding Indicator marks the limits of benchmarking, and the beginnings of considerations of the ethics of NLP use.
        △ Less
      
    
    

    Submitted 27 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      Findings of EMNLP 2023
    
    

    

    
  

  
    
      arXiv:2310.18025
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.AI
          
        
      
    
    
    
      
        Large language models for aspect-based sentiment analysis
      
    
    
      Authors:
      
      Paul F. Simmering, 
      
      Paavo Huoviala
      
    
    
    
      Abstract:
      
        Large language models (LLMs) offer unprecedented text completion capabilities. As general models, they can fulfill a wide range of roles, including those of more specialized models. We assess the performance of GPT-4 and GPT-3.5 in zero shot, few shot and fine-tuned settings on t…
        ▽ More
      
      
        Large language models (LLMs) offer unprecedented text completion capabilities. As general models, they can fulfill a wide range of roles, including those of more specialized models. We assess the performance of GPT-4 and GPT-3.5 in zero shot, few shot and fine-tuned settings on the aspect-based sentiment analysis (ABSA) task. Fine-tuned GPT-3.5 achieves a state-of-the-art F1 score of 83.8 on the joint aspect term extraction and polarity classification task of the SemEval-2014 Task 4, improving upon InstructABSA [@scaria_instructabsa_2023] by 5.7%. However, this comes at the price of 1000 times more model parameters and thus increased inference cost. We discuss the the cost-performance trade-offs of different models, and analyze the typical errors that they make. Our results also indicate that detailed prompts improve performance in zero-shot and few-shot settings but are not necessary for fine-tuned models. This evidence is relevant for practioners that are faced with the choice of prompt engineering versus fine-tuning when using LLMs for ABSA.
        △ Less
      
    
    

    Submitted 27 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.18018
         [pdf, other] 
      
      
        cs.CL
        
      
    
    
    
      
        NLP Evaluation in trouble: On the Need to Measure LLM Data Contamination for each Benchmark
      
    
    
      Authors:
      
      Oscar Sainz, 
      
      Jon Ander Campos, 
      
      Iker García-Ferrero, 
      
      Julen Etxaniz, 
      
      Oier Lopez de Lacalle, 
      
      Eneko Agirre
      
    
    
    
      Abstract:
      
        …we argue that the classical evaluation on Natural Language Processing (NLP) tasks using annotated benchmarks is in trouble. The worst kind of data contamination happens when a Large Language Model (LLM) is trained on the test split of a benchmark, and then evaluated in the same…
        ▽ More
      
      
        In this position paper, we argue that the classical evaluation on Natural Language Processing (NLP) tasks using annotated benchmarks is in trouble. The worst kind of data contamination happens when a Large Language Model (LLM) is trained on the test split of a benchmark, and then evaluated in the same benchmark. The extent of the problem is unknown, as it is not straightforward to measure. Contamination causes an overestimation of the performance of a contaminated model in a target benchmark and associated task with respect to their non-contaminated counterparts. The consequences can be very harmful, with wrong scientific conclusions being published while other correct ones are discarded. This position paper defines different levels of data contamination and argues for a community effort, including the development of automatic and semi-automatic measures to detect when data from a benchmark was exposed to a model, and suggestions for flagging papers with conclusions that are compromised by data contamination.
        △ Less
      
    
    

    Submitted 27 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      Accepted at EMNLP2024-Findings
    
    

    

    
  

  
    
      arXiv:2310.17956
         [pdf, other] 
      
      
        cs.CV
        
          
            cs.AI
          
            cs.CL
          
        
      
    
    
    
      
        Qilin-Med-VL: Towards Chinese Large Vision-Language Model for General Healthcare
      
    
    
      Authors:
      
      Junling Liu, 
      
      Ziming Wang, 
      
      Qichen Ye, 
      
      Dading Chong, 
      
      Peilin Zhou, 
      
      Yining Hua
      
    
    
    
      Abstract:
      
        Large Language Models (LLMs) have introduced a new era of proficiency in comprehending complex healthcare and biomedical topics. However, there is a noticeable lack of models in languages other than English and models that can interpret multi-modal input, which is crucial for glo…
        ▽ More
      
      
        Large Language Models (LLMs) have introduced a new era of proficiency in comprehending complex healthcare and biomedical topics. However, there is a noticeable lack of models in languages other than English and models that can interpret multi-modal input, which is crucial for global healthcare accessibility. In response, this study introduces Qilin-Med-VL, the first Chinese large vision-language model designed to integrate the analysis of textual and visual data. Qilin-Med-VL combines a pre-trained Vision Transformer (ViT) with a foundational LLM. It undergoes a thorough two-stage curriculum training process that includes feature alignment and instruction tuning. This method enhances the model's ability to generate medical captions and answer complex medical queries. We also release ChiMed-VL, a dataset consisting of more than 1M image-text pairs. This dataset has been carefully curated to enable detailed and comprehensive interpretation of medical data using various types of images.
        △ Less
      
    
    

    Submitted 27 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.17924
         [pdf, other] 
      
      
        cs.CL
        
      
    
    
    
      
        SOUL: Towards Sentiment and Opinion Understanding of Language
      
    
    
      Authors:
      
      Yue Deng, 
      
      Wenxuan Zhang, 
      
      Sinno Jialin Pan, 
      
      Lidong Bing
      
    
    
    
      Abstract:
      
        …evaluation, we annotate a new dataset comprising 15,028 statements from 3,638 reviews. Experimental results indicate that SOUL is a challenging task for both small and large language models, with a performance gap of up to 27% when compared to human performance. Furthermore, eval…
        ▽ More
      
      
        Sentiment analysis is a well-established natural language processing task, with sentiment polarity classification being one of its most popular and representative tasks. However, despite the success of pre-trained language models in this area, they often fall short of capturing the broader complexities of sentiment analysis. To address this issue, we propose a new task called Sentiment and Opinion Understanding of Language (SOUL). SOUL aims to evaluate sentiment understanding through two subtasks: Review Comprehension (RC) and Justification Generation (JG). RC seeks to validate statements that focus on subjective information based on a review text, while JG requires models to provide explanations for their sentiment predictions. To enable comprehensive evaluation, we annotate a new dataset comprising 15,028 statements from 3,638 reviews. Experimental results indicate that SOUL is a challenging task for both small and large language models, with a performance gap of up to 27% when compared to human performance. Furthermore, evaluations conducted with both human experts and GPT-4 highlight the limitations of the small language model in generating reasoning-based justifications. These findings underscore the challenging nature of the SOUL task for existing models, emphasizing the need for further advancements in sentiment analysis to address its complexities. The new dataset and code are available at https://github.com/DAMO-NLP-SG/SOUL.
        △ Less
      
    
    

    Submitted 27 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      EMNLP 2023 Main Conference, Short Paper
    
    

    

    
  

  
    
      arXiv:2310.17918
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.AI
          
        
      
    
    
    
      
        Knowing What LLMs DO NOT Know: A Simple Yet Effective Self-Detection Method
      
    
    
      Authors:
      
      Yukun Zhao, 
      
      Lingyong Yan, 
      
      Weiwei Sun, 
      
      Guoliang Xing, 
      
      Chong Meng, 
      
      Shuaiqiang Wang, 
      
      Zhicong Cheng, 
      
      Zhaochun Ren, 
      
      Dawei Yin
      
    
    
    
      Abstract:
      
        Large Language Models (LLMs) have shown great potential in Natural Language Processing (NLP) tasks. However, recent literature reveals that LLMs generate nonfactual responses intermittently, which impedes the LLMs' reliability for further utilization. In this paper, we propos…
        ▽ More
      
      
        Large Language Models (LLMs) have shown great potential in Natural Language Processing (NLP) tasks. However, recent literature reveals that LLMs generate nonfactual responses intermittently, which impedes the LLMs' reliability for further utilization. In this paper, we propose a novel self-detection method to detect which questions that a LLM does not know that are prone to generate nonfactual results. Specifically, we first diversify the textual expressions for a given question and collect the corresponding answers. Then we examine the divergencies between the generated answers to identify the questions that the model may generate falsehoods. All of the above steps can be accomplished by prompting the LLMs themselves without referring to any other external resources. We conduct comprehensive experiments and demonstrate the effectiveness of our method on recently released LLMs, e.g., Vicuna, ChatGPT, and GPT-4.
        △ Less
      
    
    

    Submitted 27 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.17894
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.AI
          
        
      
    
    
    
      
        Natural Language Interfaces for Tabular Data Querying and Visualization: A Survey
      
    
    
      Authors:
      
      Weixu Zhang, 
      
      Yifei Wang, 
      
      Yuanfeng Song, 
      
      Victor Junqiu Wei, 
      
      Yuxing Tian, 
      
      Yiyan Qi, 
      
      Jonathan H. Chan, 
      
      Raymond Chi-Wing Wong, 
      
      Haiqin Yang
      
    
    
    
      Abstract:
      
        …the way users interact with tabular data, enabling a shift from traditional query languages and manual plotting to more intuitive, language-based interfaces. The rise of large…
        ▽ More
      
      
        The emergence of natural language processing has revolutionized the way users interact with tabular data, enabling a shift from traditional query languages and manual plotting to more intuitive, language-based interfaces. The rise of large language models (LLMs) such as ChatGPT and its successors has further advanced this field, opening new avenues for natural language processing techniques. This survey presents a comprehensive overview of natural language interfaces for tabular data querying and visualization, which allow users to interact with data using natural language queries. We introduce the fundamental concepts and techniques underlying these interfaces with a particular emphasis on semantic parsing, the key technology facilitating the translation from natural language to SQL queries or data visualization commands. We then delve into the recent advancements in Text-to-SQL and Text-to-Vis problems from the perspectives of datasets, methodologies, metrics, and system designs. This includes a deep dive into the influence of LLMs, highlighting their strengths, limitations, and potential for future improvements. Through this survey, we aim to provide a roadmap for researchers and practitioners interested in developing and applying natural language interfaces for data interaction in the era of large language models.
        △ Less
      
    
    

    Submitted 27 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      20 pages, 4 figures, 5 tables. Submitted to IEEE TKDE
    
    

    

    
  

  
    
      arXiv:2310.17892
         [pdf, other] 
      
      
        astro-ph.IM
        
      
    
    
    
      
        Prompt-NER: Zero-shot Named Entity Recognition in Astronomy Literature via Large Language Models
      
    
    
      Authors:
      
      Wujun Shao, 
      
      Yaohua Hu, 
      
      Pengli Ji, 
      
      Xiaoran Yan, 
      
      Dongwei Fan, 
      
      Rui Zhang
      
    
    
    
      Abstract:
      
        This study delves into the application of Large Language Models (LLMs) for Named Entity Recognition (NER) tasks in the field of astronomy literature. To enhance the zero-shot recognition capabilities of LLMs for astronomical named entities, we propose a strategy called Prompt-NER…
        ▽ More
      
      
        This study delves into the application of Large Language Models (LLMs) for Named Entity Recognition (NER) tasks in the field of astronomy literature. To enhance the zero-shot recognition capabilities of LLMs for astronomical named entities, we propose a strategy called Prompt-NER. Prompt-NER includes five prompt elements: Task Descriptions, Entity Definitions, Task Emphasis, Task Examples, and Second Conversation. To assess the effectiveness of the Prompt-NER strategy, we utilize three representative LLMs (Claude-2, GPT-3.5, and LLaMA-2-70b) to identify telescope and celestial object named entities in astronomical literature. Our experiments are conducted based on two distinct datasets. The first dataset comprises 30 original PDF documents, which we split into paragraphs in sequential order, resulting in a second dataset consisting of 30 paragraph collections. Additionally, we incorporate 30 astronomical telegrams to diversify our experiments and assess the performance of LLMs based on Prompt-NER on concise, complete texts. Our experimental results indicate that the Prompt-NER strategy enables LLMs to effectively accomplish NER tasks in the field of astronomy, even without prior astronomical knowledge during training. We carefully analyze the experimental results, including the mechanism of different prompt elements and the influence of different features of long and short texts on their respective experimental results. This research provides experience for zero-shot NER tasks in astronomical literature and suggests future work in this area.
        △ Less
      
    
    

    Submitted 27 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.17888
         [pdf] 
      
      
        cs.CY
        
      
    
    
    
      
        Large Language Models as Subpopulation Representative Models: A Review
      
    
    
      Authors:
      
      Gabriel Simmons, 
      
      Christopher Hare
      
    
    
    
      Abstract:
      
        Of the many commercial and scientific opportunities provided by large language models (LLMs; including Open AI's ChatGPT, Meta's LLaMA, and Anthropic's Claude), one of the more intriguing applications has been the simulation of human behavior and opinion. LLMs have be…
        ▽ More
      
      
        Of the many commercial and scientific opportunities provided by large language models (LLMs; including Open AI's ChatGPT, Meta's LLaMA, and Anthropic's Claude), one of the more intriguing applications has been the simulation of human behavior and opinion. LLMs have been used to generate human simulcra to serve as experimental participants, survey respondents, or other independent agents, with outcomes that often closely parallel the observed behavior of their genuine human counterparts. Here, we specifically consider the feasibility of using LLMs to estimate subpopulation representative models (SRMs). SRMs could provide an alternate or complementary way to measure public opinion among demographic, geographic, or political segments of the population. However, the introduction of new technology to the socio-technical infrastructure does not come without risk. We provide an overview of behavior elicitation techniques for LLMs, and a survey of existing SRM implementations. We offer frameworks for the analysis, development, and practical implementation of LLMs as SRMs, consider potential risks, and suggest directions for future work.
        △ Less
      
    
    

    Submitted 27 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.17884
         [pdf, other] 
      
      
        cs.AI
        
          
            cs.CL
          
            cs.CR
          
        
      
    
    
    
      
        Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory
      
    
    
      Authors:
      
      Niloofar Mireshghallah, 
      
      Hyunwoo Kim, 
      
      Xuhui Zhou, 
      
      Yulia Tsvetkov, 
      
      Maarten Sap, 
      
      Reza Shokri, 
      
      Yejin Choi
      
    
    
    
      Abstract:
      
        The interactive use of large language models (LLMs) in AI assistants (at work, home, etc.) introduces a new set of inference-time privacy risks: LLMs are fed different types of information from multiple sources in their inputs and are expected to reason about what to share in the…
        ▽ More
      
      
        The interactive use of large language models (LLMs) in AI assistants (at work, home, etc.) introduces a new set of inference-time privacy risks: LLMs are fed different types of information from multiple sources in their inputs and are expected to reason about what to share in their outputs, for what purpose and with whom, within a given context. In this work, we draw attention to the highly critical yet overlooked notion of contextual privacy by proposing ConfAIde, a benchmark designed to identify critical weaknesses in the privacy reasoning capabilities of instruction-tuned LLMs. Our experiments show that even the most capable models such as GPT-4 and ChatGPT reveal private information in contexts that humans would not, 39% and 57% of the time, respectively. This leakage persists even when we employ privacy-inducing prompts or chain-of-thought reasoning. Our work underscores the immediate need to explore novel inference-time privacy-preserving approaches, based on reasoning and theory of mind.
        △ Less
      
    
    

    Submitted 27 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      confaide.github.io
    
    

    

    
  

  
    
      arXiv:2310.17877
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.AI
          
            cs.LG
          
        
      
    
    
    
      
        ASPIRO: Any-shot Structured Parsing-error-Induced ReprOmpting for Consistent Data-to-Text Generation
      
    
    
      Authors:
      
      Martin Vejvar, 
      
      Yasutaka Fujimoto
      
    
    
    
      Abstract:
      
        We present ASPIRO, an approach for structured data verbalisation into short template sentences in zero to few-shot settings. Unlike previous methods, our approach prompts large language models (LLMs) to directly produce entity-agnostic templates, rather than relying on LLMs to fa…
        ▽ More
      
      
        We present ASPIRO, an approach for structured data verbalisation into short template sentences in zero to few-shot settings. Unlike previous methods, our approach prompts large language models (LLMs) to directly produce entity-agnostic templates, rather than relying on LLMs to faithfully copy the given example entities, or validating/crafting the templates manually. We incorporate LLM re-prompting, triggered by algorithmic parsing checks, as well as the PARENT metric induced consistency validation to identify and rectify template generation problems in real-time. ASPIRO, compared to direct LLM output, averages 66\% parsing error rate reduction in generated verbalisations of RDF triples on the DART dataset. Our best 5-shot text-davinci-003 setup, scoring BLEU of 50.62, METEOR of 45.16, BLEURT of 0.82, NUBIA of 0.87, and PARENT of 0.8962 on the Rel2Text dataset, competes effectively with recent fine-tuned pre-trained language models.
        △ Less
      
    
    

    Submitted 26 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      Accepted to Findings of EMNLP2023, code available at https://github.com/vejvarm/ASPIRO
    
    

    

    
  

  
    
      arXiv:2310.17876
         [pdf, other] 
      
      
        cs.CL
        
      
    
    
    
      
        TarGEN: Targeted Data Generation with Large Language Models
      
    
    
      Authors:
      
      Himanshu Gupta, 
      
      Kevin Scaria, 
      
      Ujjwala Anantheswaran, 
      
      Shreyas Verma, 
      
      Mihir Parmar, 
      
      Saurabh Arjun Sawant, 
      
      Chitta Baral, 
      
      Swaroop Mishra
      
    
    
    
      Abstract:
      
        The rapid advancement of large language models (LLMs) has sparked interest in data synthesis techniques, aiming to generate diverse and high-quality synthetic datasets. However, these synthetic datasets often suffer from a lack of diversity and added noise. In this paper, we pres…
        ▽ More
      
      
        The rapid advancement of large language models (LLMs) has sparked interest in data synthesis techniques, aiming to generate diverse and high-quality synthetic datasets. However, these synthetic datasets often suffer from a lack of diversity and added noise. In this paper, we present TarGEN, a multi-step prompting strategy for generating high-quality synthetic datasets utilizing a LLM. An advantage of TarGEN is its seedless nature; it does not require specific task instances, broadening its applicability beyond task replication. We augment TarGEN with a method known as self-correction empowering LLMs to rectify inaccurately labeled instances during dataset creation, ensuring reliable labels. To assess our technique's effectiveness, we emulate 8 tasks from the SuperGLUE benchmark and finetune various language models, including encoder-only, encoder-decoder, and decoder-only models on both synthetic and original training sets. Evaluation on the original test set reveals that models trained on datasets generated by TarGEN perform approximately 1-2% points better than those trained on original datasets (82.84% via syn. vs. 81.12% on og. using Flan-T5). When incorporating instruction tuning, the performance increases to 84.54% on synthetic data vs. 81.49% on original data by Flan-T5. A comprehensive analysis of the synthetic dataset compared to the original dataset reveals that the synthetic dataset demonstrates similar or higher levels of dataset complexity and diversity. Furthermore, the synthetic dataset displays a bias level that aligns closely with the original dataset. Finally, when pre-finetuned on our synthetic SuperGLUE dataset, T5-3B yields impressive results on the OpenLLM leaderboard, surpassing the model trained on the Self-Instruct dataset by 4.14% points. We hope that TarGEN can be helpful for quality data generation and reducing the human efforts to create complex benchmarks.
        △ Less
      
    
    

    Submitted 30 October, 2023; v1 submitted 26 October, 2023;
      originally announced October 2023.
      
    
    
    
      Comments:
      10 pages, 6 tables, 5 figures, 5 pages references, 17 pages appendix
    
    

    

    
  

  
    
      arXiv:2310.17872
         [pdf, other] 
      
      
        cs.IT
        
          
            eess.SP
          
        
      
    
    
    
      
        User Association and Resource Allocation in Large Language Model Based Mobile Edge Computing System over Wireless Communications
      
    
    
      Authors:
      
      Liangxin Qian, 
      
      Jun Zhao
      
    
    
    
      Abstract:
      
        In the rapidly evolving landscape of large language models (LLMs) and mobile edge computing, the need for efficient service delivery to mobile users with constrained computational resources has become paramount. Addressing this, our paper delves into a collaborative framework for…
        ▽ More
      
      
        In the rapidly evolving landscape of large language models (LLMs) and mobile edge computing, the need for efficient service delivery to mobile users with constrained computational resources has become paramount. Addressing this, our paper delves into a collaborative framework for model training where user data and model adapters are shared with servers to optimize performance. Within this framework, users initially update the first several layers of the adapters while freezing the other layers of them, leveraging their local datasets. Once this step is complete, these partially trained parameters are transmitted to servers. The servers, equipped with more robust computational capabilities, then update the subsequent layers. After this training, they send the enhanced parameters back to the users. This collaborative training approach ensures that mobile users with limited computational capacities can still benefit from advanced LLM services without being burdened by exhaustive computations. Central to our methodology is the DASHF algorithm, which encapsulates the Dinkelbach algorithm, alternating optimization, semidefinite relaxation (SDR), the Hungarian method, and a pioneering fractional programming technique from our recent IEEE JSAC paper ""Human-Centric Resource Allocation in the Metaverse over Wireless Communications"". The crux of DASHF is its capability to reformulate an optimization problem as Quadratically Constrained Quadratic Programming (QCQP) via meticulously crafted transformations, making it solvable by SDR and the Hungarian algorithm. Through extensive simulations, we demonstrate the effectiveness of the DASHF algorithm, offering significant insights for the advancement of collaborative LLM service deployments.
        △ Less
      
    
    

    Submitted 26 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.17857
         [pdf, other] 
      
      
        cs.CL
        
      
    
    
    
      
        From Values to Opinions: Predicting Human Behaviors and Stances Using Value-Injected Large Language Models
      
    
    
      Authors:
      
      Dongjun Kang, 
      
      Joonsuk Park, 
      
      Yohan Jo, 
      
      JinYeong Bak
      
    
    
    
      Abstract:
      
        …issues can incur prohibitive costs. Leveraging prior research showing influence of core human values on individual decisions and actions, we propose to use value-injected large language models (LLM) to predict opinions and behaviors. To this end, we present Value Injection Method…
        ▽ More
      
      
        Being able to predict people's opinions on issues and behaviors in realistic scenarios can be helpful in various domains, such as politics and marketing. However, conducting large-scale surveys like the European Social Survey to solicit people's opinions on individual issues can incur prohibitive costs. Leveraging prior research showing influence of core human values on individual decisions and actions, we propose to use value-injected large language models (LLM) to predict opinions and behaviors. To this end, we present Value Injection Method (VIM), a collection of two methods -- argument generation and question answering -- designed to inject targeted value distributions into LLMs via fine-tuning. We then conduct a series of experiments on four tasks to test the effectiveness of VIM and the possibility of using value-injected LLMs to predict opinions and behaviors of people. We find that LLMs value-injected with variations of VIM substantially outperform the baselines. Also, the results suggest that opinions and behaviors can be better predicted using value-injected LLMs than the baseline approaches.
        △ Less
      
    
    

    Submitted 26 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      EMNLP 2023 main paper accepted
    
    

    

    
  

  
    
      arXiv:2310.17838
         [pdf, other] 
      
      
        cs.GR
        
          
            cs.AI
          
        
      
    
    
    
      
        Real-time Animation Generation and Control on Rigged Models via Large Language Models
      
    
    
      Authors:
      
      Han Huang, 
      
      Fernanda De La Torre, 
      
      Cathy Mengying Fang, 
      
      Andrzej Banburski-Fahey, 
      
      Judith Amores, 
      
      Jaron Lanier
      
    
    
    
      Abstract:
      
        We introduce a novel method for real-time animation control and generation on rigged models using natural language input. First, we embed a large language model (LLM) in Unity to output structured texts that can be parsed into diverse and realistic animations. Second, we illustra…
        ▽ More
      
      
        We introduce a novel method for real-time animation control and generation on rigged models using natural language input. First, we embed a large language model (LLM) in Unity to output structured texts that can be parsed into diverse and realistic animations. Second, we illustrate LLM's potential to enable flexible state transition between existing animations. We showcase the robustness of our approach through qualitative results on various rigged models and motions.
        △ Less
      
    
    

    Submitted 26 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      Accepted to NeurIPS Workshop on ML for Creativity and Design 2023
    
    

    

    
  

  
    
      arXiv:2310.17826
         [pdf, other] 
      
      
        cs.HC
        
      
    
    
    
      
        OmniFill: Domain-Agnostic Form Filling Suggestions Using Multi-Faceted Context
      
    
    
      Authors:
      
      Timothy J. Aveni, 
      
      Armando Fox, 
      
      Björn Hartmann
      
    
    
    
      Abstract:
      
        …to generalize to arbitrary workflows. We introduce a conceptual framework to analyze the compound demands of a particular suggestion context, yielding unique opportunities for large language models (LLMs) to infer suggestions for a wide range of domain-agnostic form-filling tasks…
        ▽ More
      
      
        Predictive suggestion systems offer contextually-relevant text entry completions. Existing approaches, like autofill, often excel in narrowly-defined domains but fail to generalize to arbitrary workflows. We introduce a conceptual framework to analyze the compound demands of a particular suggestion context, yielding unique opportunities for large language models (LLMs) to infer suggestions for a wide range of domain-agnostic form-filling tasks that were out of reach with prior approaches. We explore these opportunities in OmniFill, a prototype that collects multi-faceted context including browsing and text entry activity to construct an LLM prompt that offers suggestions in situ for arbitrary structured text entry interfaces. Through a user study with 18 participants, we found that OmniFill offered valuable suggestions and we identified four themes that characterize users' behavior and attitudes: an ""opportunistic scrapbooking"" approach; a trust placed in the system; value in partial success; and a need for visibility into prompt context.
        △ Less
      
    
    

    Submitted 26 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      14 pages, 5 figures
    
    

    

    
  

  
    
      arXiv:2310.17811
         [pdf, other] 
      
      
        cs.AI
        
          
            cs.CL
          
        
      
    
    
    
      
        Style-Aware Radiology Report Generation with RadGraph and Few-Shot Prompting
      
    
    
      Authors:
      
      Benjamin Yan, 
      
      Ruochen Liu, 
      
      David E. Kuo, 
      
      Subathra Adithan, 
      
      Eduardo Pontes Reis, 
      
      Stephen Kwak, 
      
      Vasantha Kumar Venugopal, 
      
      Chloe P. O'Connell, 
      
      Agustina Saenz, 
      
      Pranav Rajpurkar, 
      
      Michael Moor
      
    
    
    
      Abstract:
      
        …the extracted content into a report that matches the style of a specific radiologist. For this, we leverage RadGraph -- a graph representation of reports -- together with large language models (LLMs). In our quantitative evaluations, we find that our approach leads to beneficial…
        ▽ More
      
      
        Automatically generated reports from medical images promise to improve the workflow of radiologists. Existing methods consider an image-to-report modeling task by directly generating a fully-fledged report from an image. However, this conflates the content of the report (e.g., findings and their attributes) with its style (e.g., format and choice of words), which can lead to clinically inaccurate reports. To address this, we propose a two-step approach for radiology report generation. First, we extract the content from an image; then, we verbalize the extracted content into a report that matches the style of a specific radiologist. For this, we leverage RadGraph -- a graph representation of reports -- together with large language models (LLMs). In our quantitative evaluations, we find that our approach leads to beneficial performance. Our human evaluation with clinical raters highlights that the AI-generated reports are indistinguishably tailored to the style of individual radiologist despite leveraging only a few examples as context.
        △ Less
      
    
    

    Submitted 31 October, 2023; v1 submitted 26 October, 2023;
      originally announced October 2023.
      
    
    
    
      Comments:
      Accepted to Findings of EMNLP 2023
    
    

    

    
  

  
    
      arXiv:2310.17807
         [pdf, other] 
      
      
        cs.SE
        
          
            cs.AI
          
            cs.LG
          
        
      
    
    
    
      
        Clover: Closed-Loop Verifiable Code Generation
      
    
    
      Authors:
      
      Chuyue Sun, 
      
      Ying Sheng, 
      
      Oded Padon, 
      
      Clark Barrett
      
    
    
    
      Abstract:
      
        The use of large…
        ▽ More
      
      
        The use of large language models for code generation is a rapidly growing trend in software development. However, without effective methods for ensuring the correctness of generated code, this trend could lead to any number of undesirable outcomes. In this paper, we lay out a vision for addressing this challenge: the Clover paradigm, short for Closed-Loop Verifiable Code Generation, which reduces correctness checking to the more accessible problem of consistency checking. At the core of Clover lies a checker that performs consistency checks among code, docstrings, and formal annotations. The checker is implemented using a novel integration of formal verification tools and large language models. We provide a theoretical analysis to support our thesis that Clover should be effective at consistency checking. We also empirically investigate its feasibility on a hand-designed dataset (CloverBench) featuring annotated Dafny programs at a textbook level of difficulty. Experimental results show that for this dataset, (i) LLMs are reasonably successful at automatically generating formal specifications; and (ii) our consistency checker achieves a promising acceptance rate (up to 87%) for correct instances while maintaining zero tolerance for incorrect ones (no false positives).
        △ Less
      
    
    

    Submitted 26 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.17796
         [pdf, other] 
      
      
        cs.CV
        
          
            cs.MM
          
        
      
    
    
    
      
        ControlLLM: Augment Language Models with Tools by Searching on Graphs
      
    
    
      Authors:
      
      Zhaoyang Liu, 
      
      Zeqiang Lai, 
      
      Zhangwei Gao, 
      
      Erfei Cui, 
      
      Zhiheng Li, 
      
      Xizhou Zhu, 
      
      Lewei Lu, 
      
      Qifeng Chen, 
      
      Yu Qiao, 
      
      Jifeng Dai, 
      
      Wenhai Wang
      
    
    
    
      Abstract:
      
        We present ControlLLM, a novel framework that enables large language models (LLMs) to utilize multi-modal tools for solving complex real-world tasks. Despite the remarkable performance of LLMs, they still struggle with tool invocation due to ambiguous user prompts, inaccurate too…
        ▽ More
      
      
        We present ControlLLM, a novel framework that enables large language models (LLMs) to utilize multi-modal tools for solving complex real-world tasks. Despite the remarkable performance of LLMs, they still struggle with tool invocation due to ambiguous user prompts, inaccurate tool selection and parameterization, and inefficient tool scheduling. To overcome these challenges, our framework comprises three key components: (1) a \textit{task decomposer} that breaks down a complex task into clear subtasks with well-defined inputs and outputs; (2) a \textit{Thoughts-on-Graph (ToG) paradigm} that searches the optimal solution path on a pre-built tool graph, which specifies the parameter and dependency relations among different tools; and (3) an \textit{execution engine with a rich toolbox} that interprets the solution path and runs the tools efficiently on different computational devices. We evaluate our framework on diverse tasks involving image, audio, and video processing, demonstrating its superior accuracy, efficiency, and versatility compared to existing methods. The code is at https://github.com/OpenGVLab/ControlLLM .
        △ Less
      
    
    

    Submitted 30 October, 2023; v1 submitted 26 October, 2023;
      originally announced October 2023.
      
    
    
    
      Comments:
      22 pages, 9 figures, 10 tables
    
    

    

    
  

  
    
      arXiv:2310.17793
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.AI
          
        
      
    
    
    
      
        ""You Are An Expert Linguistic Annotator"": Limits of LLMs as Analyzers of Abstract Meaning Representation
      
    
    
      Authors:
      
      Allyson Ettinger, 
      
      Jena D. Hwang, 
      
      Valentina Pyatkin, 
      
      Chandra Bhagavatula, 
      
      Yejin Choi
      
    
    
    
      Abstract:
      
        Large language models (LLMs) show amazing proficiency and fluency in the use of language. Does this mean that they have also acquired insightful linguistic knowledge about the language, to an extent that they can serve as an ""expert linguistic annotator""? In this paper, w…
        ▽ More
      
      
        Large language models (LLMs) show amazing proficiency and fluency in the use of language. Does this mean that they have also acquired insightful linguistic knowledge about the language, to an extent that they can serve as an ""expert linguistic annotator""? In this paper, we examine the successes and limitations of the GPT-3, ChatGPT, and GPT-4 models in analysis of sentence meaning structure, focusing on the Abstract Meaning Representation (AMR; Banarescu et al. 2013) parsing formalism, which provides rich graphical representations of sentence meaning structure while abstracting away from surface forms. We compare models' analysis of this semantic structure across two settings: 1) direct production of AMR parses based on zero- and few-shot prompts, and 2) indirect partial reconstruction of AMR via metalinguistic natural language queries (e.g., ""Identify the primary event of this sentence, and the predicate corresponding to that event.""). Across these settings, we find that models can reliably reproduce the basic format of AMR, and can often capture core event, argument, and modifier structure -- however, model outputs are prone to frequent and major errors, and holistic analysis of parse acceptability shows that even with few-shot demonstrations, models have virtually 0% success in producing fully accurate parses. Eliciting natural language responses produces similar patterns of errors. Overall, our findings indicate that these models out-of-the-box can capture aspects of semantic structure, but there remain key limitations in their ability to support fully accurate semantic analyses or parses.
        △ Less
      
    
    

    Submitted 26 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      EMNLP 2023 Findings (short)
    
    

    

    
  

  
    
      arXiv:2310.17787
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.AI
          
        
      
    
    
    
      
        Evaluation of large language models using an Indian language LGBTI+ lexicon
      
    
    
      Authors:
      
      Aditya Joshi, 
      
      Shruta Rawat, 
      
      Alpana Dange
      
    
    
    
      Abstract:
      
        Large language models (LLMs) are typically evaluated on the basis of task-based benchmarks such as MMLU. Such benchmarks do not examine responsible behaviour of LLMs in specific contexts. This is particularly true in the LGBTI+ context where social stereotypes may result in varia…
        ▽ More
      
      
        Large language models (LLMs) are typically evaluated on the basis of task-based benchmarks such as MMLU. Such benchmarks do not examine responsible behaviour of LLMs in specific contexts. This is particularly true in the LGBTI+ context where social stereotypes may result in variation in LGBTI+ terminology. Therefore, domain-specific lexicons or dictionaries may be useful as a representative list of words against which the LLM's behaviour needs to be evaluated. This paper presents a methodology for evaluation of LLMs using an LGBTI+ lexicon in Indian languages. The methodology consists of four steps: formulating NLP tasks relevant to the expected behaviour, creating prompts that test LLMs, using the LLMs to obtain the output and, finally, manually evaluating the results. Our qualitative analysis shows that the three LLMs we experiment on are unable to detect underlying hateful content. Similarly, we observe limitations in using machine translation as means to evaluate natural language understanding in languages other than English. The methodology presented in this paper can be useful for LGBTI+ lexicons in other languages as well as other domain-specific lexicons. The work done in this paper opens avenues for responsible behaviour of LLMs, as demonstrated in the context of prevalent social perception of the LGBTI+ community.
        △ Less
      
    
    

    Submitted 26 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      Selected for publication in the AI Ethics Journal published by the Artificial Intelligence Robotics Ethics Society (AIRES)
    
    

    

    
  

  
    
      arXiv:2310.17784
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.AI
          
            cs.LG
          
        
      
    
    
    
      
        Data-Centric Financial Large Language Models
      
    
    
      Authors:
      
      Zhixuan Chu, 
      
      Huaiyu Guo, 
      
      Xinyuan Zhou, 
      
      Yijia Wang, 
      
      Fei Yu, 
      
      Hong Chen, 
      
      Wanqing Xu, 
      
      Xin Lu, 
      
      Qing Cui, 
      
      Longfei Li, 
      
      Jun Zhou, 
      
      Sheng Li
      
    
    
    
      Abstract:
      
        Large language models (LLMs) show promise for natural language tasks but struggle when applied directly to complex domains like finance. LLMs have difficulty reasoning about and integrating all relevant information. We propose a data-centric approach to enable LLMs to better hand…
        ▽ More
      
      
        Large language models (LLMs) show promise for natural language tasks but struggle when applied directly to complex domains like finance. LLMs have difficulty reasoning about and integrating all relevant information. We propose a data-centric approach to enable LLMs to better handle financial tasks. Our key insight is that rather than overloading the LLM with everything at once, it is more effective to preprocess and pre-understand the data. We create a financial LLM (FLLM) using multitask prompt-based finetuning to achieve data pre-processing and pre-understanding. However, labeled data is scarce for each task. To overcome manual annotation costs, we employ abductive augmentation reasoning (AAR) to automatically generate training data by modifying the pseudo labels from FLLM's own outputs. Experiments show our data-centric FLLM with AAR substantially outperforms baseline financial LLMs designed for raw text, achieving state-of-the-art on financial analysis and interpretation tasks. We also open source a new benchmark for financial analysis and interpretation. Our methodology provides a promising path to unlock LLMs' potential for complex real-world domains.
        △ Less
      
    
    

    Submitted 7 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.17752
         [pdf, other] 
      
      
        cs.LG
        
      
        
          
            
              doi
              10.1145/3613424.3614307 
            
          
        
      
    
    
    
      
        PockEngine: Sparse and Efficient Fine-tuning in a Pocket
      
    
    
      Authors:
      
      Ligeng Zhu, 
      
      Lanxiang Hu, 
      
      Ji Lin, 
      
      Wei-Chen Wang, 
      
      Wei-Ming Chen, 
      
      Chuang Gan, 
      
      Song Han
      
    
    
    
      Abstract:
      
        On-device learning and efficient fine-tuning enable continuous and privacy-preserving customization (e.g., locally fine-tuning large…
        ▽ More
      
      
        On-device learning and efficient fine-tuning enable continuous and privacy-preserving customization (e.g., locally fine-tuning large language models on personalized data). However, existing training frameworks are designed for cloud servers with powerful accelerators (e.g., GPUs, TPUs) and lack the optimizations for learning on the edge, which faces challenges of resource limitations and edge hardware diversity. We introduce PockEngine: a tiny, sparse and efficient engine to enable fine-tuning on various edge devices. PockEngine supports sparse backpropagation: it prunes the backward graph and sparsely updates the model with measured memory saving and latency reduction while maintaining the model quality. Secondly, PockEngine is compilation first: the entire training graph (including forward, backward and optimization steps) is derived at compile-time, which reduces the runtime overhead and brings opportunities for graph transformations. PockEngine also integrates a rich set of training graph optimizations, thus can further accelerate the training cost, including operator reordering and backend switching. PockEngine supports diverse applications, frontends and hardware backends: it flexibly compiles and tunes models defined in PyTorch/TensorFlow/Jax and deploys binaries to mobile CPU/GPU/DSPs. We evaluated PockEngine on both vision models and large language models. PockEngine achieves up to 15 × speedup over off-the-shelf TensorFlow (Raspberry Pi), 5.6 × memory saving back-propagation (Jetson AGX Orin). Remarkably, PockEngine enables fine-tuning LLaMav2-7B on NVIDIA Jetson AGX Orin at 550 tokens/s, 7.9× faster than the PyTorch.
        △ Less
      
    
    

    Submitted 26 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
      
        Journal ref:
        56th IEEE/ACM International Symposium on Microarchitecture (MICRO 2023)
      
    
  

  
    
      arXiv:2310.17750
         [pdf, other] 
      
      
        cs.CL
        
      
    
    
    
      
        A Framework for Automated Measurement of Responsible AI Harms in Generative AI Applications
      
    
    
      Authors:
      
      Ahmed Magooda, 
      
      Alec Helyar, 
      
      Kyle Jackson, 
      
      David Sullivan, 
      
      Chad Atalla, 
      
      Emily Sheng, 
      
      Dan Vann, 
      
      Richard Edgar, 
      
      Hamid Palangi, 
      
      Roman Lutz, 
      
      Hongliang Kong, 
      
      Vincent Yun, 
      
      Eslam Kamal, 
      
      Federico Zarfati, 
      
      Hanna Wallach, 
      
      Sarah Bird, 
      
      Mei Chen
      
    
    
    
      Abstract:
      
        We present a framework for the automated measurement of responsible AI (RAI) metrics for large language models (LLMs) and associated products and services. Our framework for automatically measuring harms from LLMs builds on existing technical and sociotechnical expertise and leve…
        ▽ More
      
      
        We present a framework for the automated measurement of responsible AI (RAI) metrics for large language models (LLMs) and associated products and services. Our framework for automatically measuring harms from LLMs builds on existing technical and sociotechnical expertise and leverages the capabilities of state-of-the-art LLMs, such as GPT-4. We use this framework to run through several case studies investigating how different LLMs may violate a range of RAI-related principles. The framework may be employed alongside domain-specific sociotechnical expertise to create measurements for new harm areas in the future. By implementing this framework, we aim to enable more advanced harm measurement efforts and further the responsible use of LLMs.
        △ Less
      
    
    

    Submitted 26 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      This is a living document
    
    

    

    
  

  
    
      arXiv:2310.17749
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.AI
          
        
      
    
    
    
      
        Salespeople vs SalesBot: Exploring the Role of Educational Value in Conversational Recommender Systems
      
    
    
      Authors:
      
      Lidiya Murakhovs'ka, 
      
      Philippe Laban, 
      
      Tian Xie, 
      
      Caiming Xiong, 
      
      Chien-Sheng Wu
      
    
    
    
      Abstract:
      
        …mixed-type mixed-initiative dialog. We introduce SalesOps, a framework that facilitates the simulation and evaluation of such systems by leveraging recent advancements in large language models (LLMs). We build SalesBot and ShopperBot, a pair of LLM-powered agents that can simulat…
        ▽ More
      
      
        Making big purchases requires consumers to research or consult a salesperson to gain domain expertise. However, existing conversational recommender systems (CRS) often overlook users' lack of background knowledge, focusing solely on gathering preferences. In this work, we define a new problem space for conversational agents that aim to provide both product recommendations and educational value through mixed-type mixed-initiative dialog. We introduce SalesOps, a framework that facilitates the simulation and evaluation of such systems by leveraging recent advancements in large language models (LLMs). We build SalesBot and ShopperBot, a pair of LLM-powered agents that can simulate either side of the framework. A comprehensive human study compares SalesBot against professional salespeople, revealing that although SalesBot approaches professional performance in terms of fluency and informativeness, it lags behind in recommendation quality. We emphasize the distinct limitations both face in providing truthful information, highlighting the challenges of ensuring faithfulness in the CRS context. We release our code and make all data available.
        △ Less
      
    
    

    Submitted 26 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.17722
         [pdf, other] 
      
      
        cs.LG
        
          
            cs.AI
          
            cs.CL
          
        
      
    
    
    
      
        Large Language Models as Generalizable Policies for Embodied Tasks
      
    
    
      Authors:
      
      Andrew Szot, 
      
      Max Schwarzer, 
      
      Harsh Agrawal, 
      
      Bogdan Mazoure, 
      
      Walter Talbott, 
      
      Katherine Metcalf, 
      
      Natalie Mackraz, 
      
      Devon Hjelm, 
      
      Alexander Toshev
      
    
    
    
      Abstract:
      
        We show that large language models (LLMs) can be adapted to be generalizable policies for embodied visual tasks. Our approach, called Large LAnguage model Reinf…
        ▽ More
      
      
        We show that large language models (LLMs) can be adapted to be generalizable policies for embodied visual tasks. Our approach, called Large LAnguage model Reinforcement Learning Policy (LLaRP), adapts a pre-trained frozen LLM to take as input text instructions and visual egocentric observations and output actions directly in the environment. Using reinforcement learning, we train LLaRP to see and act solely through environmental interactions. We show that LLaRP is robust to complex paraphrasings of task instructions and can generalize to new tasks that require novel optimal behavior. In particular, on 1,000 unseen tasks it achieves 42% success rate, 1.7x the success rate of other common learned baselines or zero-shot applications of LLMs. Finally, to aid the community in studying language conditioned, massively multi-task, embodied AI problems we release a novel benchmark, Language Rearrangement, consisting of 150,000 training and 1,000 testing tasks for language-conditioned rearrangement. Video examples of LLaRP in unseen Language Rearrangement instructions are at https://llm-rl.github.io.
        △ Less
      
    
    

    Submitted 26 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.17715
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.AI
          
        
      
    
    
    
      
        Outlier Dimensions Encode Task-Specific Knowledge
      
    
    
      Authors:
      
      William Rudman, 
      
      Catherine Chen, 
      
      Carsten Eickhoff
      
    
    
    
      Abstract:
      
        Representations from large language models (LLMs) are known to be dominated by a small subset of dimensions with exceedingly high variance. Previous works have argued that although ablating these outlier dimensions in LLM representations hurts downstream performance, outlier dime…
        ▽ More
      
      
        Representations from large language models (LLMs) are known to be dominated by a small subset of dimensions with exceedingly high variance. Previous works have argued that although ablating these outlier dimensions in LLM representations hurts downstream performance, outlier dimensions are detrimental to the representational quality of embeddings. In this study, we investigate how fine-tuning impacts outlier dimensions and show that 1) outlier dimensions that occur in pre-training persist in fine-tuned models and 2) a single outlier dimension can complete downstream tasks with a minimal error rate. Our results suggest that outlier dimensions can encode crucial task-specific knowledge and that the value of a representation in a single outlier dimension drives downstream model decisions.
        △ Less
      
    
    

    Submitted 26 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      Camera-ready version for EMNLP 2023
    
    

    

    
  

  
    
      arXiv:2310.17714
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.CE
          
        
      
    
    
    
      
        Nearest Neighbor Search over Vectorized Lexico-Syntactic Patterns for Relation Extraction from Financial Documents
      
    
    
      Authors:
      
      Pawan Kumar Rajpoot, 
      
      Ankur Parikh
      
    
    
    
      Abstract:
      
        …relation classes, caused by language complexity and data sparsity. Further, these approaches and models are largely inaccessible to users who don't have direct access to large language models (LLMs) and/or infrastructure for supervised training or fine-tuning. Rule-based syst…
        ▽ More
      
      
        Relation extraction (RE) has achieved remarkable progress with the help of pre-trained language models. However, existing RE models are usually incapable of handling two situations: implicit expressions and long-tail relation classes, caused by language complexity and data sparsity. Further, these approaches and models are largely inaccessible to users who don't have direct access to large language models (LLMs) and/or infrastructure for supervised training or fine-tuning. Rule-based systems also struggle with implicit expressions. Apart from this, Real world financial documents such as various 10-X reports (including 10-K, 10-Q, etc.) of publicly traded companies pose another challenge to rule-based systems in terms of longer and complex sentences. In this paper, we introduce a simple approach that consults training relations at test time through a nearest-neighbor search over dense vectors of lexico-syntactic patterns and provides a simple yet effective means to tackle the above issues. We evaluate our approach on REFinD and show that our method achieves state-of-the-art performance. We further show that it can provide a good start for human in the loop setup when a small number of annotations are available and it is also beneficial when domain experts can provide high quality patterns.
        △ Less
      
    
    

    Submitted 26 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.17703
         [pdf] 
      
      
        cs.CL
        
      
    
    
    
      
        The impact of using an AI chatbot to respond to patient messages
      
    
    
      Authors:
      
      Shan Chen, 
      
      Marco Guevara, 
      
      Shalini Moningi, 
      
      Frank Hoebers, 
      
      Hesham Elhalawani, 
      
      Benjamin H. Kann, 
      
      Fallon E. Chipidza, 
      
      Jonathan Leeman, 
      
      Hugo J. W. L. Aerts, 
      
      Timothy Miller, 
      
      Guergana K. Savova, 
      
      Raymond H. Mak, 
      
      Maryam Lustberg, 
      
      Majid Afshar, 
      
      Danielle S. Bitterman
      
    
    
    
      Abstract:
      
        …medical record systems, AI chatbots utility and impact on clinical decision-making have not been studied for this intended use. We are the first to examine the utility of large language models in assisting clinicians draft responses to patient questions. In our two-stage cross-se…
        ▽ More
      
      
        Documentation burden is a major contributor to clinician burnout, which is rising nationally and is an urgent threat to our ability to care for patients. Artificial intelligence (AI) chatbots, such as ChatGPT, could reduce clinician burden by assisting with documentation. Although many hospitals are actively integrating such systems into electronic medical record systems, AI chatbots utility and impact on clinical decision-making have not been studied for this intended use. We are the first to examine the utility of large language models in assisting clinicians draft responses to patient questions. In our two-stage cross-sectional study, 6 oncologists responded to 100 realistic synthetic cancer patient scenarios and portal messages developed to reflect common medical situations, first manually, then with AI assistance.
  We find AI-assisted responses were longer, less readable, but provided acceptable drafts without edits 58% of time. AI assistance improved efficiency 77% of time, with low harm risk (82% safe). However, 7.7% unedited AI responses could severely harm. In 31% cases, physicians thought AI drafts were human-written. AI assistance led to more patient education recommendations, fewer clinical actions than manual responses. Results show promise for AI to improve clinician efficiency and patient care through assisting documentation, if used judiciously. Monitoring model outputs and human-AI interaction remains crucial for safe implementation.
        △ Less
      
    
    

    Submitted 26 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      4 figures and tables in main, submitted for review
    
    

    

    
  

  
    
      arXiv:2310.17639
         [pdf, other] 
      
      
        cs.AI
        
          
            cs.CL
          
            cs.LG
          
        
      
    
    
    
      
        In-Context Learning Dynamics with Random Binary Sequences
      
    
    
      Authors:
      
      Eric J. Bigelow, 
      
      Ekdeep Singh Lubana, 
      
      Robert P. Dick, 
      
      Hidenori Tanaka, 
      
      Tomer D. Ullman
      
    
    
    
      Abstract:
      
        Large language models (LLMs) trained on huge corpora of text datasets demonstrate complex, emergent capabilities, achieving state-of-the-art performance on tasks they were not explicitly trained for. The precise nature of LLM capabilities is often mysterious, and different prompt…
        ▽ More
      
      
        Large language models (LLMs) trained on huge corpora of text datasets demonstrate complex, emergent capabilities, achieving state-of-the-art performance on tasks they were not explicitly trained for. The precise nature of LLM capabilities is often mysterious, and different prompts can elicit different capabilities through in-context learning. We propose a Cognitive Interpretability framework that enables us to analyze in-context learning dynamics to understand latent concepts in LLMs underlying behavioral patterns. This provides a more nuanced understanding than success-or-failure evaluation benchmarks, but does not require observing internal activations as a mechanistic interpretation of circuits would. Inspired by the cognitive science of human randomness perception, we use random binary sequences as context and study dynamics of in-context learning by manipulating properties of context data, such as sequence length. In the latest GPT-3.5+ models, we find emergent abilities to generate pseudo-random numbers and learn basic formal languages, with striking in-context learning dynamics where model outputs transition sharply from pseudo-random behaviors to deterministic repetition.
        △ Less
      
    
    

    Submitted 26 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.17631
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.AI
          
        
      
    
    
    
      
        JudgeLM: Fine-tuned Large Language Models are Scalable Judges
      
    
    
      Authors:
      
      Lianghui Zhu, 
      
      Xinggang Wang, 
      
      Xinlong Wang
      
    
    
    
      Abstract:
      
        Evaluating Large Language Models (LLMs) in open-ended scenarios is challenging because existing benchmarks and metrics can not measure them comprehensively. To address this problem, we propose to fine-tune LLMs as scalable judges (JudgeLM) to evaluate LLMs efficiently and effecti…
        ▽ More
      
      
        Evaluating Large Language Models (LLMs) in open-ended scenarios is challenging because existing benchmarks and metrics can not measure them comprehensively. To address this problem, we propose to fine-tune LLMs as scalable judges (JudgeLM) to evaluate LLMs efficiently and effectively in open-ended benchmarks. We first propose a comprehensive, large-scale, high-quality dataset containing task seeds, LLMs-generated answers, and GPT-4-generated judgments for fine-tuning high-performance judges, as well as a new benchmark for evaluating the judges. We train JudgeLM at different scales from 7B, 13B, to 33B parameters, and conduct a systematic analysis of its capabilities and behaviors. We then analyze the key biases in fine-tuning LLM as a judge and consider them as position bias, knowledge bias, and format bias. To address these issues, JudgeLM introduces a bag of techniques including swap augmentation, reference support, and reference drop, which clearly enhance the judge's performance. JudgeLM obtains the state-of-the-art judge performance on both the existing PandaLM benchmark and our proposed new benchmark. Our JudgeLM is efficient and the JudgeLM-7B only needs 3 minutes to judge 5K samples with 8 A100 GPUs. JudgeLM obtains high agreement with the teacher judge, achieving an agreement exceeding 90% that even surpasses human-to-human agreement. JudgeLM also demonstrates extended capabilities in being judges of the single answer, multimodal models, multiple answers, and multi-turn chat.
        △ Less
      
    
    

    Submitted 26 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      30 pages, 23 figures
    
    

    

    
  

  
    
      arXiv:2310.17630
         [pdf, other] 
      
      
        cs.CL
        
      
    
    
    
      
        InstOptima: Evolutionary Multi-objective Instruction Optimization via Large Language Model-based Instruction Operators
      
    
    
      Authors:
      
      Heng Yang, 
      
      Ke Li
      
    
    
    
      Abstract:
      
        …(i.e., InstOptima) that treats instruction generation as an evolutionary multi-objective optimization problem. In contrast to text edition-based methods, our approach utilizes a large language model (LLM) to simulate instruction operators, including mutation and crossover. Furthe…
        ▽ More
      
      
        Instruction-based language modeling has received significant attention in pretrained language models. However, the efficiency of instruction engineering remains low and hinders the development of instruction studies. Recent studies have focused on automating instruction generation, but they primarily aim to improve performance without considering other crucial objectives that impact instruction quality, such as instruction length and perplexity. Therefore, we propose a novel approach (i.e., InstOptima) that treats instruction generation as an evolutionary multi-objective optimization problem. In contrast to text edition-based methods, our approach utilizes a large language model (LLM) to simulate instruction operators, including mutation and crossover. Furthermore, we introduce an objective-guided mechanism for these operators, allowing the LLM to comprehend the objectives and enhance the quality of the generated instructions. Experimental results demonstrate improved fine-tuning performance and the generation of a diverse set of high-quality instructions.
        △ Less
      
    
    

    Submitted 26 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      Accepted by EMNLP Findings
    
    

    

    
  

  
    
      arXiv:2310.17623
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.LG
          
        
      
    
    
    
      
        Proving Test Set Contamination in Black Box Language Models
      
    
    
      Authors:
      
      Yonatan Oren, 
      
      Nicole Meister, 
      
      Niladri Chatterji, 
      
      Faisal Ladhak, 
      
      Tatsunori B. Hashimoto
      
    
    
    
      Abstract:
      
        Large language models are trained on vast amounts of internet data, prompting concerns and speculation that they have memorized public benchmarks. Going from speculation to proof of contamination is challenging, as the pretraining data used by proprietary models are often not pub…
        ▽ More
      
      
        Large language models are trained on vast amounts of internet data, prompting concerns and speculation that they have memorized public benchmarks. Going from speculation to proof of contamination is challenging, as the pretraining data used by proprietary models are often not publicly accessible. We show that it is possible to provide provable guarantees of test set contamination in language models without access to pretraining data or model weights. Our approach leverages the fact that when there is no data contamination, all orderings of an exchangeable benchmark should be equally likely. In contrast, the tendency for language models to memorize example order means that a contaminated language model will find certain canonical orderings to be much more likely than others. Our test flags potential contamination whenever the likelihood of a canonically ordered benchmark dataset is significantly higher than the likelihood after shuffling the examples. We demonstrate that our procedure is sensitive enough to reliably prove test set contamination in challenging situations, including models as small as 1.4 billion parameters, on small test sets of only 1000 examples, and datasets that appear only a few times in the pretraining corpus. Using our test, we audit five popular publicly accessible language models for test set contamination and find little evidence for pervasive contamination.
        △ Less
      
    
    

    Submitted 26 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.17589
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.AI
          
        
      
    
    
    
      
        An Open Source Data Contamination Report for Llama Series Models
      
    
    
      Authors:
      
      Yucheng Li
      
    
    
    
      Abstract:
      
        Data contamination in language model evaluation is increasingly prevalent as the popularity of large language models. It allows models to ""cheat"" via memorisation instead of displaying true capabilities. Therefore, contamination analysis has became an crucial part of reli…
        ▽ More
      
      
        Data contamination in language model evaluation is increasingly prevalent as the popularity of large language models. It allows models to ""cheat"" via memorisation instead of displaying true capabilities. Therefore, contamination analysis has became an crucial part of reliable model evaluation to validate results. However, existing contamination analysis is usually conducted internally by LLM developers and often lacks transparency and completeness. This paper present an open source data contamination reports for the Llama series models. We analyse six popular multi-choice QA benchmarks and quantify their overlapping with the training set of Llama. Various levels of contamination ranging from 1\% to 8.7\% are found across benchmarks. Our comparison also reveals that Llama models can gain over 5\% higher accuracy on contaminated subsets versus clean subsets. Data and code are available at: https://github.com/liyucheng09/Contamination_Detector.
        △ Less
      
    
    

    Submitted 26 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.17555
         [pdf, other] 
      
      
        cs.RO
        
          
            cs.AI
          
            cs.LG
          
        
      
    
    
    
      
        Interactive Robot Learning from Verbal Correction
      
    
    
      Authors:
      
      Huihan Liu, 
      
      Alice Chen, 
      
      Yuke Zhu, 
      
      Adith Swaminathan, 
      
      Andrey Kolobov, 
      
      Ching-An Cheng
      
    
    
    
      Abstract:
      
        …has become ever more important for robots as we design them to operate in unstructured environments like households. In this work, we design a new learning system based on large language model (LLM), OLAF, that allows everyday users to teach a robot using verbal corrections when…
        ▽ More
      
      
        The ability to learn and refine behavior after deployment has become ever more important for robots as we design them to operate in unstructured environments like households. In this work, we design a new learning system based on large language model (LLM), OLAF, that allows everyday users to teach a robot using verbal corrections when the robot makes mistakes, e.g., by saying ""Stop what you're doing. You should move closer to the cup."" A key feature of OLAF is its ability to update the robot's visuomotor neural policy based on the verbal feedback to avoid repeating mistakes in the future. This is in contrast to existing LLM-based robotic systems, which only follow verbal commands or corrections but not learn from them. We demonstrate the efficacy of our design in experiments where a user teaches a robot to perform long-horizon manipulation tasks both in simulation and on physical hardware, achieving on average 20.0% improvement in policy success rate. Videos and more results are at https://ut-austin-rpl.github.io/olaf/
        △ Less
      
    
    

    Submitted 26 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.17526
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.AI
          
            cs.LG
          
        
      
    
    
    
      
        Can large language models replace humans in the systematic review process? Evaluating GPT-4's efficacy in screening and extracting data from peer-reviewed and grey literature in multiple languages
      
    
    
      Authors:
      
      Qusai Khraisha, 
      
      Sophie Put, 
      
      Johanna Kappenberg, 
      
      Azza Warraitch, 
      
      Kristin Hadfield
      
    
    
    
      Abstract:
      
        Systematic reviews are vital for guiding practice, research, and policy, yet they are often slow and labour-intensive. Large language models (LLMs) could offer a way to speed up and automate systematic reviews, but their performance in such tasks has not been comprehensively eval…
        ▽ More
      
      
        Systematic reviews are vital for guiding practice, research, and policy, yet they are often slow and labour-intensive. Large language models (LLMs) could offer a way to speed up and automate systematic reviews, but their performance in such tasks has not been comprehensively evaluated against humans, and no study has tested GPT-4, the biggest LLM so far. This pre-registered study evaluates GPT-4's capability in title/abstract screening, full-text review, and data extraction across various literature types and languages using a 'human-out-of-the-loop' approach. Although GPT-4 had accuracy on par with human performance in most tasks, results were skewed by chance agreement and dataset imbalance. After adjusting for these, there was a moderate level of performance for data extraction, and - barring studies that used highly reliable prompts - screening performance levelled at none to moderate for different stages and languages. When screening full-text literature using highly reliable prompts, GPT-4's performance was 'almost perfect.' Penalising GPT-4 for missing key studies using highly reliable prompts improved its performance even more. Our findings indicate that, currently, substantial caution should be used if LLMs are being used to conduct systematic reviews, but suggest that, for certain systematic review tasks delivered under reliable prompts, LLMs can rival human performance.
        △ Less
      
    
    

    Submitted 27 October, 2023; v1 submitted 26 October, 2023;
      originally announced October 2023.
      
    
    
    
      Comments:
      9 pages, 2 figures, 1 table
    
    

    

    
  

  
    
      arXiv:2310.17513
         [pdf, other] 
      
      
        cs.LG
        
          
            cs.AI
          
            cs.CL
          
            stat.ML
          
        
      
    
    
    
      
        The Expressive Power of Low-Rank Adaptation
      
    
    
      Authors:
      
      Yuchen Zeng, 
      
      Kangwook Lee
      
    
    
    
      Abstract:
      
        …a parameter-efficient fine-tuning method that leverages low-rank adaptation of weight matrices, has emerged as a prevalent technique for fine-tuning pre-trained models such as large language models and diffusion models. Despite its huge success in practice, the theoretical under…
        ▽ More
      
      
        Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning method that leverages low-rank adaptation of weight matrices, has emerged as a prevalent technique for fine-tuning pre-trained models such as large language models and diffusion models. Despite its huge success in practice, the theoretical underpinnings of LoRA have largely remained unexplored. This paper takes the first step to bridge this gap by theoretically analyzing the expressive power of LoRA. We prove that, for fully connected neural networks, LoRA can adapt any model f to accurately represent any smaller target model f¯¯¯ if LoRA-rank ≥(width of f)×depth of f¯¯¯depth of f. We also quantify the approximation error when LoRA-rank is lower than the threshold. For Transformer networks, we show any model can be adapted to a target model of the same size with rank-(embedding size2) LoRA adapters.
        △ Less
      
    
    

    Submitted 26 October, 2023; v1 submitted 26 October, 2023;
      originally announced October 2023.
      
    
    
    
      Comments:
      40 pages, 5 figures
    
    

    

    
  

  
    
      arXiv:2310.17512
         [pdf, other] 
      
      
        cs.AI
        
          
            cs.CL
          
            cs.HC
          
            cs.MA
          
        
      
    
    
    
      
        CompeteAI: Understanding the Competition Behaviors in Large Language Model-based Agents
      
    
    
      Authors:
      
      Qinlin Zhao, 
      
      Jindong Wang, 
      
      Yixuan Zhang, 
      
      Yiqiao Jin, 
      
      Kaijie Zhu, 
      
      Hao Chen, 
      
      Xing Xie
      
    
    
    
      Abstract:
      
        Large language models (LLMs) have been widely used as agents to complete different tasks, such as personal assistance or event planning. While most work has focused on cooperation and collaboration between agents, little work explores competition, another important mechanism that…
        ▽ More
      
      
        Large language models (LLMs) have been widely used as agents to complete different tasks, such as personal assistance or event planning. While most work has focused on cooperation and collaboration between agents, little work explores competition, another important mechanism that fosters the development of society and economy. In this paper, we seek to examine the competition behaviors in LLM-based agents. We first propose a general framework to study the competition between agents. Then, we implement a practical competitive environment using GPT-4 to simulate a virtual town with two types of agents, including restaurant agents and customer agents. Specifically, restaurant agents compete with each other to attract more customers, where the competition fosters them to transform, such as cultivating new operating strategies. The results of our experiments reveal several interesting findings ranging from social learning to Matthew Effect, which aligns well with existing sociological and economic theories. We believe that competition between agents deserves further investigation to help us understand society better. The code will be released soon.
        △ Less
      
    
    

    Submitted 26 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      Technical report; 21 pages
    
    

    

    
  

  
    
      arXiv:2310.17490
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.AI
          
        
      
    
    
    
      
        Improving Zero-shot Reader by Reducing Distractions from Irrelevant Documents in Open-Domain Question Answering
      
    
    
      Authors:
      
      Sukmin Cho, 
      
      Jeong yeon Seo, 
      
      Soyeong Jeong, 
      
      Jong C. Park
      
    
    
    
      Abstract:
      
        Large language models (LLMs) enable zero-shot approaches in open-domain question answering (ODQA), yet with limited advancements as the reader is compared to the retriever. This study aims at the feasibility of a zero-shot reader that addresses the challenges of computational cos…
        ▽ More
      
      
        Large language models (LLMs) enable zero-shot approaches in open-domain question answering (ODQA), yet with limited advancements as the reader is compared to the retriever. This study aims at the feasibility of a zero-shot reader that addresses the challenges of computational cost and the need for labeled data. We find that LLMs are distracted due to irrelevant documents in the retrieved set and the overconfidence of the generated answers when they are exploited as zero-shot readers. To tackle these problems, we mitigate the impact of such documents via Distraction-aware Answer Selection (DAS) with a negation-based instruction and score adjustment for proper answer selection. Experimental results show that our approach successfully handles distraction across diverse scenarios, enhancing the performance of zero-shot readers. Furthermore, unlike supervised readers struggling with unseen data, zero-shot readers demonstrate outstanding transferability without any training.
        △ Less
      
    
    

    Submitted 26 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      Findings of EMNLP 2023 Camera Ready
    
    

    

    
  

  
    
      arXiv:2310.17407
         [pdf] 
      
      
        cs.CL
        
      
    
    
    
      
        Meaning and understanding in large language models
      
    
    
      Authors:
      
      Vladimír Havlík
      
    
    
    
      Abstract:
      
        Can a machine understand the meanings of natural language? Recent developments in the generative large language models (LLMs) of artificial intelligence have led to the belief that traditional philosophical assumptions about machine understanding of language need to be revised. T…
        ▽ More
      
      
        Can a machine understand the meanings of natural language? Recent developments in the generative large language models (LLMs) of artificial intelligence have led to the belief that traditional philosophical assumptions about machine understanding of language need to be revised. This article critically evaluates the prevailing tendency to regard machine language performance as mere syntactic manipulation and the simulation of understanding, which is only partial and very shallow, without sufficient referential grounding in the world. The aim is to highlight the conditions crucial to attributing natural language understanding to state-of-the-art LLMs, where it can be legitimately argued that LLMs not only use syntax but also semantics, their understanding not being simulated but duplicated; and determine how they ground the meanings of linguistic expressions.
        △ Less
      
    
    

    Submitted 26 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      20 pages
    
    

    

    
  

  
    
      arXiv:2310.17389
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.AI
          
        
      
    
    
    
      
        ToxicChat: Unveiling Hidden Challenges of Toxicity Detection in Real-World User-AI Conversation
      
    
    
      Authors:
      
      Zi Lin, 
      
      Zihan Wang, 
      
      Yongqi Tong, 
      
      Yangkun Wang, 
      
      Yuxin Guo, 
      
      Yujia Wang, 
      
      Jingbo Shang
      
    
    
    
      Abstract:
      
        Despite remarkable advances that large language models have achieved in chatbots, maintaining a non-toxic user-AI interactive environment has become increasingly critical nowadays. However, previous efforts in toxicity detection have been mostly based on benchmarks derived from s…
        ▽ More
      
      
        Despite remarkable advances that large language models have achieved in chatbots, maintaining a non-toxic user-AI interactive environment has become increasingly critical nowadays. However, previous efforts in toxicity detection have been mostly based on benchmarks derived from social media content, leaving the unique challenges inherent to real-world user-AI interactions insufficiently explored. In this work, we introduce ToxicChat, a novel benchmark based on real user queries from an open-source chatbot. This benchmark contains the rich, nuanced phenomena that can be tricky for current toxicity detection models to identify, revealing a significant domain difference compared to social media content. Our systematic evaluation of models trained on existing toxicity datasets has shown their shortcomings when applied to this unique domain of ToxicChat. Our work illuminates the potentially overlooked challenges of toxicity detection in real-world user-AI conversations. In the future, ToxicChat can be a valuable resource to drive further advancements toward building a safe and healthy environment for user-AI interactions.
        △ Less
      
    
    

    Submitted 26 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
      
        Journal ref:
        EMNLP findings 2023
      
    
  

  
    
      arXiv:2310.17372
         [pdf, other] 
      
      
        cs.AI
        
          
            cs.CL
          
            cs.RO
          
        
      
    
    
    
      
        Dialogue-based generation of self-driving simulation scenarios using Large Language Models
      
    
    
      Authors:
      
      Antonio Valerio Miceli-Barone, 
      
      Alex Lascarides, 
      
      Craig Innes
      
    
    
    
      Abstract:
      
        …the user can follow up prior instructions with refinements or revisions, in reaction to the simulations that have been generated from their utterances so far. We use Large Language Models (LLMs) to map the user's English utterances in this interaction into domain-specific cod…
        ▽ More
      
      
        Simulation is an invaluable tool for developing and evaluating controllers for self-driving cars. Current simulation frameworks are driven by highly-specialist domain specific languages, and so a natural language interface would greatly enhance usability. But there is often a gap, consisting of tacit assumptions the user is making, between a concise English utterance and the executable code that captures the user's intent. In this paper we describe a system that addresses this issue by supporting an extended multimodal interaction: the user can follow up prior instructions with refinements or revisions, in reaction to the simulations that have been generated from their utterances so far. We use Large Language Models (LLMs) to map the user's English utterances in this interaction into domain-specific code, and so we explore the extent to which LLMs capture the context sensitivity that's necessary for computing the speaker's intended message in discourse.
        △ Less
      
    
    

    Submitted 26 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      12 pages, 6 figures, SpLU-RoboNLP 2023
    
    

    

    
  

  
    
      arXiv:2310.17353
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.AI
          
        
      
    
    
    
      
        Cultural Adaptation of Recipes
      
    
    
      Authors:
      
      Yong Cao, 
      
      Yova Kementchedjhieva, 
      
      Ruixiang Cui, 
      
      Antonia Karamolegkou, 
      
      Li Zhou, 
      
      Megan Dare, 
      
      Lucia Donatelli, 
      
      Daniel Hershcovich
      
    
    
    
      Abstract:
      
        Building upon the considerable advances in Large Language Models (LLMs), we are now equipped to address more sophisticated tasks demanding a nuanced understanding of cross-cultural contexts. A key example is recipe adaptation, which goes beyond simple translation to include a gra…
        ▽ More
      
      
        Building upon the considerable advances in Large Language Models (LLMs), we are now equipped to address more sophisticated tasks demanding a nuanced understanding of cross-cultural contexts. A key example is recipe adaptation, which goes beyond simple translation to include a grasp of ingredients, culinary techniques, and dietary preferences specific to a given culture. We introduce a new task involving the translation and cultural adaptation of recipes between Chinese and English-speaking cuisines. To support this investigation, we present CulturalRecipes, a unique dataset comprised of automatically paired recipes written in Mandarin Chinese and English. This dataset is further enriched with a human-written and curated test set. In this intricate task of cross-cultural recipe adaptation, we evaluate the performance of various methods, including GPT-4 and other LLMs, traditional machine translation, and information retrieval techniques. Our comprehensive analysis includes both automatic and human evaluation metrics. While GPT-4 exhibits impressive abilities in adapting Chinese recipes into English, it still lags behind human expertise when translating English recipes into Chinese. This underscores the multifaceted nature of cultural adaptations. We anticipate that these insights will significantly contribute to future research on culturally-aware language models and their practical application in culturally diverse contexts.
        △ Less
      
    
    

    Submitted 26 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      Accepted to TACL
    
    

    

    
  

  
    
      arXiv:2310.17342
         [pdf, other] 
      
      
        cs.CL
        
      
    
    
    
      
        ACT-SQL: In-Context Learning for Text-to-SQL with Automatically-Generated Chain-of-Thought
      
    
    
      Authors:
      
      Hanchong Zhang, 
      
      Ruisheng Cao, 
      
      Lu Chen, 
      
      Hongshen Xu, 
      
      Kai Yu
      
    
    
    
      Abstract:
      
        Recently Large Language Models (LLMs) have been proven to have strong abilities in various domains and tasks. We study the problem of prompt designing in the text-to-SQL task and attempt to improve the LLMs' reasoning ability when generating SQL queries. Besides the trivial f…
        ▽ More
      
      
        Recently Large Language Models (LLMs) have been proven to have strong abilities in various domains and tasks. We study the problem of prompt designing in the text-to-SQL task and attempt to improve the LLMs' reasoning ability when generating SQL queries. Besides the trivial few-shot in-context learning setting, we design our chain-of-thought (CoT) prompt with a similar method to schema linking. We provide a method named ACT-SQL to automatically generate auto-CoT exemplars and thus the whole process doesn't need manual labeling. Our approach is cost-saving since we only use the LLMs' API call once when generating one SQL query. Furthermore, we extend our in-context learning method to the multi-turn text-to-SQL task. The experiment results show that the LLMs' performance can benefit from our ACT-SQL approach. Our approach achieves SOTA performance on the Spider dev set among existing in-context learning approaches.
        △ Less
      
    
    

    Submitted 26 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.17312
         [pdf, other] 
      
      
        cs.CL
        
      
    
    
    
      
        An Ensemble Method Based on the Combination of Transformers with Convolutional Neural Networks to Detect Artificially Generated Text
      
    
    
      Authors:
      
      Vijini Liyanage, 
      
      Davide Buscaldi
      
    
    
    
      Abstract:
      
        Thanks to the state-of-the-art Large Language Models (LLMs), language generation has reached outstanding levels. These models are capable of generating high quality content, thus making it a challenging task to detect generated text from human-written content. Despite the advanta…
        ▽ More
      
      
        Thanks to the state-of-the-art Large Language Models (LLMs), language generation has reached outstanding levels. These models are capable of generating high quality content, thus making it a challenging task to detect generated text from human-written content. Despite the advantages provided by Natural Language Generation, the inability to distinguish automatically generated text can raise ethical concerns in terms of authenticity. Consequently, it is important to design and develop methodologies to detect artificial content. In our work, we present some classification models constructed by ensembling transformer models such as Sci-BERT, DeBERTa and XLNet, with Convolutional Neural Networks (CNNs). Our experiments demonstrate that the considered ensemble architectures surpass the performance of the individual transformer models for classification. Furthermore, the proposed SciBERT-CNN ensemble model produced an F1-score of 98.36% on the ALTA shared task 2023 data.
        △ Less
      
    
    

    Submitted 26 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      In Proceedings of the 21st Annual Workshop of the Australasian Language Technology Association (ALTA 2023)
    
    

    

    
  

  
    
      arXiv:2310.17228
         [pdf, other] 
      
      
        cs.AI
        
          
            cs.CL
          
            cs.SE
          
        
      
    
    
    
      
        TSTR: Target Similarity Tuning Meets the Real World
      
    
    
      Authors:
      
      Anirudh Khatry, 
      
      Sumit Gulwani, 
      
      Priyanshu Gupta, 
      
      Vu Le, 
      
      Ananya Singha, 
      
      Mukul Singh, 
      
      Gust Verbruggen
      
    
    
    
      Abstract:
      
        Target similarity tuning (TST) is a method of selecting relevant examples in natural language (NL) to code generation through large language models (LLMs) to improve performance. Its goal is to adapt a sentence embedding model to have the similarity between two NL inputs match th…
        ▽ More
      
      
        Target similarity tuning (TST) is a method of selecting relevant examples in natural language (NL) to code generation through large language models (LLMs) to improve performance. Its goal is to adapt a sentence embedding model to have the similarity between two NL inputs match the similarity between their associated code outputs. In this paper, we propose different methods to apply and improve TST in the real world. First, we replace the sentence transformer with embeddings from a larger model, which reduces sensitivity to the language distribution and thus provides more flexibility in synthetic generation of examples, and we train a tiny model that transforms these embeddings to a space where embedding similarity matches code similarity, which allows the model to remain a black box and only requires a few matrix multiplications at inference time. Second, we show how to efficiently select a smaller number of training examples to train the TST model. Third, we introduce a ranking-based evaluation for TST that does not require end-to-end code generation experiments, which can be expensive to perform.
        △ Less
      
    
    

    Submitted 28 October, 2023; v1 submitted 26 October, 2023;
      originally announced October 2023.
      
    
    
    
      Comments:
      Accepted for EMNLP-Findings, 2023
    
    

    

    
  

  
    
      arXiv:2310.17217
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.AI
          
            cs.LG
          
        
      
    
    
    
      
        Beyond MLE: Convex Learning for Text Generation
      
    
    
      Authors:
      
      Chenze Shao, 
      
      Zhengrui Ma, 
      
      Min Zhang, 
      
      Yang Feng
      
    
    
    
      Abstract:
      
        …beam search, and facilitates the learning of non-autoregressive models with a maximum improvement of 9+ BLEU points. Moreover, our approach also exhibits significant impact on large language models (LLMs), substantially enhancing their generative capability on various tasks. Sour…
        ▽ More
      
      
        Maximum likelihood estimation (MLE) is a statistical method used to estimate the parameters of a probability distribution that best explain the observed data. In the context of text generation, MLE is often used to train generative language models, which can then be used to generate new text. However, we argue that MLE is not always necessary and optimal, especially for closed-ended text generation tasks like machine translation. In these tasks, the goal of model is to generate the most appropriate response, which does not necessarily require it to estimate the entire data distribution with MLE. To this end, we propose a novel class of training objectives based on convex functions, which enables text generation models to focus on highly probable outputs without having to estimate the entire data distribution. We investigate the theoretical properties of the optimal predicted distribution when applying convex functions to the loss, demonstrating that convex functions can sharpen the optimal distribution, thereby enabling the model to better capture outputs with high probabilities. Experiments on various text generation tasks and models show the effectiveness of our approach. It enables autoregressive models to bridge the gap between greedy and beam search, and facilitates the learning of non-autoregressive models with a maximum improvement of 9+ BLEU points. Moreover, our approach also exhibits significant impact on large language models (LLMs), substantially enhancing their generative capability on various tasks. Source code is available at \url{https://github.com/ictnlp/Convex-Learning}.
        △ Less
      
    
    

    Submitted 26 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      NeurIPS 2023
    
    

    

    
  

  
    
      arXiv:2310.17162
         [pdf, other] 
      
      
        cs.AI
        
          
            cs.SD
          
            eess.AS
          
        
      
    
    
    
      
        Content-based Controls For Music Large Language Modeling
      
    
    
      Authors:
      
      Liwei Lin, 
      
      Gus Xia, 
      
      Junyan Jiang, 
      
      Yixiao Zhang
      
    
    
    
      Abstract:
      
        …direct and content-based controls on innate music languages such as pitch, chords and drum track. To this end, we contribute Coco-Mulla, a content-based control method for music large language modeling. It uses a parameter-efficient fine-tuning (PEFT) method tailored for Transfor…
        ▽ More
      
      
        Recent years have witnessed a rapid growth of large-scale language models in the domain of music audio. Such models enable end-to-end generation of higher-quality music, and some allow conditioned generation using text descriptions. However, the control power of text controls on music is intrinsically limited, as they can only describe music indirectly through meta-data (such as singers and instruments) or high-level representations (such as genre and emotion). We aim to further equip the models with direct and content-based controls on innate music languages such as pitch, chords and drum track. To this end, we contribute Coco-Mulla, a content-based control method for music large language modeling. It uses a parameter-efficient fine-tuning (PEFT) method tailored for Transformer-based audio models. Experiments show that our approach achieved high-quality music generation with low-resource semi-supervised learning, tuning with less than 4% parameters compared to the original model and training on a small dataset with fewer than 300 songs. Moreover, our approach enables effective content-based controls, and we illustrate the control power via chords and rhythms, two of the most salient features of music audio. Furthermore, we show that by combining content-based controls and text descriptions, our system achieves flexible music variation generation and style transfer. Our source codes and demos are available online.
        △ Less
      
    
    

    Submitted 26 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.17157
         [pdf, other] 
      
      
        cs.LG
        
      
    
    
    
      
        Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time
      
    
    
      Authors:
      
      Zichang Liu, 
      
      Jue Wang, 
      
      Tri Dao, 
      
      Tianyi Zhou, 
      
      Binhang Yuan, 
      
      Zhao Song, 
      
      Anshumali Shrivastava, 
      
      Ce Zhang, 
      
      Yuandong Tian, 
      
      Christopher Re, 
      
      Beidi Chen
      
    
    
    
      Abstract:
      
        Large language models (LLMs) with hundreds of billions of parameters have sparked a new wave of exciting AI applications. However, they are computationally expensive at inference time. Sparsity is a natural approach to reduce this cost, but existing methods either require costly…
        ▽ More
      
      
        Large language models (LLMs) with hundreds of billions of parameters have sparked a new wave of exciting AI applications. However, they are computationally expensive at inference time. Sparsity is a natural approach to reduce this cost, but existing methods either require costly retraining, have to forgo LLM's in-context learning ability, or do not yield wall-clock time speedup on modern hardware. We hypothesize that contextual sparsity, which are small, input-dependent sets of attention heads and MLP parameters that yield approximately the same output as the dense model for a given input, can address these issues. We show that contextual sparsity exists, that it can be accurately predicted, and that we can exploit it to speed up LLM inference in wall-clock time without compromising LLM's quality or in-context learning ability. Based on these insights, we propose DejaVu, a system that uses a low-cost algorithm to predict contextual sparsity on the fly given inputs to each layer, along with an asynchronous and hardware-aware implementation that speeds up LLM inference. We validate that DejaVu can reduce the inference latency of OPT-175B by over 2X compared to the state-of-the-art FasterTransformer, and over 6X compared to the widely used Hugging Face implementation, without compromising model quality. The code is available at https://github.com/FMInference/DejaVu.
        △ Less
      
    
    

    Submitted 26 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
      
        Journal ref:
        Proceedings of the 40th International Conference on Machine Learning, 2023, 919
      
    
  

  
    
      arXiv:2310.17143
         [pdf] 
      
      
        cs.CY
        
          
            cs.CL
          
        
      
    
    
    
      
        Supercharging academic writing with generative AI: framework, techniques, and caveats
      
    
    
      Authors:
      
      Zhicheng Lin
      
    
    
    
      Abstract:
      
        …an indispensable yet laborious part of the research enterprise. This Perspective maps out principles and methods for using generative artificial intelligence (AI), specifically large language models (LLMs), to elevate the quality and efficiency of academic writing. We introduce a…
        ▽ More
      
      
        Academic writing is an indispensable yet laborious part of the research enterprise. This Perspective maps out principles and methods for using generative artificial intelligence (AI), specifically large language models (LLMs), to elevate the quality and efficiency of academic writing. We introduce a human-AI collaborative framework that delineates the rationale (why), process (how), and nature (what) of AI engagement in writing. The framework pinpoints both short-term and long-term reasons for engagement and their underlying mechanisms (e.g., cognitive offloading and imaginative stimulation). It reveals the role of AI throughout the writing process, conceptualized through a two-stage model for human-AI collaborative writing, and the nature of AI assistance in writing, represented through a model of writing-assistance types and levels. Building on this framework, we describe effective prompting techniques for incorporating AI into the writing routine (outlining, drafting, and editing) as well as strategies for maintaining rigorous scholarship, adhering to varied journal policies, and avoiding overreliance on AI. Ultimately, the prudent integration of AI into academic writing can ease the communication burden, empower authors, accelerate discovery, and promote diversity in science.
        △ Less
      
    
    

    Submitted 26 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      14 pages, 2 figures, 1 table, 1 box
    
    

    

    
  

  
    
      arXiv:2310.17140
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.AI
          
        
      
    
    
    
      
        Symbolic Planning and Code Generation for Grounded Dialogue
      
    
    
      Authors:
      
      Justin T. Chiu, 
      
      Wenting Zhao, 
      
      Derek Chen, 
      
      Saujas Vaduguru, 
      
      Alexander M. Rush, 
      
      Daniel Fried
      
    
    
    
      Abstract:
      
        Large language models (LLMs) excel at processing and generating both text and code. However, LLMs have had limited applicability in grounded task-oriented dialogue as they are difficult to steer toward task objectives and fail to handle novel grounding. We present a modular and i…
        ▽ More
      
      
        Large language models (LLMs) excel at processing and generating both text and code. However, LLMs have had limited applicability in grounded task-oriented dialogue as they are difficult to steer toward task objectives and fail to handle novel grounding. We present a modular and interpretable grounded dialogue system that addresses these shortcomings by composing LLMs with a symbolic planner and grounded code execution. Our system consists of a reader and planner: the reader leverages an LLM to convert partner utterances into executable code, calling functions that perform grounding. The translated code's output is stored to track dialogue state, while a symbolic planner determines the next appropriate response. We evaluate our system's performance on the demanding OneCommon dialogue task, involving collaborative reference resolution on abstract images of scattered dots. Our system substantially outperforms the previous state-of-the-art, including improving task success in human evaluations from 56% to 69% in the most challenging setting.
        △ Less
      
    
    

    Submitted 26 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      Accepted to EMNLP 2023
    
    

    

    
  

  
    
      arXiv:2310.17133
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.AI
          
        
      
    
    
    
      
        Incorporating Probing Signals into Multimodal Machine Translation via Visual Question-Answering Pairs
      
    
    
      Authors:
      
      Yuxin Zuo, 
      
      Bei Li, 
      
      Chuanhao Lv, 
      
      Tong Zheng, 
      
      Tong Xiao, 
      
      Jingbo Zhu
      
    
    
    
      Abstract:
      
        …A novel approach is proposed to generate parallel Visual Question-Answering (VQA) style pairs from the source text, fostering more robust cross-modal interaction. Using Large Language Models (LLMs), we explicitly model the probing signal in MMT to convert it into VQA-style data…
        ▽ More
      
      
        This paper presents an in-depth study of multimodal machine translation (MMT), examining the prevailing understanding that MMT systems exhibit decreased sensitivity to visual information when text inputs are complete. Instead, we attribute this phenomenon to insufficient cross-modal interaction, rather than image information redundancy. A novel approach is proposed to generate parallel Visual Question-Answering (VQA) style pairs from the source text, fostering more robust cross-modal interaction. Using Large Language Models (LLMs), we explicitly model the probing signal in MMT to convert it into VQA-style data to create the Multi30K-VQA dataset. An MMT-VQA multitask learning framework is introduced to incorporate explicit probing signals from the dataset into the MMT training process. Experimental results on two widely-used benchmarks demonstrate the effectiveness of this novel approach. Our code and data would be available at: \url{https://github.com/libeineu/MMT-VQA}.
        △ Less
      
    
    

    Submitted 26 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      Findings of EMNLP2023
    
    

    

    
  

  
    
      arXiv:2310.17130
         [pdf, other] 
      
      
        cs.CL
        
      
    
    
    
      
        M2C: Towards Automatic Multimodal Manga Complement
      
    
    
      Authors:
      
      Hongcheng Guo, 
      
      Boyang Wang, 
      
      Jiaqi Bai, 
      
      Jiaheng Liu, 
      
      Jian Yang, 
      
      Zhoujun Li
      
    
    
    
      Abstract:
      
        …task by establishing a new M2C benchmark dataset covering two languages. First, we design a manga argumentation method called MCoT to mine event knowledge in comics with large language models. Then, an effective baseline FVP-M2 using fine-grained visual prompts is proposed t…
        ▽ More
      
      
        Multimodal manga analysis focuses on enhancing manga understanding with visual and textual features, which has attracted considerable attention from both natural language processing and computer vision communities. Currently, most comics are hand-drawn and prone to problems such as missing pages, text contamination, and aging, resulting in missing comic text content and seriously hindering human comprehension. In other words, the Multimodal Manga Complement (M2C) task has not been investigated, which aims to handle the aforementioned issues by providing a shared semantic space for vision and language understanding. To this end, we first propose the Multimodal Manga Complement task by establishing a new M2C benchmark dataset covering two languages. First, we design a manga argumentation method called MCoT to mine event knowledge in comics with large language models. Then, an effective baseline FVP-M2 using fine-grained visual prompts is proposed to support manga complement. Extensive experimental results show the effectiveness of FVP-M2 method for Multimodal Mange Complement.
        △ Less
      
    
    

    Submitted 26 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      EMNLP2023. arXiv admin note: text overlap with arXiv:2210.15461
    
    

    

    
  

  
    
      arXiv:2310.17119
         [pdf, other] 
      
      
        cs.CL
        
      
    
    
    
      
        FLEEK: Factual Error Detection and Correction with Evidence Retrieved from External Knowledge
      
    
    
      Authors:
      
      Farima Fatahi Bayat, 
      
      Kun Qian, 
      
      Benjamin Han, 
      
      Yisi Sang, 
      
      Anton Belyi, 
      
      Samira Khorshidi, 
      
      Fei Wu, 
      
      Ihab F. Ilyas, 
      
      Yunyao Li
      
    
    
    
      Abstract:
      
        Detecting factual errors in textual information, whether generated by large language models (LLM) or curated by humans, is crucial for making informed decisions. LLMs' inability to attribute their claims to external knowledge and their tendency to hallucinate makes it difficu…
        ▽ More
      
      
        Detecting factual errors in textual information, whether generated by large language models (LLM) or curated by humans, is crucial for making informed decisions. LLMs' inability to attribute their claims to external knowledge and their tendency to hallucinate makes it difficult to rely on their responses. Humans, too, are prone to factual errors in their writing. Since manual detection and correction of factual errors is labor-intensive, developing an automatic approach can greatly reduce human effort. We present FLEEK, a prototype tool that automatically extracts factual claims from text, gathers evidence from external knowledge sources, evaluates the factuality of each claim, and suggests revisions for identified errors using the collected evidence. Initial empirical evaluation on fact error detection (77-85\% F1) shows the potential of FLEEK. A video demo of FLEEK can be found at https://youtu.be/NapJFUlkPdQ.
        △ Less
      
    
    

    Submitted 25 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      EMNLP 2023 (Demonstration Track)
    
    

    

    
  

  
    
      arXiv:2310.17110
         [pdf, other] 
      
      
        cs.LG
        
      
    
    
    
      
        LLM4DyG: Can Large Language Models Solve Problems on Dynamic Graphs?
      
    
    
      Authors:
      
      Zeyang Zhang, 
      
      Xin Wang, 
      
      Ziwei Zhang, 
      
      Haoyang Li, 
      
      Yijian Qin, 
      
      Simin Wu, 
      
      Wenwu Zhu
      
    
    
    
      Abstract:
      
        In an era marked by the increasing adoption of Large Language Models (LLMs) for various tasks, there is a growing focus on exploring LLMs' capabilities in handling web data, particularly graph data. Dynamic graphs, which capture temporal network evolution patterns, are ubiqui…
        ▽ More
      
      
        In an era marked by the increasing adoption of Large Language Models (LLMs) for various tasks, there is a growing focus on exploring LLMs' capabilities in handling web data, particularly graph data. Dynamic graphs, which capture temporal network evolution patterns, are ubiquitous in real-world web data. Evaluating LLMs' competence in understanding spatial-temporal information on dynamic graphs is essential for their adoption in web applications, which remains unexplored in the literature. In this paper, we bridge the gap via proposing to evaluate LLMs' spatial-temporal understanding abilities on dynamic graphs, to the best of our knowledge, for the first time. Specifically, we propose the LLM4DyG benchmark, which includes nine specially designed tasks considering the capability evaluation of LLMs from both temporal and spatial dimensions. Then, we conduct extensive experiments to analyze the impacts of different data generators, data statistics, prompting techniques, and LLMs on the model performance. Finally, we propose Disentangled Spatial-Temporal Thoughts (DST2) for LLMs on dynamic graphs to enhance LLMs' spatial-temporal understanding abilities. Our main observations are: 1) LLMs have preliminary spatial-temporal understanding abilities on dynamic graphs, 2) Dynamic graph tasks show increasing difficulties for LLMs as the graph size and density increase, while not sensitive to the time span and data generation mechanism, 3) the proposed DST2 prompting method can help to improve LLMs' spatial-temporal understanding abilities on dynamic graphs for most tasks. The data and codes will be open-sourced at publication time.
        △ Less
      
    
    

    Submitted 25 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.17064
         [pdf, other] 
      
      
        cs.AI
        
          
            cs.CL
          
            cs.LG
          
            cs.LO
          
        
      
    
    
    
      
        math-PVS: A Large Language Model Framework to Map Scientific Publications to PVS Theories
      
    
    
      Authors:
      
      Hassen Saidi, 
      
      Susmit Jha, 
      
      Tuhin Sahai
      
    
    
    
      Abstract:
      
        …connections between different mathematical areas, to name a few.
  While prior work has leveraged computers for exhaustive mathematical proof search, recent efforts based on large language models (LLMs) aspire to position computing platforms as co-contributors in the mathematical…
        ▽ More
      
      
        As artificial intelligence (AI) gains greater adoption in a wide variety of applications, it has immense potential to contribute to mathematical discovery, by guiding conjecture generation, constructing counterexamples, assisting in formalizing mathematics, and discovering connections between different mathematical areas, to name a few.
  While prior work has leveraged computers for exhaustive mathematical proof search, recent efforts based on large language models (LLMs) aspire to position computing platforms as co-contributors in the mathematical research process. Despite their current limitations in logic and mathematical tasks, there is growing interest in melding theorem proving systems with foundation models. This work investigates the applicability of LLMs in formalizing advanced mathematical concepts and proposes a framework that can critically review and check mathematical reasoning in research papers. Given the noted reasoning shortcomings of LLMs, our approach synergizes the capabilities of proof assistants, specifically PVS, with LLMs, enabling a bridge between textual descriptions in academic papers and formal specifications in PVS. By harnessing the PVS environment, coupled with data ingestion and conversion mechanisms, we envision an automated process, called \emph{math-PVS}, to extract and formalize mathematical theorems from research papers, offering an innovative tool for academic review and discovery.
        △ Less
      
    
    

    Submitted 25 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.17054
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.LG
          
        
      
    
    
    
      
        BOOST: Harnessing Black-Box Control to Boost Commonsense in LMs' Generation
      
    
    
      Authors:
      
      Yufei Tian, 
      
      Felix Zhang, 
      
      Nanyun Peng
      
    
    
    
      Abstract:
      
        Large language models (LLMs) such as GPT-3 have demonstrated a strong capability to generate coherent and contextually relevant text. However, amidst their successes, a crucial issue persists: their generated outputs still lack commonsense at times. Moreover, fine-tuning the enti…
        ▽ More
      
      
        Large language models (LLMs) such as GPT-3 have demonstrated a strong capability to generate coherent and contextually relevant text. However, amidst their successes, a crucial issue persists: their generated outputs still lack commonsense at times. Moreover, fine-tuning the entire LLM towards more commonsensical outputs is computationally expensive if not infeasible. In this paper, we present a computation-efficient framework that steers a frozen Pre-Trained Language Model (PTLM) towards more commonsensical generation (i.e., producing a plausible output that incorporates a list of concepts in a meaningful way). Specifically, we first construct a reference-free evaluator that assigns a sentence with a commonsensical score by grounding the sentence to a dynamic commonsense knowledge base from four different relational aspects. We then use the scorer as the oracle for commonsense knowledge, and extend the controllable generation method called NADO to train an auxiliary head that guides a fixed PTLM to better satisfy the oracle. We test our framework on a series of GPT-2-, Flan-T5-, and Alpaca-based language models (LMs) on two constrained concept-to-sentence benchmarks. Human evaluation results demonstrate that our method consistently leads to the most commonsensical outputs.
        △ Less
      
    
    

    Submitted 25 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      EMNLP 2023
    
    

    

    
  

  
    
      arXiv:2310.17019
         [pdf, other] 
      
      
        cs.LG
        
          
            cs.CL
          
            cs.RO
          
        
      
    
    
    
      
        Conditionally Combining Robot Skills using Large Language Models
      
    
    
      Authors:
      
      K. R. Zentner, 
      
      Ryan Julian, 
      
      Brian Ichter, 
      
      Gaurav S. Sukhatme
      
    
    
    
      Abstract:
      
        This paper combines two contributions. First, we introduce an extension of the Meta-World benchmark, which we call ""Language-World,"" which allows a large…
        ▽ More
      
      
        This paper combines two contributions. First, we introduce an extension of the Meta-World benchmark, which we call ""Language-World,"" which allows a large language model to operate in a simulated robotic environment using semi-structured natural language queries and scripted skills described using natural language. By using the same set of tasks as Meta-World, Language-World results can be easily compared to Meta-World results, allowing for a point of comparison between recent methods using Large Language Models (LLMs) and those using Deep Reinforcement Learning. Second, we introduce a method we call Plan Conditioned Behavioral Cloning (PCBC), that allows finetuning the behavior of high-level plans using end-to-end demonstrations. Using Language-World, we show that PCBC is able to achieve strong performance in a variety of few-shot regimes, often achieving task generalization with as little as a single demonstration. We have made Language-World available as open-source software at https://github.com/krzentner/language-world/.
        △ Less
      
    
    

    Submitted 25 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.16984
         [pdf, other] 
      
      
        cs.CY
        
      
    
    
    
      
        Patterns of Student Help-Seeking When Using a Large Language Model-Powered Programming Assistant
      
    
    
      Authors:
      
      Brad Sheese, 
      
      Mark Liffiton, 
      
      Jaromir Savelka, 
      
      Paul Denny
      
    
    
    
      Abstract:
      
        Providing personalized assistance at scale is a long-standing challenge for computing educators, but a new generation of tools powered by large language models (LLMs) offers immense promise. Such tools can, in theory, provide on-demand help in large class settings and be configur…
        ▽ More
      
      
        Providing personalized assistance at scale is a long-standing challenge for computing educators, but a new generation of tools powered by large language models (LLMs) offers immense promise. Such tools can, in theory, provide on-demand help in large class settings and be configured with appropriate guardrails to prevent misuse and mitigate common concerns around learner over-reliance. However, the deployment of LLM-powered tools in authentic classroom settings is still rare, and very little is currently known about how students will use them in practice and what type of help they will seek. To address this, we examine students' use of an innovative LLM-powered tool that provides on-demand programming assistance without revealing solutions directly. We deployed the tool for 12 weeks in an introductory computer and data science course (n=52), collecting more than 2,500 queries submitted by students throughout the term. We manually categorized all student queries based on the type of assistance sought, and we automatically analyzed several additional query characteristics. We found that most queries requested immediate help with programming assignments, whereas fewer requests asked for help on related concepts or for deepening conceptual understanding. Furthermore, students often provided minimal information to the tool, suggesting this is an area in which targeted instruction would be beneficial. We also found that students who achieved more success in the course tended to have used the tool more frequently overall. Lessons from this research can be leveraged by programming educators and institutions who plan to augment their teaching with emerging LLM-powered tools.
        △ Less
      
    
    

    Submitted 25 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.16960
         [pdf, other] 
      
      
        cs.LG
        
          
            cs.CR
          
        
      
    
    
    
      
        Privately Aligning Language Models with Reinforcement Learning
      
    
    
      Authors:
      
      Fan Wu, 
      
      Huseyin A. Inan, 
      
      Arturs Backurs, 
      
      Varun Chandrasekaran, 
      
      Janardhan Kulkarni, 
      
      Robert Sim
      
    
    
    
      Abstract:
      
        Positioned between pre-training and user deployment, aligning large language models (LLMs) through reinforcement learning (RL) has emerged as a prevailing strategy for training instruction following-models such as ChatGPT. In this work, we initiate the study of privacy-preserving…
        ▽ More
      
      
        Positioned between pre-training and user deployment, aligning large language models (LLMs) through reinforcement learning (RL) has emerged as a prevailing strategy for training instruction following-models such as ChatGPT. In this work, we initiate the study of privacy-preserving alignment of LLMs through Differential Privacy (DP) in conjunction with RL. Following the influential work of Ziegler et al. (2020), we study two dominant paradigms: (i) alignment via RL without human in the loop (e.g., positive review generation) and (ii) alignment via RL from human feedback (RLHF) (e.g., summarization in a human-preferred way). We give a new DP framework to achieve alignment via RL, and prove its correctness. Our experimental results validate the effectiveness of our approach, offering competitive utility while ensuring strong privacy protections.
        △ Less
      
    
    

    Submitted 25 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.16959
         [pdf, other] 
      
      
        cs.LG
        
      
    
    
    
      
        Improving Few-shot Generalization of Safety Classifiers via Data Augmented Parameter-Efficient Fine-Tuning
      
    
    
      Authors:
      
      Ananth Balashankar, 
      
      Xiao Ma, 
      
      Aradhana Sinha, 
      
      Ahmad Beirami, 
      
      Yao Qin, 
      
      Jilin Chen, 
      
      Alex Beutel
      
    
    
    
      Abstract:
      
        As large language models (LLMs) are widely adopted, new safety issues and policies emerge, to which existing safety classifiers do not generalize well. If we have only observed a few examples of violations of a new safety rule, how can we build a classifier to detect violations?…
        ▽ More
      
      
        As large language models (LLMs) are widely adopted, new safety issues and policies emerge, to which existing safety classifiers do not generalize well. If we have only observed a few examples of violations of a new safety rule, how can we build a classifier to detect violations? In this paper, we study the novel setting of domain-generalized few-shot learning for LLM-based text safety classifiers. Unlike prior few-shot work, these new safety issues can be hard to uncover and we do not get to choose the few examples. We demonstrate that existing few-shot techniques do not perform well in this setting, and rather we propose to do parameter-efficient fine-tuning (PEFT) combined with augmenting training data based on similar examples in prior existing rules. We empirically show that our approach of similarity-based data-augmentation + prompt-tuning (DAPT) consistently outperforms baselines that either do not rely on data augmentation or on PEFT by 7-17% F1 score in the Social Chemistry moral judgement and 9-13% AUC in the Toxicity detection tasks, even when the new rule is loosely correlated with existing ones.
        △ Less
      
    
    

    Submitted 25 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.16958
         [pdf, other] 
      
      
        cs.LG
        
          
            physics.chem-ph
          
        
      
    
    
    
      
        Transferring a molecular foundation model for polymer property predictions
      
    
    
      Authors:
      
      Pei Zhang, 
      
      Logan Kearney, 
      
      Debsindhu Bhowmik, 
      
      Zachary Fox, 
      
      Amit K. Naskar, 
      
      John Gounley
      
    
    
    
      Abstract:
      
        Transformer-based large language models have remarkable potential to accelerate design optimization for applications such as drug development and materials discovery. Self-supervised pretraining of transformer models requires large-scale datasets, which are often sparsely populat…
        ▽ More
      
      
        Transformer-based large language models have remarkable potential to accelerate design optimization for applications such as drug development and materials discovery. Self-supervised pretraining of transformer models requires large-scale datasets, which are often sparsely populated in topical areas such as polymer science. State-of-the-art approaches for polymers conduct data augmentation to generate additional samples but unavoidably incurs extra computational costs. In contrast, large-scale open-source datasets are available for small molecules and provide a potential solution to data scarcity through transfer learning. In this work, we show that using transformers pretrained on small molecules and fine-tuned on polymer properties achieve comparable accuracy to those trained on augmented polymer datasets for a series of benchmark prediction tasks.
        △ Less
      
    
    

    Submitted 25 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.16937
         [pdf, other] 
      
      
        cs.CL
        
      
    
    
    
      
        Learning Transfers over Several Programming Languages
      
    
    
      Authors:
      
      Razan Baltaji, 
      
      Saurabh Pujar, 
      
      Louis Mandel, 
      
      Martin Hirzel, 
      
      Luca Buratti, 
      
      Lav Varshney
      
    
    
    
      Abstract:
      
        Large language models (LLMs) have recently become remarkably good at improving developer productivity for high-resource programming languages. These models use two kinds of data: large amounts of unlabeled code samples for pretraining and relatively smaller amounts of labeled cod…
        ▽ More
      
      
        Large language models (LLMs) have recently become remarkably good at improving developer productivity for high-resource programming languages. These models use two kinds of data: large amounts of unlabeled code samples for pretraining and relatively smaller amounts of labeled code samples for fine-tuning or in-context learning. Unfortunately, many programming languages are low-resource, lacking labeled samples for most tasks and often even lacking unlabeled samples. Therefore, users of low-resource languages (e.g., legacy or new languages) miss out on the benefits of LLMs. Cross-lingual transfer learning uses data from a source language to improve model performance on a target language. It has been well-studied for natural languages, but has received little attention for programming languages. This paper reports extensive experiments on four tasks using a transformer-based LLM and 11 to 41 programming languages to explore the following questions. First, how well cross-lingual transfer works for a given task across different language pairs. Second, given a task and target language, how to best choose a source language. Third, the characteristics of a language pair that are predictive of transfer performance, and fourth, how that depends on the given task.
        △ Less
      
    
    

    Submitted 25 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      16 pages, 5 figures, 5 tables
    
    

    
      
        

        

        
          ACM Class:
          I.2.7; I.2.5
        
      
    

    
  

  
    
      arXiv:2310.16861
         [pdf, other] 
      
      
        cs.LG
        
          
            cs.CV
          
        
      
    
    
    
      
        General Point Model with Autoencoding and Autoregressive
      
    
    
      Authors:
      
      Zhe Li, 
      
      Zhangyang Gao, 
      
      Cheng Tan, 
      
      Stan Z. Li, 
      
      Laurence T. Yang
      
    
    
    
      Abstract:
      
        The pre-training architectures of large…
        ▽ More
      
      
        The pre-training architectures of large language models encompass various types, including autoencoding models, autoregressive models, and encoder-decoder models. We posit that any modality can potentially benefit from a large language model, as long as it undergoes vector quantization to become discrete tokens. Inspired by GLM, we propose a General Point Model (GPM) which seamlessly integrates autoencoding and autoregressive tasks in point cloud transformer. This model is versatile, allowing fine-tuning for downstream point cloud representation tasks, as well as unconditional and conditional generation tasks. GPM enhances masked prediction in autoencoding through various forms of mask padding tasks, leading to improved performance in point cloud understanding. Additionally, GPM demonstrates highly competitive results in unconditional point cloud generation tasks, even exhibiting the potential for conditional generation tasks by modifying the input's conditional information. Compared to models like Point-BERT, MaskPoint and PointMAE, our GPM achieves superior performance in point cloud understanding tasks. Furthermore, the integration of autoregressive and autoencoding within the same transformer underscores its versatility across different downstream tasks.
        △ Less
      
    
    

    Submitted 25 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.16836
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.AI
          
            cs.AR
          
            cs.CV
          
        
      
    
    
    
      
        LLM-FP4: 4-Bit Floating-Point Quantized Transformers
      
    
    
      Authors:
      
      Shih-yang Liu, 
      
      Zechun Liu, 
      
      Xijie Huang, 
      
      Pingcheng Dong, 
      
      Kwang-Ting Cheng
      
    
    
    
      Abstract:
      
        We propose LLM-FP4 for quantizing both weights and activations in large language models (LLMs) down to 4-bit floating-point values, in a post-training manner. Existing post-training quantization (PTQ) solutions are primarily integer-based and struggle with bit widths below 8 bits…
        ▽ More
      
      
        We propose LLM-FP4 for quantizing both weights and activations in large language models (LLMs) down to 4-bit floating-point values, in a post-training manner. Existing post-training quantization (PTQ) solutions are primarily integer-based and struggle with bit widths below 8 bits. Compared to integer quantization, floating-point (FP) quantization is more flexible and can better handle long-tail or bell-shaped distributions, and it has emerged as a default choice in many hardware platforms. One characteristic of FP quantization is that its performance largely depends on the choice of exponent bits and clipping range. In this regard, we construct a strong FP-PTQ baseline by searching for the optimal quantization parameters. Furthermore, we observe a high inter-channel variance and low intra-channel variance pattern in activation distributions, which adds activation quantization difficulty. We recognize this pattern to be consistent across a spectrum of transformer models designed for diverse tasks, such as LLMs, BERT, and Vision Transformer models. To tackle this, we propose per-channel activation quantization and show that these additional scaling factors can be reparameterized as exponential biases of weights, incurring a negligible cost. Our method, for the first time, can quantize both weights and activations in the LLaMA-13B to only 4-bit and achieves an average score of 63.1 on the common sense zero-shot reasoning tasks, which is only 5.8 lower than the full-precision model, significantly outperforming the previous state-of-the-art by 12.7 points. Code is available at: https://github.com/nbasyl/LLM-FP4.
        △ Less
      
    
    

    Submitted 25 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      EMNLP 2023 Main Conference
    
    

    

    
  

  
    
      arXiv:2310.16810
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.AI
          
        
      
    
    
    
      
        Can GPT models Follow Human Summarization Guidelines? Evaluating ChatGPT and GPT-4 for Dialogue Summarization
      
    
    
      Authors:
      
      Yongxin Zhou, 
      
      Fabien Ringeval, 
      
      François Portet
      
    
    
    
      Abstract:
      
        This study explores the capabilities of prompt-driven Large Language Models (LLMs) like ChatGPT and GPT-4 in adhering to human guidelines for dialogue summarization. Experiments employed DialogSum (English social conversations) and DECODA (French call center interactions), testin…
        ▽ More
      
      
        This study explores the capabilities of prompt-driven Large Language Models (LLMs) like ChatGPT and GPT-4 in adhering to human guidelines for dialogue summarization. Experiments employed DialogSum (English social conversations) and DECODA (French call center interactions), testing various prompts: including prompts from existing literature and those from human summarization guidelines, as well as a two-step prompt approach. Our findings indicate that GPT models often produce lengthy summaries and deviate from human summarization guidelines. However, using human guidelines as an intermediate step shows promise, outperforming direct word-length constraint prompts in some cases. The results reveal that GPT models exhibit unique stylistic tendencies in their summaries. While BERTScores did not dramatically decrease for GPT outputs suggesting semantic similarity to human references and specialised pre-trained models, ROUGE scores reveal grammatical and lexical disparities between GPT-generated and human-written summaries. These findings shed light on the capabilities and limitations of GPT models in following human instructions for dialogue summarization.
        △ Less
      
    
    

    Submitted 25 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.16795
         [pdf, other] 
      
      
        cs.LG
        
      
    
    
    
      
        QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models
      
    
    
      Authors:
      
      Elias Frantar, 
      
      Dan Alistarh
      
    
    
    
      Abstract:
      
        Mixture-of-Experts (MoE) architectures offer a general solution to the high inference costs of large language models (LLMs) via sparse routing, bringing faster and more accurate models, at the cost of massive parameter counts. For example, the SwitchTransformer-c2048 model has 1.…
        ▽ More
      
      
        Mixture-of-Experts (MoE) architectures offer a general solution to the high inference costs of large language models (LLMs) via sparse routing, bringing faster and more accurate models, at the cost of massive parameter counts. For example, the SwitchTransformer-c2048 model has 1.6 trillion parameters, requiring 3.2TB of accelerator memory to run efficiently, which makes practical deployment challenging and expensive. In this paper, we present a solution to this memory problem, in form of a new compression and execution framework called QMoE. Specifically, QMoE consists of a scalable algorithm which accurately compresses trillion-parameter MoEs to less than 1 bit per parameter, in a custom format co-designed with bespoke GPU decoding kernels to facilitate efficient end-to-end compressed inference, with minor runtime overheads relative to uncompressed execution. Concretely, QMoE can compress the 1.6 trillion parameter SwitchTransformer-c2048 model to less than 160GB (20x compression, 0.8 bits per parameter) at only minor accuracy loss, in less than a day on a single GPU. This enables, for the first time, the execution of a trillion-parameter model on affordable commodity hardware, like a single server with 4x NVIDIA A6000 or 8x NVIDIA 3090 GPUs, at less than 5% runtime overhead relative to ideal uncompressed inference. The source code and compressed models are available at github.com/IST-DASLab/qmoe.
        △ Less
      
    
    

    Submitted 25 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.16789
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.CR
          
            cs.LG
          
        
      
    
    
    
      
        Detecting Pretraining Data from Large Language Models
      
    
    
      Authors:
      
      Weijia Shi, 
      
      Anirudh Ajith, 
      
      Mengzhou Xia, 
      
      Yangsibo Huang, 
      
      Daogao Liu, 
      
      Terra Blevins, 
      
      Danqi Chen, 
      
      Luke Zettlemoyer
      
    
    
    
      Abstract:
      
        Although large language models (LLMs) are widely deployed, the data used to train them is rarely disclosed. Given the incredible scale of this data, up to trillions of tokens, it is all but certain that it includes potentially problematic text such as copyrighted materials, perso…
        ▽ More
      
      
        Although large language models (LLMs) are widely deployed, the data used to train them is rarely disclosed. Given the incredible scale of this data, up to trillions of tokens, it is all but certain that it includes potentially problematic text such as copyrighted materials, personally identifiable information, and test data for widely reported reference benchmarks. However, we currently have no way to know which data of these types is included or in what proportions. In this paper, we study the pretraining data detection problem: given a piece of text and black-box access to an LLM without knowing the pretraining data, can we determine if the model was trained on the provided text? To facilitate this study, we introduce a dynamic benchmark WIKIMIA that uses data created before and after model training to support gold truth detection. We also introduce a new detection method Min-K% Prob based on a simple hypothesis: an unseen example is likely to contain a few outlier words with low probabilities under the LLM, while a seen example is less likely to have words with such low probabilities. Min-K% Prob can be applied without any knowledge about the pretraining corpus or any additional training, departing from previous detection methods that require training a reference model on data that is similar to the pretraining data. Moreover, our experiments demonstrate that Min-K% Prob achieves a 7.4% improvement on WIKIMIA over these previous methods. We apply Min-K% Prob to two real-world scenarios, copyrighted book detection, and contaminated downstream example detection, and find it a consistently effective solution.
        △ Less
      
    
    

    Submitted 25 October, 2023; 
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.16776
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.AI
          
        
      
    
    
    
      
        DEFT: Data Efficient Fine-Tuning for Large Language Models via Unsupervised Core-Set Selection
      
    
    
      Authors:
      
      Devleena Das, 
      
      Vivek Khetan
      
    
    
    
      Abstract:
      
        Recent advances have led to the availability of many pre-trained language models (PLMs); however, a question that remains is how much data is truly needed to fine-tune PLMs for downstream tasks? In this work, we introduce DEFT, a data-efficient fine-tuning framework that leverages unsupervised core-set selection to minimize the amount of data needed to fine-tune PLMs for downstream tasks. We demon…
        ▽ More
      
      
        Recent advances have led to the availability of many pre-trained language models (PLMs); however, a question that remains is how much data is truly needed to fine-tune PLMs for downstream tasks? In this work, we introduce DEFT, a data-efficient fine-tuning framework that leverages unsupervised core-set selection to minimize the amount of data needed to fine-tune PLMs for downstream tasks. We demonstrate the efficacy of our DEFT framework in the context of text-editing LMs, and compare to the state-of-the art text-editing model, CoEDIT. Our quantitative and qualitative results demonstrate that DEFT models are just as accurate as CoEDIT while being finetuned on ~70% less data.
        △ Less
      
    
    

    Submitted 25 October, 2023; v1 submitted 25 October, 2023;
      originally announced October 2023.
      
    
    

    

    
  

  
    
      arXiv:2310.16763
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.AI
          
            cs.LG
          
        
      
    
    
    
      
        SuperHF: Supervised Iterative Learning from Human Feedback
      
    
    
      Authors:
      
      Gabriel Mukobi, 
      
      Peter Chatain, 
      
      Su Fong, 
      
      Robert Windesheim, 
      
      Gitta Kutyniok, 
      
      Kush Bhatia, 
      
      Silas Alberti
      
    
    
    
      Abstract:
      
        While large language models demonstrate remarkable capabilities, they often present challenges in terms of safety, alignment with human values, and stability during training. Here, we focus on two prevalent methods used to align these models, Supervised Fine-Tuning (SFT) and Rein…
        ▽ More
      
      
        While large language models demonstrate remarkable capabilities, they often present challenges in terms of safety, alignment with human values, and stability during training. Here, we focus on two prevalent methods used to align these models, Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF). SFT is simple and robust, powering a host of open-source models, while RLHF is a more sophisticated method used in top-tier models like ChatGPT but also suffers from instability and susceptibility to reward hacking. We propose a novel approach, Supervised Iterative Learning from Human Feedback (SuperHF), which seeks to leverage the strengths of both methods. Our hypothesis is two-fold: that the reward model used in RLHF is critical for efficient data use and model generalization and that the use of Proximal Policy Optimization (PPO) in RLHF may not be necessary and could contribute to instability issues. SuperHF replaces PPO with a simple supervised loss and a Kullback-Leibler (KL) divergence prior. It creates its own training data by repeatedly sampling a batch of model outputs and filtering them through the reward model in an online learning regime. We then break down the reward optimization problem into three components: robustly optimizing the training rewards themselves, preventing reward hacking-exploitation of the reward model that degrades model performance-as measured by a novel METEOR similarity metric, and maintaining good performance on downstream evaluations. Our experimental results show SuperHF exceeds PPO-based RLHF on the training objective, easily and favorably trades off high reward with low reward hacking, improves downstream calibration, and performs the same on our GPT-4 based qualitative evaluation scheme all the while being significantly simpler to implement, highlighting SuperHF's potential as a competitive language model alignment technique.
        △ Less
      
    
    

    Submitted 25 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      Accepted to the Socially Responsible Language Modelling Research (SoLaR) workshop at NeurIPS 2023
    
    

    

    
  

  
    
      arXiv:2310.16755
         [pdf, other] 
      
      
        cs.CL
        
          
            cs.AI
          
        
      
    
    
    
      
        HI-TOM: A Benchmark for Evaluating Higher-Order Theory of Mind Reasoning in Large Language Models
      
    
    
      Authors:
      
      Yinghui He, 
      
      Yufan Wu, 
      
      Yilin Jia, 
      
      Rada Mihalcea, 
      
      Yulong Chen, 
      
      Naihao Deng
      
    
    
    
      Abstract:
      
        …ToM, which involves recursive reasoning on others' beliefs. We introduce HI-TOM, a Higher Order Theory of Mind benchmark. Our experimental evaluation using various Large Language Models (LLMs) indicates a decline in performance on higher-order ToM tasks, demonstrating the lim…
        ▽ More
      
      
        Theory of Mind (ToM) is the ability to reason about one's own and others' mental states. ToM plays a critical role in the development of intelligence, language understanding, and cognitive processes. While previous work has primarily focused on first and second-order ToM, we explore higher-order ToM, which involves recursive reasoning on others' beliefs. We introduce HI-TOM, a Higher Order Theory of Mind benchmark. Our experimental evaluation using various Large Language Models (LLMs) indicates a decline in performance on higher-order ToM tasks, demonstrating the limitations of current LLMs. We conduct a thorough analysis of different failure cases of LLMs, and share our thoughts on the implications of our findings on the future of NLP.
        △ Less
      
    
    

    Submitted 25 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      Accepted at Findings of EMNLP 2023
    
    

    

    
      
        Journal ref:
        Findings of EMNLP 2023
      
    
  

  
    
      arXiv:2310.16746
         [pdf, other] 
      
      
        cs.CL
        
      
    
    
    
      
        HANSEN: Human and AI Spoken Text Benchmark for Authorship Analysis
      
    
    
      Authors:
      
      Nafis Irtiza Tripto, 
      
      Adaku Uchendu, 
      
      Thai Le, 
      
      Mattia Setzu, 
      
      Fosca Giannotti, 
      
      Dongwon Lee
      
    
    
    
      Abstract:
      
        Authorship Analysis, also known as stylometry, has been an essential aspect of Natural Language Processing (NLP) for a long time. Likewise, the recent advancement of Large Language Models (LLMs) has made authorship analysis increasingly crucial for distinguishing between human-wr…
        ▽ More
      
      
        Authorship Analysis, also known as stylometry, has been an essential aspect of Natural Language Processing (NLP) for a long time. Likewise, the recent advancement of Large Language Models (LLMs) has made authorship analysis increasingly crucial for distinguishing between human-written and AI-generated texts. However, these authorship analysis tasks have primarily been focused on written texts, not considering spoken texts. Thus, we introduce the largest benchmark for spoken texts - HANSEN (Human ANd ai Spoken tExt beNchmark). HANSEN encompasses meticulous curation of existing speech datasets accompanied by transcripts, alongside the creation of novel AI-generated spoken text datasets. Together, it comprises 17 human datasets, and AI-generated spoken texts created using 3 prominent LLMs: ChatGPT, PaLM2, and Vicuna13B. To evaluate and demonstrate the utility of HANSEN, we perform Authorship Attribution (AA) & Author Verification (AV) on human-spoken datasets and conducted Human vs. AI spoken text detection using state-of-the-art (SOTA) models. While SOTA methods, such as, character ngram or Transformer-based model, exhibit similar AA & AV performance in human-spoken datasets compared to written ones, there is much room for improvement in AI-generated spoken text detection. The HANSEN benchmark is available at: https://huggingface.co/datasets/HANSEN-REPO/HANSEN.
        △ Less
      
    
    

    Submitted 25 October, 2023; 
      originally announced October 2023.
      
    
    
    
      Comments:
      9 pages, EMNLP-23 findings, 5 pages appendix, 6 figures, 17 tables
    
    

    

    
  




  
    
    Previous
    
    
    
      Next
      
    
    

      
        1
        
      

      
                                     
          
          
            2
            
          
          
          
            3
            
          
          
          
            4
            
          
          
          
            5
            
          
          
          …
        
      
    
  
  

  


      
        
        Search v0.5.6 released 2020-02-24",,,,,,,,,
1698806000-1,https://scholar.google.ca/scholar?hl=en&as_sdt=0%2C5&q=LLM+vulnerabilities&btnG=,,,,"[PDF] arxiv.orgHow well does LLM generate security tests?Y Zhang, W Song, Z Ji, N Meng - arXiv preprint arXiv:2310.00710, 2023 - arxiv.org… We consider an API in 𝐿𝑖𝑏 to be vulnerable if it (1) is mentioned or implied by the 

vulnerability description of CVE or JIRA entry and is revised, (2) directly or indirectly calls the …Save Cite  View as HTML   [PDF] arxiv.orgDemystifying RCE Vulnerabilities in LLM-Integrated AppsT Liu, Z Deng, G Meng, Y Li, K Chen - arXiv preprint arXiv:2309.02926, 2023 - arxiv.org… an automated approach LLMSMITH to identify vulnerabilities in LLM-integrated frameworks 

and apps. As shown in Figure 2, it consists of four main modules: vulnerable API detection, …Save Cite  View as HTML   [PDF] diva-portal.orgKARTAL: Web Application Vulnerability Hunting Using Large Language Models: Novel method for detecting logical vulnerabilities in web applications with finetuned …S Sakaoglu - 2023 - diva-portal.org… Broken Access Control vulnerabilities. We modeled the … vulnerability detection using a 

Large Language Model (LLM). It … an LLM which we have finetuned for detecting vulnerabilities…Save Cite  View as HTML   [PDF] arxiv.orgLLM for SoC Security: A Paradigm ShiftD Saha, S Tarek, K Yahyaei, SK Saha, J Zhou… - arXiv preprint arXiv …, 2023 - arxiv.org… 1) Vulnerability Insertion: We show how adeptly LLM can introduce potential vulnerability 

and … vulnerabilities, weaknesses, and threats through LLM. We also examine the ability of …Save Cite All 3 versions  View as HTML   [PDF] arxiv.orgHarnessing the Power of LLM to Support Binary Taint AnalysisP Liu, C Sun, Y Zheng, X Feng, C Qin, Y Wang… - arXiv preprint arXiv …, 2023 - arxiv.org… first study for vulnerability discovery that explores the synergistic combination of the LLM’s … 

We present LATTE, an LLM-powered binary vulnerability methodology that utilizes LLMs to …Save Cite  View as HTML   [PDF] arxiv.orgDefectHunter: A Novel LLM-Driven Boosted-Conformer-based Code Vulnerability Detection MechanismJ Wang, Z Huang, H Liu, N Yang, Y Xiao - arXiv preprint arXiv:2309.15324, 2023 - arxiv.org… Subsequently, we isolated code snippets featuring vulnerabilities and formatted them for 

LLM training as depicted in Fig.4. Specifically, the LLM requires a specialized input format, …Save Cite  View as HTML   [PDF] arxiv.orgPentestgpt: An llm-empowered automatic penetration testing toolG Deng, Y Liu, V Mayoral-Vilches, P Liu, Y Li… - arXiv preprint arXiv …, 2023 - arxiv.org… all vulnerabilities appearing in OWASP’s top 10 vulnerability list … a penetration testing goal 

for the LLM, soliciting it for the … the test outputs back to the LLM for next-step reasoning (Figure …Save Cite Cited by 5 All 3 versions  View as HTML   [PDF] arxiv.orgFrom Prompt Injections to SQL Injection Attacks: How Protected is Your LLM-Integrated Web Application?R Pedro, D Castro, P Carreira, N Santos - arXiv preprint arXiv:2308.01990, 2023 - arxiv.org… To address this question, we surveyed seven state-of-the-art LLM technologies, including … 

), we discovered that LLMintegrated applications based on Langchain are highly vulnerable to …Save Cite Cited by 2 All 2 versions  View as HTML   [PDF] aclanthology.orgNot The End of Story: An Evaluation of ChatGPT-Driven Vulnerability Description MappingsX Liu, Y Tan, Z Xiao, J Zhuge… - Findings of the Association …, 2023 - aclanthology.org… engineers in vulnerability analysis. In a word, closedsource LLM is not the end of story. … 

provide CWE IDs for all vulnerabilities, and we excluded vulnerabilities for which CVE did …Save Cite Cited by 2 All 2 versions  View as HTML   [PDF] arxiv.orgUse of LLMs for Illicit Purposes: Threats, Prevention Measures, and VulnerabilitiesM Mozes, X He, B Kleinberg, LD Griffin - arXiv preprint arXiv:2308.12833, 2023 - arxiv.org… (2023) provide a categorization of LLM vulnerabilities … first covers vulnerabilities such as 

factual errors where an LLM … development errors enable LLM vulnerabilities. With respect to …Save Cite Cited by 3 All 2 versions  View as HTML Create alertPrevious12345678910Next12345678910",,,,,,
1698806000-2,https://scholar.google.ca/scholar?hl=en&as_sdt=0%2C5&q=LLM+vulnerabilities&btnG=,,,,,"[PDF] arxiv.orgHow well does LLM generate security tests?Y Zhang, W Song, Z Ji, N Meng - arXiv preprint arXiv:2310.00710, 2023 - arxiv.org… We consider an API in 𝐿𝑖𝑏 to be vulnerable if it (1) is mentioned or implied by the 

vulnerability description of CVE or JIRA entry and is revised, (2) directly or indirectly calls the …Save Cite  View as HTML   [PDF] arxiv.orgDemystifying RCE Vulnerabilities in LLM-Integrated AppsT Liu, Z Deng, G Meng, Y Li, K Chen - arXiv preprint arXiv:2309.02926, 2023 - arxiv.org… an automated approach LLMSMITH to identify vulnerabilities in LLM-integrated frameworks 

and apps. As shown in Figure 2, it consists of four main modules: vulnerable API detection, …Save Cite  View as HTML   [PDF] diva-portal.orgKARTAL: Web Application Vulnerability Hunting Using Large Language Models: Novel method for detecting logical vulnerabilities in web applications with finetuned …S Sakaoglu - 2023 - diva-portal.org… Broken Access Control vulnerabilities. We modeled the … vulnerability detection using a 

Large Language Model (LLM). It … an LLM which we have finetuned for detecting vulnerabilities…Save Cite  View as HTML   [PDF] arxiv.orgLLM for SoC Security: A Paradigm ShiftD Saha, S Tarek, K Yahyaei, SK Saha, J Zhou… - arXiv preprint arXiv …, 2023 - arxiv.org… 1) Vulnerability Insertion: We show how adeptly LLM can introduce potential vulnerability 

and … vulnerabilities, weaknesses, and threats through LLM. We also examine the ability of …Save Cite All 3 versions  View as HTML   [PDF] arxiv.orgHarnessing the Power of LLM to Support Binary Taint AnalysisP Liu, C Sun, Y Zheng, X Feng, C Qin, Y Wang… - arXiv preprint arXiv …, 2023 - arxiv.org… first study for vulnerability discovery that explores the synergistic combination of the LLM’s … 

We present LATTE, an LLM-powered binary vulnerability methodology that utilizes LLMs to …Save Cite  View as HTML   [PDF] arxiv.orgDefectHunter: A Novel LLM-Driven Boosted-Conformer-based Code Vulnerability Detection MechanismJ Wang, Z Huang, H Liu, N Yang, Y Xiao - arXiv preprint arXiv:2309.15324, 2023 - arxiv.org… Subsequently, we isolated code snippets featuring vulnerabilities and formatted them for 

LLM training as depicted in Fig.4. Specifically, the LLM requires a specialized input format, …Save Cite  View as HTML   [PDF] arxiv.orgPentestgpt: An llm-empowered automatic penetration testing toolG Deng, Y Liu, V Mayoral-Vilches, P Liu, Y Li… - arXiv preprint arXiv …, 2023 - arxiv.org… all vulnerabilities appearing in OWASP’s top 10 vulnerability list … a penetration testing goal 

for the LLM, soliciting it for the … the test outputs back to the LLM for next-step reasoning (Figure …Save Cite Cited by 5 All 3 versions  View as HTML   [PDF] arxiv.orgFrom Prompt Injections to SQL Injection Attacks: How Protected is Your LLM-Integrated Web Application?R Pedro, D Castro, P Carreira, N Santos - arXiv preprint arXiv:2308.01990, 2023 - arxiv.org… To address this question, we surveyed seven state-of-the-art LLM technologies, including … 

), we discovered that LLMintegrated applications based on Langchain are highly vulnerable to …Save Cite Cited by 2 All 2 versions  View as HTML   [PDF] aclanthology.orgNot The End of Story: An Evaluation of ChatGPT-Driven Vulnerability Description MappingsX Liu, Y Tan, Z Xiao, J Zhuge… - Findings of the Association …, 2023 - aclanthology.org… engineers in vulnerability analysis. In a word, closedsource LLM is not the end of story. … 

provide CWE IDs for all vulnerabilities, and we excluded vulnerabilities for which CVE did …Save Cite Cited by 2 All 2 versions  View as HTML   [PDF] arxiv.orgUse of LLMs for Illicit Purposes: Threats, Prevention Measures, and VulnerabilitiesM Mozes, X He, B Kleinberg, LD Griffin - arXiv preprint arXiv:2308.12833, 2023 - arxiv.org… (2023) provide a categorization of LLM vulnerabilities … first covers vulnerabilities such as 

factual errors where an LLM … development errors enable LLM vulnerabilities. With respect to …Save Cite Cited by 3 All 2 versions  View as HTML Create alertPrevious12345678910Next12345678910",,,,,
1698807591-1,https://www.researchgate.net/search.Search.html?query=%22Large+Language+Model+Security%22+&type=publication,,,,,,,"Fixing Hardware Security Bugs with Large Language ModelsPreprintFebruary 202325 ReadsBaleegh AhmadShailja ThakurBenjamin Tan[...]Hammond PearceNovel AI-based code-writing Large Language Models (LLMs) such as OpenAI's Codex have demonstrated capabilities in many coding-adjacent domains. In this work we consider how LLMs maybe leveraged to automatically repair security relevant bugs present in hardware designs. We focus on bug repair in code written in the Hardware Description Language Verilog. For this study we build a corpus of domain-representative hardware security bugs. We then design and implement a framework to quantitatively evaluate the performance of any LLM tasked with fixing the specified bugs. The framework supports design space exploration of prompts (i.e., prompt engineering) and identifying the best parameters for the LLM. We show that an ensemble of LLMs can repair all ten of our benchmarks. This ensemble outperforms the state-of-the-art Cirfix hardware bug repair tool on its own suite of bugs. These results show that LLMs can repair hardware security bugs and the framework is an important step towards the ultimate goal of an automated end-to-end bug repair framework.Request fileRecommendFollowShare",,,
1698807591-2,https://www.researchgate.net/search.Search.html?query=%22Large+Language+Model+Security%22+&type=publication,,,,,,,"Building Resilient SMEs: Harnessing Large Language Models for Cyber Security in AustraliaPreprintJune 202387 ReadsBenjamin Kereopa-YorkeThe escalating digitalisation of our lives and enterprises has led to a parallel growth in the complexity and frequency of cyber-attacks. Small and medium-sized enterprises (SMEs), particularly in Australia, are experiencing increased vulnerability to cyber threats, posing a significant challenge to the nation's cyber security landscape. Embracing transformative technologies such as Artificial Intelligence (AI), Machine Learning (ML) and Large Language Models (LLMs) can potentially strengthen cyber security policies for Australian SMEs. However, their practical application, advantages, and limitations remain underexplored, with prior research mainly focusing on large corporations. This study aims to address this gap by providing a comprehensive understanding of the potential role of LLMs in enhancing cyber security policies for Australian SMEs. Employing a mixed-methods study design, this research includes a literature review, qualitative analysis of SME case studies, and a quantitative assessment of LLM performance metrics in cyber security applications. The findings highlight the promising potential of LLMs across various performance criteria, including relevance, accuracy, and applicability, though gaps remain in areas such as completeness and clarity. The study underlines the importance of integrating human expertise with LLM technology and refining model development to address these limitations. By proposing a robust conceptual framework guiding the effective adoption of LLMs, this research aims to contribute to a safer and more resilient cyber environment for Australian SMEs, enabling sustainable growth and competitiveness in the digital era.DownloadRecommendFollowShare",,,
1698807591-3,https://www.researchgate.net/search.Search.html?query=%22Large+Language+Model+Security%22+&type=publication,,,,,,,"An Empirical Study on Using Large Language Models to Analyze Software Supply Chain Security FailuresPreprintAugust 20239 ReadsTanmay SinglaDharun AnandayuvarajKelechi Kalu[...]James C. DavisAs we increasingly depend on software systems, the consequences of breaches in the software supply chain become more severe. High-profile cyber attacks like those on SolarWinds and ShadowHammer have resulted in significant financial and data losses, underlining the need for stronger cybersecurity. One way to prevent future breaches is by studying past failures. However, traditional methods of analyzing these failures require manually reading and summarizing reports about them. Automated support could reduce costs and allow analysis of more failures. Natural Language Processing (NLP) techniques such as Large Language Models (LLMs) could be leveraged to assist the analysis of failures. In this study, we assessed the ability of Large Language Models (LLMs) to analyze historical software supply chain breaches. We used LLMs to replicate the manual analysis of 69 software supply chain security failures performed by members of the Cloud Native Computing Foundation (CNCF). We developed prompts for LLMs to categorize these by four dimensions: type of compromise, intent, nature, and impact. GPT 3.5s categorizations had an average accuracy of 68% and Bard had an accuracy of 58% over these dimensions. We report that LLMs effectively characterize software supply chain failures when the source articles are detailed enough for consensus among manual analysts, but cannot yet replace human analysts. Future work can improve LLM performance in this context, and study a broader range of articles and failures.Request fileRecommendFollowShare",,,
1698807591-4,https://www.researchgate.net/search.Search.html?query=%22Large+Language+Model+Security%22+&type=publication,,,,,,,"Can Large Language Models Provide Security & Privacy Advice? Measuring the Ability of LLMs to Refute MisconceptionsPreprintOctober 202333 ReadsYufan ChenArjun ArunasalamZ. Berkay CelikUsers seek security & privacy (S&P) advice from online resources, including trusted websites and content-sharing platforms. These resources help users understand S&P technologies and tools and suggest actionable strategies. Large Language Models (LLMs) have recently emerged as trusted information sources. However, their accuracy and correctness have been called into question. Prior research has outlined the shortcomings of LLMs in answering multiple-choice questions and user ability to inadvertently circumvent model restrictions (e.g., to produce toxic content). Yet, the ability of LLMs to provide reliable S&P advice is not well-explored. In this paper, we measure their ability to refute popular S&P misconceptions that the general public holds. We first study recent academic literature to curate a dataset of over a hundred S&P-related misconceptions across six different topics. We then query two popular LLMs (Bard and ChatGPT) and develop a labeling guide to evaluate their responses to these misconceptions. To comprehensively evaluate their responses, we further apply three strategies: query each misconception multiple times, generate and query their paraphrases, and solicit source URLs of the responses. Both models demonstrate, on average, a 21.3% non-negligible error rate, incorrectly supporting popular S&P misconceptions. The error rate increases to 32.6% when we repeatedly query LLMs with the same or paraphrased misconceptions. We also expose that models may partially support a misconception or remain noncommittal, refusing a firm stance on misconceptions. Our exploration of information sources for responses revealed that LLMs are susceptible to providing invalid URLs (21.2% for Bard and 67.7% for ChatGPT) or point to unrelated sources (44.2% returned by Bard and 18.3% by ChatGPT). Our findings highlight that existing LLMs are not completely reliable for S&P advice and motivate future work in understanding how users can better interact with this technology.DownloadRecommendFollowShare",,,
1698807591-5,https://www.researchgate.net/search.Search.html?query=%22Large+Language+Model+Security%22+&type=publication,,,,,,,"Security Implications of Large Language Model Code Assistants: A User StudyPreprintAugust 202219 ReadsGustavo SandovalHammond PearceTeo Nys[...]Siddharth GargAdvances in Deep Learning have led to the emergence of Large Language Models (LLMs) such as OpenAI Codex which powers GitHub Copilot. LLMs have been fine tuned and packaged so that programmers can use them in an Integrated Development Environment (IDE) to write code. An emerging line of work is assessing the code quality of code written with the help of these LLMs, with security studies warning that LLMs do not fundamentally have any understanding of the code they are writing, so they are more likely to make mistakes that may be exploitable. We thus conducted a user study (N=58) to assess the security of code written by student programmers when guided by LLMs. Half of the students in our study had the help of the LLM and the other half did not. The students were asked to write code in C that performed operations over a singly linked list, including node operations such as inserting, updating, removing, combining, and others. While the results of our study showed that the students who had the help of an LLM were more likely to write functional code, no generalizable impact on security was observed -- the security impacts were localized to individual functions. We also investigate systematic stylistic differences between unaided and LLM-assisted code, finding that LLM code is more repetitive, which may have an amplifying effect if vulnerable code is repeated in addition to the impact on source code attribution.Request fileRecommendFollowShare",,,
1698807591-6,https://www.researchgate.net/search.Search.html?query=%22Large+Language+Model+Security%22+&type=publication,,,,,,,"Speech and Language Disorders in Children: Implications for the Social Security Administration's Supplemental Security Income ProgramBookApril 2016382 Reads33 CitationsNational Academies of Sciences, Engineering, and MedicineInstitute of MedicineDivision of Behavioral and Social Sciences and Education[...]Committee on the Evaluation of the Supplemental Security Income (SSI) Disability Program for Children with Speech Disorders and Language DisordersSpeech and language are central to the human experience; they are the vital means by which people convey and receive knowledge, thoughts, feelings, and other internal experiences. Acquisition of communication skills begins early in childhood and is foundational to the ability to gain access to culturally transmitted knowledge, organize and share thoughts and feelings, and participate in social interactions and relationships. Thus, speech disorders and language disorders--disruptions in communication development--can have wide-ranging and adverse impacts on the ability to communicate and also to acquire new knowledge and fully participate in society. Severe disruptions in speech or language acquisition have both direct and indirect consequences for child and adolescent development, not only in communication, but also in associated abilities such as reading and academic achievement that depend on speech and language skills. The Supplemental Security Income (SSI) program for children provides financial assistance to children from low-income, resource-limited families who are determined to have conditions that meet the disability standard required under law. Between 2000 and 2010, there was an unprecedented rise in the number of applications and the number of children found to meet the disability criteria. The factors that contribute to these changes are a primary focus of this report. Speech and Language Disorders in Children provides an overview of the current status of the diagnosis and treatment of speech and language disorders and levels of impairment in the U.S. population under age 18. This study identifies past and current trends in the prevalence and persistence of speech disorders and language disorders for the general U.S. population under age 18 and compares those trends to trends in the SSI childhood disability population. © 2016 by the National Academy of Sciences. All rights reserved.Request full-textRecommendFollowShare",,,
1698807591-7,https://www.researchgate.net/search.Search.html?query=%22Large+Language+Model+Security%22+&type=publication,,,,,,,"Large Language Models in Cryptocurrency Securities Cases: Can ChatGPT Replace Lawyers?PreprintAugust 202343 ReadsArianna TrozzeToby DaviesBennett KleinbergLarge Language Models (LLMs) could enhance access to the legal system. However, empirical research on their effectiveness in conducting legal tasks is scant. We study securities cases involving cryptocurrencies as one of numerous contexts where AI could support the legal process, studying LLMs' legal reasoning and drafting capabilities. We examine whether a) an LLM can accurately determine which laws are potentially being violated from a fact pattern, and b) whether there is a difference in juror decision-making based on complaints written by a lawyer compared to an LLM. We feed fact patterns from real-life cases to GPT-3.5 and evaluate its ability to determine correct potential violations from the scenario and exclude spurious violations. Second, we had mock jurors assess complaints written by the LLM and lawyers. GPT-3.5's legal reasoning skills proved weak, though we expect improvement in future models, particularly given the violations it suggested tended to be correct (it merely missed additional, correct violations). GPT-3.5 performed better at legal drafting, and jurors' decisions were not statistically significantly associated with the author of the document upon which they based their decisions. Because LLMs cannot satisfactorily conduct legal reasoning tasks, they would be unable to replace lawyers at this stage. However, their drafting skills (though, perhaps, still inferior to lawyers), could provide access to justice for more individuals by reducing the cost of legal services. Our research is the first to systematically study LLMs' legal drafting and reasoning capabilities in litigation, as well as in securities law and cryptocurrency-related misconduct.DownloadRecommendFollowShare",,,
1698807591-8,https://www.researchgate.net/search.Search.html?query=%22Large+Language+Model+Security%22+&type=publication,,,,,,,"Business Language for Information SecurityChapterJuly 202320 ReadsDinh Uy TranAudun JøsangProminent standards and frameworks for information security clearly state that business aspects on the one side, and technical aspects on the other, are equally important for the management of cyber security. Organisations with a relatively low maturity level in security management typically consider information security primarily as a technological issue. For those organisations, information security might not get the necessary support from top-level management because they are predominantly focused on business aspects, and are blind to the role information security plays for business. To obtain support from top-level management the information security practitioners need the skills to influence and help relevant stakeholders to understand how information security can support business objectives. In this debate, it is often argued that it is important to speak the language of management. This means that information security practitioners should learn how to translate technical terms to a business context, so top-level management can understand what it means for them. However, this debate has mostly focused on the importance of speaking the “Business Language for Information Security (BLIS)” but has not elaborated on what this language consists of and how to learn it. This paper proposes BLIS and a framework for how to learn it. By mastering BLIS, security professionals can articulate arguments that top-executive management can easily understand and act on. Therefore, we argue that taking a learning module on BLIS will be valuable and useful for the next generation of students in information security. Said briefly, learning BLIS will help students understand how information security can support business, and also how this can be explained to others.KeywordsBusiness Language for Information SecurityInformation Security GovernanceInformation Security ManagementInformation Security ReportingRequest full-textRecommendFollowShare",,,
1698807591-9,https://www.researchgate.net/search.Search.html?query=%22Large+Language+Model+Security%22+&type=publication,,,,,,,A Multi-dimensional Generic Evaluation Framework for the Security of Large Language ModelsConference PaperAugust 20233 ReadsZhiyin YuRequest full-textRecommendFollowShare,,,
1698807591-10,https://www.researchgate.net/search.Search.html?query=%22Large+Language+Model+Security%22+&type=publication,,,,,,,"LLM for SoC Security: A Paradigm ShiftPreprintOctober 202335 ReadsDipayan SahaShams TarekKatayoon Yahyaei[...]Farimah FarahmandiAs the ubiquity and complexity of system-on-chip (SoC) designs increase across electronic devices, the task of incorporating security into an SoC design flow poses significant challenges. Existing security solutions are inadequate to provide effective verification of modern SoC designs due to their limitations in scalability, comprehensiveness, and adaptability. On the other hand, Large Language Models (LLMs) are celebrated for their remarkable success in natural language understanding, advanced reasoning, and program synthesis tasks. Recognizing an opportunity, our research delves into leveraging the emergent capabilities of Generative Pre-trained Transformers (GPTs) to address the existing gaps in SoC security, aiming for a more efficient, scalable, and adaptable methodology. By integrating LLMs into the SoC security verification paradigm, we open a new frontier of possibilities and challenges to ensure the security of increasingly complex SoCs. This paper offers an in-depth analysis of existing works, showcases practical case studies, demonstrates comprehensive experiments, and provides useful promoting guidelines. We also present the achievements, prospects, and challenges of employing LLM in different SoC security verification tasks.DownloadRecommendFollowShare",,,
1698807591-11,https://www.researchgate.net/search.Search.html?query=%22Large+Language+Model+Security%22+&type=publication,,,,,,,,"Fixing Hardware Security Bugs with Large Language ModelsPreprintFebruary 202325 ReadsBaleegh AhmadShailja ThakurBenjamin Tan[...]Hammond PearceNovel AI-based code-writing Large Language Models (LLMs) such as OpenAI's Codex have demonstrated capabilities in many coding-adjacent domains. In this work we consider how LLMs maybe leveraged to automatically repair security relevant bugs present in hardware designs. We focus on bug repair in code written in the Hardware Description Language Verilog. For this study we build a corpus of domain-representative hardware security bugs. We then design and implement a framework to quantitatively evaluate the performance of any LLM tasked with fixing the specified bugs. The framework supports design space exploration of prompts (i.e., prompt engineering) and identifying the best parameters for the LLM. We show that an ensemble of LLMs can repair all ten of our benchmarks. This ensemble outperforms the state-of-the-art Cirfix hardware bug repair tool on its own suite of bugs. These results show that LLMs can repair hardware security bugs and the framework is an important step towards the ultimate goal of an automated end-to-end bug repair framework.Request fileRecommendFollowShare",,
1698807591-12,https://www.researchgate.net/search.Search.html?query=%22Large+Language+Model+Security%22+&type=publication,,,,,,,,"Building Resilient SMEs: Harnessing Large Language Models for Cyber Security in AustraliaPreprintJune 202387 ReadsBenjamin Kereopa-YorkeThe escalating digitalisation of our lives and enterprises has led to a parallel growth in the complexity and frequency of cyber-attacks. Small and medium-sized enterprises (SMEs), particularly in Australia, are experiencing increased vulnerability to cyber threats, posing a significant challenge to the nation's cyber security landscape. Embracing transformative technologies such as Artificial Intelligence (AI), Machine Learning (ML) and Large Language Models (LLMs) can potentially strengthen cyber security policies for Australian SMEs. However, their practical application, advantages, and limitations remain underexplored, with prior research mainly focusing on large corporations. This study aims to address this gap by providing a comprehensive understanding of the potential role of LLMs in enhancing cyber security policies for Australian SMEs. Employing a mixed-methods study design, this research includes a literature review, qualitative analysis of SME case studies, and a quantitative assessment of LLM performance metrics in cyber security applications. The findings highlight the promising potential of LLMs across various performance criteria, including relevance, accuracy, and applicability, though gaps remain in areas such as completeness and clarity. The study underlines the importance of integrating human expertise with LLM technology and refining model development to address these limitations. By proposing a robust conceptual framework guiding the effective adoption of LLMs, this research aims to contribute to a safer and more resilient cyber environment for Australian SMEs, enabling sustainable growth and competitiveness in the digital era.DownloadRecommendFollowShare",,
1698807591-13,https://www.researchgate.net/search.Search.html?query=%22Large+Language+Model+Security%22+&type=publication,,,,,,,,"An Empirical Study on Using Large Language Models to Analyze Software Supply Chain Security FailuresPreprintAugust 20239 ReadsTanmay SinglaDharun AnandayuvarajKelechi Kalu[...]James C. DavisAs we increasingly depend on software systems, the consequences of breaches in the software supply chain become more severe. High-profile cyber attacks like those on SolarWinds and ShadowHammer have resulted in significant financial and data losses, underlining the need for stronger cybersecurity. One way to prevent future breaches is by studying past failures. However, traditional methods of analyzing these failures require manually reading and summarizing reports about them. Automated support could reduce costs and allow analysis of more failures. Natural Language Processing (NLP) techniques such as Large Language Models (LLMs) could be leveraged to assist the analysis of failures. In this study, we assessed the ability of Large Language Models (LLMs) to analyze historical software supply chain breaches. We used LLMs to replicate the manual analysis of 69 software supply chain security failures performed by members of the Cloud Native Computing Foundation (CNCF). We developed prompts for LLMs to categorize these by four dimensions: type of compromise, intent, nature, and impact. GPT 3.5s categorizations had an average accuracy of 68% and Bard had an accuracy of 58% over these dimensions. We report that LLMs effectively characterize software supply chain failures when the source articles are detailed enough for consensus among manual analysts, but cannot yet replace human analysts. Future work can improve LLM performance in this context, and study a broader range of articles and failures.Request fileRecommendFollowShare",,
1698807591-14,https://www.researchgate.net/search.Search.html?query=%22Large+Language+Model+Security%22+&type=publication,,,,,,,,"Can Large Language Models Provide Security & Privacy Advice? Measuring the Ability of LLMs to Refute MisconceptionsPreprintOctober 202333 ReadsYufan ChenArjun ArunasalamZ. Berkay CelikUsers seek security & privacy (S&P) advice from online resources, including trusted websites and content-sharing platforms. These resources help users understand S&P technologies and tools and suggest actionable strategies. Large Language Models (LLMs) have recently emerged as trusted information sources. However, their accuracy and correctness have been called into question. Prior research has outlined the shortcomings of LLMs in answering multiple-choice questions and user ability to inadvertently circumvent model restrictions (e.g., to produce toxic content). Yet, the ability of LLMs to provide reliable S&P advice is not well-explored. In this paper, we measure their ability to refute popular S&P misconceptions that the general public holds. We first study recent academic literature to curate a dataset of over a hundred S&P-related misconceptions across six different topics. We then query two popular LLMs (Bard and ChatGPT) and develop a labeling guide to evaluate their responses to these misconceptions. To comprehensively evaluate their responses, we further apply three strategies: query each misconception multiple times, generate and query their paraphrases, and solicit source URLs of the responses. Both models demonstrate, on average, a 21.3% non-negligible error rate, incorrectly supporting popular S&P misconceptions. The error rate increases to 32.6% when we repeatedly query LLMs with the same or paraphrased misconceptions. We also expose that models may partially support a misconception or remain noncommittal, refusing a firm stance on misconceptions. Our exploration of information sources for responses revealed that LLMs are susceptible to providing invalid URLs (21.2% for Bard and 67.7% for ChatGPT) or point to unrelated sources (44.2% returned by Bard and 18.3% by ChatGPT). Our findings highlight that existing LLMs are not completely reliable for S&P advice and motivate future work in understanding how users can better interact with this technology.DownloadRecommendFollowShare",,
1698807591-15,https://www.researchgate.net/search.Search.html?query=%22Large+Language+Model+Security%22+&type=publication,,,,,,,,"Security Implications of Large Language Model Code Assistants: A User StudyPreprintAugust 202219 ReadsGustavo SandovalHammond PearceTeo Nys[...]Siddharth GargAdvances in Deep Learning have led to the emergence of Large Language Models (LLMs) such as OpenAI Codex which powers GitHub Copilot. LLMs have been fine tuned and packaged so that programmers can use them in an Integrated Development Environment (IDE) to write code. An emerging line of work is assessing the code quality of code written with the help of these LLMs, with security studies warning that LLMs do not fundamentally have any understanding of the code they are writing, so they are more likely to make mistakes that may be exploitable. We thus conducted a user study (N=58) to assess the security of code written by student programmers when guided by LLMs. Half of the students in our study had the help of the LLM and the other half did not. The students were asked to write code in C that performed operations over a singly linked list, including node operations such as inserting, updating, removing, combining, and others. While the results of our study showed that the students who had the help of an LLM were more likely to write functional code, no generalizable impact on security was observed -- the security impacts were localized to individual functions. We also investigate systematic stylistic differences between unaided and LLM-assisted code, finding that LLM code is more repetitive, which may have an amplifying effect if vulnerable code is repeated in addition to the impact on source code attribution.Request fileRecommendFollowShare",,
1698807591-16,https://www.researchgate.net/search.Search.html?query=%22Large+Language+Model+Security%22+&type=publication,,,,,,,,"Speech and Language Disorders in Children: Implications for the Social Security Administration's Supplemental Security Income ProgramBookApril 2016382 Reads33 CitationsNational Academies of Sciences, Engineering, and MedicineInstitute of MedicineDivision of Behavioral and Social Sciences and Education[...]Committee on the Evaluation of the Supplemental Security Income (SSI) Disability Program for Children with Speech Disorders and Language DisordersSpeech and language are central to the human experience; they are the vital means by which people convey and receive knowledge, thoughts, feelings, and other internal experiences. Acquisition of communication skills begins early in childhood and is foundational to the ability to gain access to culturally transmitted knowledge, organize and share thoughts and feelings, and participate in social interactions and relationships. Thus, speech disorders and language disorders--disruptions in communication development--can have wide-ranging and adverse impacts on the ability to communicate and also to acquire new knowledge and fully participate in society. Severe disruptions in speech or language acquisition have both direct and indirect consequences for child and adolescent development, not only in communication, but also in associated abilities such as reading and academic achievement that depend on speech and language skills. The Supplemental Security Income (SSI) program for children provides financial assistance to children from low-income, resource-limited families who are determined to have conditions that meet the disability standard required under law. Between 2000 and 2010, there was an unprecedented rise in the number of applications and the number of children found to meet the disability criteria. The factors that contribute to these changes are a primary focus of this report. Speech and Language Disorders in Children provides an overview of the current status of the diagnosis and treatment of speech and language disorders and levels of impairment in the U.S. population under age 18. This study identifies past and current trends in the prevalence and persistence of speech disorders and language disorders for the general U.S. population under age 18 and compares those trends to trends in the SSI childhood disability population. © 2016 by the National Academy of Sciences. All rights reserved.Request full-textRecommendFollowShare",,
1698807591-17,https://www.researchgate.net/search.Search.html?query=%22Large+Language+Model+Security%22+&type=publication,,,,,,,,"Large Language Models in Cryptocurrency Securities Cases: Can ChatGPT Replace Lawyers?PreprintAugust 202343 ReadsArianna TrozzeToby DaviesBennett KleinbergLarge Language Models (LLMs) could enhance access to the legal system. However, empirical research on their effectiveness in conducting legal tasks is scant. We study securities cases involving cryptocurrencies as one of numerous contexts where AI could support the legal process, studying LLMs' legal reasoning and drafting capabilities. We examine whether a) an LLM can accurately determine which laws are potentially being violated from a fact pattern, and b) whether there is a difference in juror decision-making based on complaints written by a lawyer compared to an LLM. We feed fact patterns from real-life cases to GPT-3.5 and evaluate its ability to determine correct potential violations from the scenario and exclude spurious violations. Second, we had mock jurors assess complaints written by the LLM and lawyers. GPT-3.5's legal reasoning skills proved weak, though we expect improvement in future models, particularly given the violations it suggested tended to be correct (it merely missed additional, correct violations). GPT-3.5 performed better at legal drafting, and jurors' decisions were not statistically significantly associated with the author of the document upon which they based their decisions. Because LLMs cannot satisfactorily conduct legal reasoning tasks, they would be unable to replace lawyers at this stage. However, their drafting skills (though, perhaps, still inferior to lawyers), could provide access to justice for more individuals by reducing the cost of legal services. Our research is the first to systematically study LLMs' legal drafting and reasoning capabilities in litigation, as well as in securities law and cryptocurrency-related misconduct.DownloadRecommendFollowShare",,
1698807591-18,https://www.researchgate.net/search.Search.html?query=%22Large+Language+Model+Security%22+&type=publication,,,,,,,,"Business Language for Information SecurityChapterJuly 202320 ReadsDinh Uy TranAudun JøsangProminent standards and frameworks for information security clearly state that business aspects on the one side, and technical aspects on the other, are equally important for the management of cyber security. Organisations with a relatively low maturity level in security management typically consider information security primarily as a technological issue. For those organisations, information security might not get the necessary support from top-level management because they are predominantly focused on business aspects, and are blind to the role information security plays for business. To obtain support from top-level management the information security practitioners need the skills to influence and help relevant stakeholders to understand how information security can support business objectives. In this debate, it is often argued that it is important to speak the language of management. This means that information security practitioners should learn how to translate technical terms to a business context, so top-level management can understand what it means for them. However, this debate has mostly focused on the importance of speaking the “Business Language for Information Security (BLIS)” but has not elaborated on what this language consists of and how to learn it. This paper proposes BLIS and a framework for how to learn it. By mastering BLIS, security professionals can articulate arguments that top-executive management can easily understand and act on. Therefore, we argue that taking a learning module on BLIS will be valuable and useful for the next generation of students in information security. Said briefly, learning BLIS will help students understand how information security can support business, and also how this can be explained to others.KeywordsBusiness Language for Information SecurityInformation Security GovernanceInformation Security ManagementInformation Security ReportingRequest full-textRecommendFollowShare",,
1698807591-19,https://www.researchgate.net/search.Search.html?query=%22Large+Language+Model+Security%22+&type=publication,,,,,,,,A Multi-dimensional Generic Evaluation Framework for the Security of Large Language ModelsConference PaperAugust 20233 ReadsZhiyin YuRequest full-textRecommendFollowShare,,
1698807591-20,https://www.researchgate.net/search.Search.html?query=%22Large+Language+Model+Security%22+&type=publication,,,,,,,,"LLM for SoC Security: A Paradigm ShiftPreprintOctober 202335 ReadsDipayan SahaShams TarekKatayoon Yahyaei[...]Farimah FarahmandiAs the ubiquity and complexity of system-on-chip (SoC) designs increase across electronic devices, the task of incorporating security into an SoC design flow poses significant challenges. Existing security solutions are inadequate to provide effective verification of modern SoC designs due to their limitations in scalability, comprehensiveness, and adaptability. On the other hand, Large Language Models (LLMs) are celebrated for their remarkable success in natural language understanding, advanced reasoning, and program synthesis tasks. Recognizing an opportunity, our research delves into leveraging the emergent capabilities of Generative Pre-trained Transformers (GPTs) to address the existing gaps in SoC security, aiming for a more efficient, scalable, and adaptable methodology. By integrating LLMs into the SoC security verification paradigm, we open a new frontier of possibilities and challenges to ensure the security of increasingly complex SoCs. This paper offers an in-depth analysis of existing works, showcases practical case studies, demonstrates comprehensive experiments, and provides useful promoting guidelines. We also present the achievements, prospects, and challenges of employing LLM in different SoC security verification tasks.DownloadRecommendFollowShare",,
1698807591-21,https://www.researchgate.net/search.Search.html?query=%22Large+Language+Model+Security%22+&type=publication,,,,,,,,,"Fixing Hardware Security Bugs with Large Language ModelsPreprintFebruary 202325 ReadsBaleegh AhmadShailja ThakurBenjamin Tan[...]Hammond PearceNovel AI-based code-writing Large Language Models (LLMs) such as OpenAI's Codex have demonstrated capabilities in many coding-adjacent domains. In this work we consider how LLMs maybe leveraged to automatically repair security relevant bugs present in hardware designs. We focus on bug repair in code written in the Hardware Description Language Verilog. For this study we build a corpus of domain-representative hardware security bugs. We then design and implement a framework to quantitatively evaluate the performance of any LLM tasked with fixing the specified bugs. The framework supports design space exploration of prompts (i.e., prompt engineering) and identifying the best parameters for the LLM. We show that an ensemble of LLMs can repair all ten of our benchmarks. This ensemble outperforms the state-of-the-art Cirfix hardware bug repair tool on its own suite of bugs. These results show that LLMs can repair hardware security bugs and the framework is an important step towards the ultimate goal of an automated end-to-end bug repair framework.Request fileRecommendFollowShare",
1698807591-22,https://www.researchgate.net/search.Search.html?query=%22Large+Language+Model+Security%22+&type=publication,,,,,,,,,"Building Resilient SMEs: Harnessing Large Language Models for Cyber Security in AustraliaPreprintJune 202387 ReadsBenjamin Kereopa-YorkeThe escalating digitalisation of our lives and enterprises has led to a parallel growth in the complexity and frequency of cyber-attacks. Small and medium-sized enterprises (SMEs), particularly in Australia, are experiencing increased vulnerability to cyber threats, posing a significant challenge to the nation's cyber security landscape. Embracing transformative technologies such as Artificial Intelligence (AI), Machine Learning (ML) and Large Language Models (LLMs) can potentially strengthen cyber security policies for Australian SMEs. However, their practical application, advantages, and limitations remain underexplored, with prior research mainly focusing on large corporations. This study aims to address this gap by providing a comprehensive understanding of the potential role of LLMs in enhancing cyber security policies for Australian SMEs. Employing a mixed-methods study design, this research includes a literature review, qualitative analysis of SME case studies, and a quantitative assessment of LLM performance metrics in cyber security applications. The findings highlight the promising potential of LLMs across various performance criteria, including relevance, accuracy, and applicability, though gaps remain in areas such as completeness and clarity. The study underlines the importance of integrating human expertise with LLM technology and refining model development to address these limitations. By proposing a robust conceptual framework guiding the effective adoption of LLMs, this research aims to contribute to a safer and more resilient cyber environment for Australian SMEs, enabling sustainable growth and competitiveness in the digital era.DownloadRecommendFollowShare",
1698807591-23,https://www.researchgate.net/search.Search.html?query=%22Large+Language+Model+Security%22+&type=publication,,,,,,,,,"An Empirical Study on Using Large Language Models to Analyze Software Supply Chain Security FailuresPreprintAugust 20239 ReadsTanmay SinglaDharun AnandayuvarajKelechi Kalu[...]James C. DavisAs we increasingly depend on software systems, the consequences of breaches in the software supply chain become more severe. High-profile cyber attacks like those on SolarWinds and ShadowHammer have resulted in significant financial and data losses, underlining the need for stronger cybersecurity. One way to prevent future breaches is by studying past failures. However, traditional methods of analyzing these failures require manually reading and summarizing reports about them. Automated support could reduce costs and allow analysis of more failures. Natural Language Processing (NLP) techniques such as Large Language Models (LLMs) could be leveraged to assist the analysis of failures. In this study, we assessed the ability of Large Language Models (LLMs) to analyze historical software supply chain breaches. We used LLMs to replicate the manual analysis of 69 software supply chain security failures performed by members of the Cloud Native Computing Foundation (CNCF). We developed prompts for LLMs to categorize these by four dimensions: type of compromise, intent, nature, and impact. GPT 3.5s categorizations had an average accuracy of 68% and Bard had an accuracy of 58% over these dimensions. We report that LLMs effectively characterize software supply chain failures when the source articles are detailed enough for consensus among manual analysts, but cannot yet replace human analysts. Future work can improve LLM performance in this context, and study a broader range of articles and failures.Request fileRecommendFollowShare",
1698807591-24,https://www.researchgate.net/search.Search.html?query=%22Large+Language+Model+Security%22+&type=publication,,,,,,,,,"Can Large Language Models Provide Security & Privacy Advice? Measuring the Ability of LLMs to Refute MisconceptionsPreprintOctober 202333 ReadsYufan ChenArjun ArunasalamZ. Berkay CelikUsers seek security & privacy (S&P) advice from online resources, including trusted websites and content-sharing platforms. These resources help users understand S&P technologies and tools and suggest actionable strategies. Large Language Models (LLMs) have recently emerged as trusted information sources. However, their accuracy and correctness have been called into question. Prior research has outlined the shortcomings of LLMs in answering multiple-choice questions and user ability to inadvertently circumvent model restrictions (e.g., to produce toxic content). Yet, the ability of LLMs to provide reliable S&P advice is not well-explored. In this paper, we measure their ability to refute popular S&P misconceptions that the general public holds. We first study recent academic literature to curate a dataset of over a hundred S&P-related misconceptions across six different topics. We then query two popular LLMs (Bard and ChatGPT) and develop a labeling guide to evaluate their responses to these misconceptions. To comprehensively evaluate their responses, we further apply three strategies: query each misconception multiple times, generate and query their paraphrases, and solicit source URLs of the responses. Both models demonstrate, on average, a 21.3% non-negligible error rate, incorrectly supporting popular S&P misconceptions. The error rate increases to 32.6% when we repeatedly query LLMs with the same or paraphrased misconceptions. We also expose that models may partially support a misconception or remain noncommittal, refusing a firm stance on misconceptions. Our exploration of information sources for responses revealed that LLMs are susceptible to providing invalid URLs (21.2% for Bard and 67.7% for ChatGPT) or point to unrelated sources (44.2% returned by Bard and 18.3% by ChatGPT). Our findings highlight that existing LLMs are not completely reliable for S&P advice and motivate future work in understanding how users can better interact with this technology.DownloadRecommendFollowShare",
1698807591-25,https://www.researchgate.net/search.Search.html?query=%22Large+Language+Model+Security%22+&type=publication,,,,,,,,,"Security Implications of Large Language Model Code Assistants: A User StudyPreprintAugust 202219 ReadsGustavo SandovalHammond PearceTeo Nys[...]Siddharth GargAdvances in Deep Learning have led to the emergence of Large Language Models (LLMs) such as OpenAI Codex which powers GitHub Copilot. LLMs have been fine tuned and packaged so that programmers can use them in an Integrated Development Environment (IDE) to write code. An emerging line of work is assessing the code quality of code written with the help of these LLMs, with security studies warning that LLMs do not fundamentally have any understanding of the code they are writing, so they are more likely to make mistakes that may be exploitable. We thus conducted a user study (N=58) to assess the security of code written by student programmers when guided by LLMs. Half of the students in our study had the help of the LLM and the other half did not. The students were asked to write code in C that performed operations over a singly linked list, including node operations such as inserting, updating, removing, combining, and others. While the results of our study showed that the students who had the help of an LLM were more likely to write functional code, no generalizable impact on security was observed -- the security impacts were localized to individual functions. We also investigate systematic stylistic differences between unaided and LLM-assisted code, finding that LLM code is more repetitive, which may have an amplifying effect if vulnerable code is repeated in addition to the impact on source code attribution.Request fileRecommendFollowShare",
1698807591-26,https://www.researchgate.net/search.Search.html?query=%22Large+Language+Model+Security%22+&type=publication,,,,,,,,,"Speech and Language Disorders in Children: Implications for the Social Security Administration's Supplemental Security Income ProgramBookApril 2016382 Reads33 CitationsNational Academies of Sciences, Engineering, and MedicineInstitute of MedicineDivision of Behavioral and Social Sciences and Education[...]Committee on the Evaluation of the Supplemental Security Income (SSI) Disability Program for Children with Speech Disorders and Language DisordersSpeech and language are central to the human experience; they are the vital means by which people convey and receive knowledge, thoughts, feelings, and other internal experiences. Acquisition of communication skills begins early in childhood and is foundational to the ability to gain access to culturally transmitted knowledge, organize and share thoughts and feelings, and participate in social interactions and relationships. Thus, speech disorders and language disorders--disruptions in communication development--can have wide-ranging and adverse impacts on the ability to communicate and also to acquire new knowledge and fully participate in society. Severe disruptions in speech or language acquisition have both direct and indirect consequences for child and adolescent development, not only in communication, but also in associated abilities such as reading and academic achievement that depend on speech and language skills. The Supplemental Security Income (SSI) program for children provides financial assistance to children from low-income, resource-limited families who are determined to have conditions that meet the disability standard required under law. Between 2000 and 2010, there was an unprecedented rise in the number of applications and the number of children found to meet the disability criteria. The factors that contribute to these changes are a primary focus of this report. Speech and Language Disorders in Children provides an overview of the current status of the diagnosis and treatment of speech and language disorders and levels of impairment in the U.S. population under age 18. This study identifies past and current trends in the prevalence and persistence of speech disorders and language disorders for the general U.S. population under age 18 and compares those trends to trends in the SSI childhood disability population. © 2016 by the National Academy of Sciences. All rights reserved.Request full-textRecommendFollowShare",
1698807591-27,https://www.researchgate.net/search.Search.html?query=%22Large+Language+Model+Security%22+&type=publication,,,,,,,,,"Large Language Models in Cryptocurrency Securities Cases: Can ChatGPT Replace Lawyers?PreprintAugust 202343 ReadsArianna TrozzeToby DaviesBennett KleinbergLarge Language Models (LLMs) could enhance access to the legal system. However, empirical research on their effectiveness in conducting legal tasks is scant. We study securities cases involving cryptocurrencies as one of numerous contexts where AI could support the legal process, studying LLMs' legal reasoning and drafting capabilities. We examine whether a) an LLM can accurately determine which laws are potentially being violated from a fact pattern, and b) whether there is a difference in juror decision-making based on complaints written by a lawyer compared to an LLM. We feed fact patterns from real-life cases to GPT-3.5 and evaluate its ability to determine correct potential violations from the scenario and exclude spurious violations. Second, we had mock jurors assess complaints written by the LLM and lawyers. GPT-3.5's legal reasoning skills proved weak, though we expect improvement in future models, particularly given the violations it suggested tended to be correct (it merely missed additional, correct violations). GPT-3.5 performed better at legal drafting, and jurors' decisions were not statistically significantly associated with the author of the document upon which they based their decisions. Because LLMs cannot satisfactorily conduct legal reasoning tasks, they would be unable to replace lawyers at this stage. However, their drafting skills (though, perhaps, still inferior to lawyers), could provide access to justice for more individuals by reducing the cost of legal services. Our research is the first to systematically study LLMs' legal drafting and reasoning capabilities in litigation, as well as in securities law and cryptocurrency-related misconduct.DownloadRecommendFollowShare",
1698807591-28,https://www.researchgate.net/search.Search.html?query=%22Large+Language+Model+Security%22+&type=publication,,,,,,,,,"Business Language for Information SecurityChapterJuly 202320 ReadsDinh Uy TranAudun JøsangProminent standards and frameworks for information security clearly state that business aspects on the one side, and technical aspects on the other, are equally important for the management of cyber security. Organisations with a relatively low maturity level in security management typically consider information security primarily as a technological issue. For those organisations, information security might not get the necessary support from top-level management because they are predominantly focused on business aspects, and are blind to the role information security plays for business. To obtain support from top-level management the information security practitioners need the skills to influence and help relevant stakeholders to understand how information security can support business objectives. In this debate, it is often argued that it is important to speak the language of management. This means that information security practitioners should learn how to translate technical terms to a business context, so top-level management can understand what it means for them. However, this debate has mostly focused on the importance of speaking the “Business Language for Information Security (BLIS)” but has not elaborated on what this language consists of and how to learn it. This paper proposes BLIS and a framework for how to learn it. By mastering BLIS, security professionals can articulate arguments that top-executive management can easily understand and act on. Therefore, we argue that taking a learning module on BLIS will be valuable and useful for the next generation of students in information security. Said briefly, learning BLIS will help students understand how information security can support business, and also how this can be explained to others.KeywordsBusiness Language for Information SecurityInformation Security GovernanceInformation Security ManagementInformation Security ReportingRequest full-textRecommendFollowShare",
1698807591-29,https://www.researchgate.net/search.Search.html?query=%22Large+Language+Model+Security%22+&type=publication,,,,,,,,,A Multi-dimensional Generic Evaluation Framework for the Security of Large Language ModelsConference PaperAugust 20233 ReadsZhiyin YuRequest full-textRecommendFollowShare,
1698807591-30,https://www.researchgate.net/search.Search.html?query=%22Large+Language+Model+Security%22+&type=publication,,,,,,,,,"LLM for SoC Security: A Paradigm ShiftPreprintOctober 202335 ReadsDipayan SahaShams TarekKatayoon Yahyaei[...]Farimah FarahmandiAs the ubiquity and complexity of system-on-chip (SoC) designs increase across electronic devices, the task of incorporating security into an SoC design flow poses significant challenges. Existing security solutions are inadequate to provide effective verification of modern SoC designs due to their limitations in scalability, comprehensiveness, and adaptability. On the other hand, Large Language Models (LLMs) are celebrated for their remarkable success in natural language understanding, advanced reasoning, and program synthesis tasks. Recognizing an opportunity, our research delves into leveraging the emergent capabilities of Generative Pre-trained Transformers (GPTs) to address the existing gaps in SoC security, aiming for a more efficient, scalable, and adaptable methodology. By integrating LLMs into the SoC security verification paradigm, we open a new frontier of possibilities and challenges to ensure the security of increasingly complex SoCs. This paper offers an in-depth analysis of existing works, showcases practical case studies, demonstrates comprehensive experiments, and provides useful promoting guidelines. We also present the achievements, prospects, and challenges of employing LLM in different SoC security verification tasks.DownloadRecommendFollowShare",
1698805625-1,https://scholar.google.ca/scholar?hl=en&as_sdt=0%2C5&q=LLM+security&btnG=,,,,,,,,,,LLM Censorship: A Machine Learning Challenge or a Computer Security Problem?
1698805625-2,https://scholar.google.ca/scholar?hl=en&as_sdt=0%2C5&q=LLM+security&btnG=,,,,,,,,,,"D Glukhov, I Shumailov, Y Gal, N Papernot… - arXiv preprint arXiv …, 2023 - arxiv.org"
1698805625-3,https://scholar.google.ca/scholar?hl=en&as_sdt=0%2C5&q=LLM+security&btnG=,,,,,,,,,,How well does LLM generate security tests?
1698805625-4,https://scholar.google.ca/scholar?hl=en&as_sdt=0%2C5&q=LLM+security&btnG=,,,,,,,,,,"Y Zhang, W Song, Z Ji, N Meng - arXiv preprint arXiv:2310.00710, 2023 - arxiv.org"
1698805625-5,https://scholar.google.ca/scholar?hl=en&as_sdt=0%2C5&q=LLM+security&btnG=,,,,,,,,,,LLM for SoC Security: A Paradigm Shift
1698805625-6,https://scholar.google.ca/scholar?hl=en&as_sdt=0%2C5&q=LLM+security&btnG=,,,,,,,,,,"D Saha, S Tarek, K Yahyaei, SK Saha, J Zhou… - arXiv preprint arXiv …, 2023 - arxiv.org"
1698805625-7,https://scholar.google.ca/scholar?hl=en&as_sdt=0%2C5&q=LLM+security&btnG=,,,,,,,,,,Exploiting programmatic behavior of llms: Dual-use through standard security attacks
1698805625-8,https://scholar.google.ca/scholar?hl=en&as_sdt=0%2C5&q=LLM+security&btnG=,,,,,,,,,,"D Kang, X Li, I Stoica, C Guestrin, M Zaharia… - arXiv preprint arXiv …, 2023 - arxiv.org"
1698805625-9,https://scholar.google.ca/scholar?hl=en&as_sdt=0%2C5&q=LLM+security&btnG=,,,,,,,,,,DIVAS: An LLM-based End-to-End Framework for SoC Security Analysis and Policy-based Protection
1698805625-10,https://scholar.google.ca/scholar?hl=en&as_sdt=0%2C5&q=LLM+security&btnG=,,,,,,,,,,"S Paria, A Dasgupta, S Bhunia - arXiv preprint arXiv:2308.06932, 2023 - arxiv.org"
1698805625-11,https://scholar.google.ca/scholar?hl=en&as_sdt=0%2C5&q=LLM+security&btnG=,,,,,,,,,,LLM Platform Security: Applying a Systematic Evaluation Framework to OpenAI's ChatGPT Plugins
1698805625-12,https://scholar.google.ca/scholar?hl=en&as_sdt=0%2C5&q=LLM+security&btnG=,,,,,,,,,,"U Iqbal, T Kohno, F Roesner - arXiv preprint arXiv:2309.10254, 2023 - arxiv.org"
1698805625-13,https://scholar.google.ca/scholar?hl=en&as_sdt=0%2C5&q=LLM+security&btnG=,,,,,,,,,,Optimizing National Security Strategies through LLM-Driven Artificial Intelligence Integration
1698805625-14,https://scholar.google.ca/scholar?hl=en&as_sdt=0%2C5&q=LLM+security&btnG=,,,,,,,,,,"DI Mikhailov - arXiv preprint arXiv:2305.13927, 2023 - arxiv.org"
1698805625-15,https://scholar.google.ca/scholar?hl=en&as_sdt=0%2C5&q=LLM+security&btnG=,,,,,,,,,,Prompt Injection attack against LLM-integrated Applications
1698805625-16,https://scholar.google.ca/scholar?hl=en&as_sdt=0%2C5&q=LLM+security&btnG=,,,,,,,,,,"Y Liu, G Deng, Y Li, K Wang, T Zhang, Y Liu… - arXiv preprint arXiv …, 2023 - arxiv.org"
1698805625-17,https://scholar.google.ca/scholar?hl=en&as_sdt=0%2C5&q=LLM+security&btnG=,,,,,,,,,,Security implications of large language model code assistants: A user study
1698805625-18,https://scholar.google.ca/scholar?hl=en&as_sdt=0%2C5&q=LLM+security&btnG=,,,,,,,,,,"G Sandoval, H Pearce, T Nys, R Karri… - arXiv preprint arXiv …, 2022 - arxiv.org"
1698805625-19,https://scholar.google.ca/scholar?hl=en&as_sdt=0%2C5&q=LLM+security&btnG=,,,,,,,,,,LLM-assisted Generation of Hardware Assertions
1698805625-20,https://scholar.google.ca/scholar?hl=en&as_sdt=0%2C5&q=LLM+security&btnG=,,,,,,,,,,"R Kande, H Pearce, B Tan, B Dolan-Gavitt… - arXiv preprint arXiv …, 2023 - arxiv.org"
